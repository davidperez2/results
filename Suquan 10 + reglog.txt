Images -> gris
pas de distored_inputs
f=tf.Variable(initial_value=tf.range(-5,5,10/576,dtype=tf.float32),trainable=True)



EVALUATION
2018-01-30 16:15:59.157643: precision @ 1 = 0.094
2018-01-30 16:16:27.995953: precision @ 1 = 0.094
2018-01-30 16:16:56.503071: precision @ 1 = 0.189
2018-01-30 16:17:25.153289: precision @ 1 = 0.189
2018-01-30 16:17:45.211754: precision @ 1 = 0.189
2018-01-30 16:18:12.739456: precision @ 1 = 0.190
2018-01-30 16:18:38.543081: precision @ 1 = 0.190
2018-01-30 16:19:07.308928: precision @ 1 = 0.193
2018-01-30 16:19:34.950206: precision @ 1 = 0.193
2018-01-30 16:20:02.609698: precision @ 1 = 0.193
2018-01-30 16:20:30.857269: precision @ 1 = 0.193
2018-01-30 16:20:58.743358: precision @ 1 = 0.194
2018-01-30 16:21:26.888017: precision @ 1 = 0.194
2018-01-30 16:21:52.939979: precision @ 1 = 0.192
2018-01-30 16:22:20.798951: precision @ 1 = 0.192
2018-01-30 16:22:42.933281: precision @ 1 = 0.192
2018-01-30 16:23:10.894573: precision @ 1 = 0.193
2018-01-30 16:23:37.266906: precision @ 1 = 0.193
2018-01-30 16:24:05.490656: precision @ 1 = 0.193
2018-01-30 16:24:33.332343: precision @ 1 = 0.193
2018-01-30 16:25:00.414068: precision @ 1 = 0.194
2018-01-30 16:25:28.803091: precision @ 1 = 0.194
2018-01-30 16:25:56.582207: precision @ 1 = 0.195
2018-01-30 16:26:24.079730: precision @ 1 = 0.195
2018-01-30 16:26:44.710516: precision @ 1 = 0.195
2018-01-30 16:27:12.624537: precision @ 1 = 0.192
2018-01-30 16:27:38.652939: precision @ 1 = 0.192
2018-01-30 16:28:06.379764: precision @ 1 = 0.192
2018-01-30 16:28:33.900438: precision @ 1 = 0.192
2018-01-30 16:29:01.827715: precision @ 1 = 0.191
2018-01-30 16:29:29.793604: precision @ 1 = 0.191
2018-01-30 16:29:57.401261: precision @ 1 = 0.188
2018-01-30 16:30:25.444797: precision @ 1 = 0.188
2018-01-30 16:30:45.375636: precision @ 1 = 0.188
2018-01-30 16:31:13.064498: precision @ 1 = 0.186
2018-01-30 16:31:38.980677: precision @ 1 = 0.186
2018-01-30 16:32:06.936990: precision @ 1 = 0.186
2018-01-30 16:32:34.924294: precision @ 1 = 0.186
2018-01-30 16:33:03.120505: precision @ 1 = 0.184
2018-01-30 16:33:30.914681: precision @ 1 = 0.184
2018-01-30 16:33:58.623863: precision @ 1 = 0.183
2018-01-30 16:34:26.660637: precision @ 1 = 0.183
2018-01-30 16:34:50.460052: precision @ 1 = 0.182
2018-01-30 16:35:18.339303: precision @ 1 = 0.182
2018-01-30 16:35:41.827312: precision @ 1 = 0.182
2018-01-30 16:36:09.519285: precision @ 1 = 0.184
2018-01-30 16:36:36.831039: precision @ 1 = 0.184
2018-01-30 16:37:04.409752: precision @ 1 = 0.183
2018-01-30 16:37:31.687317: precision @ 1 = 0.183
2018-01-30 16:37:59.412574: precision @ 1 = 0.182
2018-01-30 16:38:26.954188: precision @ 1 = 0.182
2018-01-30 16:38:53.075006: precision @ 1 = 0.182
2018-01-30 16:39:20.931590: precision @ 1 = 0.182
2018-01-30 16:39:43.035598: precision @ 1 = 0.182
2018-01-30 16:40:10.312265: precision @ 1 = 0.181
2018-01-30 16:40:37.346193: precision @ 1 = 0.181
2018-01-30 16:41:05.192386: precision @ 1 = 0.181
2018-01-30 16:41:33.198966: precision @ 1 = 0.181
2018-01-30 16:42:01.132667: precision @ 1 = 0.180
2018-01-30 16:42:28.170574: precision @ 1 = 0.180
2018-01-30 16:42:55.857562: precision @ 1 = 0.181
2018-01-30 16:43:23.547772: precision @ 1 = 0.181
2018-01-30 16:43:45.026464: precision @ 1 = 0.181
2018-01-30 16:44:12.418778: precision @ 1 = 0.181
2018-01-30 16:44:38.264973: precision @ 1 = 0.181
2018-01-30 16:45:06.204660: precision @ 1 = 0.181
2018-01-30 16:45:33.690955: precision @ 1 = 0.181
2018-01-30 16:46:00.876831: precision @ 1 = 0.180
2018-01-30 16:46:28.372571: precision @ 1 = 0.180
2018-01-30 16:46:55.927898: precision @ 1 = 0.180
2018-01-30 16:47:24.113012: precision @ 1 = 0.180
2018-01-30 16:47:44.850682: precision @ 1 = 0.180
2018-01-30 16:48:12.373962: precision @ 1 = 0.180
2018-01-30 16:48:37.906072: precision @ 1 = 0.180
2018-01-30 16:49:05.535766: precision @ 1 = 0.179
2018-01-30 16:49:33.217120: precision @ 1 = 0.179
2018-01-30 16:50:00.547036: precision @ 1 = 0.180
2018-01-30 16:50:28.353316: precision @ 1 = 0.180
2018-01-30 16:50:56.039340: precision @ 1 = 0.181
2018-01-30 16:51:23.887440: precision @ 1 = 0.181
2018-01-30 16:51:44.502531: precision @ 1 = 0.181
2018-01-30 16:52:11.672414: precision @ 1 = 0.182
2018-01-30 16:52:37.681609: precision @ 1 = 0.182
2018-01-30 16:53:05.166658: precision @ 1 = 0.182
2018-01-30 16:53:32.877508: precision @ 1 = 0.182
2018-01-30 16:54:00.599630: precision @ 1 = 0.181
2018-01-30 16:54:28.146535: precision @ 1 = 0.181
2018-01-30 16:54:55.756921: precision @ 1 = 0.182
2018-01-30 16:55:23.255063: precision @ 1 = 0.182
2018-01-30 16:55:44.404696: precision @ 1 = 0.182
2018-01-30 16:56:12.338351: precision @ 1 = 0.181
2018-01-30 16:56:38.332443: precision @ 1 = 0.181
2018-01-30 16:57:06.398457: precision @ 1 = 0.181
2018-01-30 16:57:33.868653: precision @ 1 = 0.181
2018-01-30 16:58:01.847419: precision @ 1 = 0.181
2018-01-30 16:58:29.410009: precision @ 1 = 0.181
2018-01-30 16:58:56.907367: precision @ 1 = 0.181
2018-01-30 16:59:24.967952: precision @ 1 = 0.181
2018-01-30 16:59:45.372686: precision @ 1 = 0.181
2018-01-30 17:00:13.001486: precision @ 1 = 0.181
2018-01-30 17:00:38.511367: precision @ 1 = 0.181
2018-01-30 17:01:06.468611: precision @ 1 = 0.181
2018-01-30 17:01:34.466552: precision @ 1 = 0.181
2018-01-30 17:02:02.189263: precision @ 1 = 0.180
2018-01-30 17:02:30.203008: precision @ 1 = 0.180
2018-01-30 17:02:58.422114: precision @ 1 = 0.179
2018-01-30 17:03:26.626257: precision @ 1 = 0.179
2018-01-30 17:03:47.409377: precision @ 1 = 0.179
2018-01-30 17:04:15.342478: precision @ 1 = 0.179
2018-01-30 17:04:40.176007: precision @ 1 = 0.179
2018-01-30 17:05:08.038526: precision @ 1 = 0.179
2018-01-30 17:05:35.564681: precision @ 1 = 0.179
2018-01-30 17:06:02.928231: precision @ 1 = 0.179
2018-01-30 17:06:30.834295: precision @ 1 = 0.179
2018-01-30 17:06:58.757915: precision @ 1 = 0.178
2018-01-30 17:07:26.404701: precision @ 1 = 0.178
2018-01-30 17:07:45.985647: precision @ 1 = 0.178
2018-01-30 17:08:13.607360: precision @ 1 = 0.178
2018-01-30 17:08:39.147726: precision @ 1 = 0.178
2018-01-30 17:09:07.049175: precision @ 1 = 0.178
2018-01-30 17:09:34.626224: precision @ 1 = 0.178
2018-01-30 17:10:01.931618: precision @ 1 = 0.178
2018-01-30 17:10:29.836820: precision @ 1 = 0.178
2018-01-30 17:10:57.301877: precision @ 1 = 0.178
2018-01-30 17:11:25.125761: precision @ 1 = 0.178
2018-01-30 17:11:45.408030: precision @ 1 = 0.178
2018-01-30 17:12:13.016432: precision @ 1 = 0.178
2018-01-30 17:12:38.564103: precision @ 1 = 0.178
2018-01-30 17:13:05.841248: precision @ 1 = 0.178
2018-01-30 17:13:33.658392: precision @ 1 = 0.178
2018-01-30 17:14:02.050455: precision @ 1 = 0.178
2018-01-30 17:14:29.728570: precision @ 1 = 0.178
2018-01-30 17:14:57.412108: precision @ 1 = 0.178
2018-01-30 17:15:25.240665: precision @ 1 = 0.178
2018-01-30 17:15:45.602845: precision @ 1 = 0.178
2018-01-30 17:16:13.452629: precision @ 1 = 0.178
2018-01-30 17:16:39.331935: precision @ 1 = 0.178
2018-01-30 17:29:37.546454: precision @ 1 = 0.178
2018-01-30 17:30:08.937816: precision @ 1 = 0.178
2018-01-30 17:30:34.006161: precision @ 1 = 0.178
2018-01-30 17:31:02.406701: precision @ 1 = 0.178
2018-01-30 17:31:29.156257: precision @ 1 = 0.178
2018-01-30 17:31:56.723813: precision @ 1 = 0.178
2018-01-30 17:32:24.910900: precision @ 1 = 0.178
2018-01-30 17:32:53.299806: precision @ 1 = 0.178
2018-01-30 17:33:21.332820: precision @ 1 = 0.178
2018-01-30 17:33:49.686260: precision @ 1 = 0.178
2018-01-30 17:34:17.544001: precision @ 1 = 0.178
2018-01-30 17:34:37.403668: precision @ 1 = 0.178
2018-01-30 17:35:05.553210: precision @ 1 = 0.179
2018-01-30 17:35:31.029990: precision @ 1 = 0.179
2018-01-30 17:35:59.122342: precision @ 1 = 0.179
2018-01-30 17:36:26.761243: precision @ 1 = 0.179
2018-01-30 17:36:54.678971: precision @ 1 = 0.179
2018-01-30 17:37:22.455876: precision @ 1 = 0.179
2018-01-30 17:37:50.072582: precision @ 1 = 0.179
2018-01-30 17:38:18.103924: precision @ 1 = 0.179
2018-01-30 17:38:37.714656: precision @ 1 = 0.179
2018-01-30 17:39:00.268200: precision @ 1 = 0.179
2018-01-30 17:39:19.782977: precision @ 1 = 0.179
2018-01-30 17:39:39.094991: precision @ 1 = 0.179
2018-01-30 17:39:58.909671: precision @ 1 = 0.179


LOSS (Train)


2018-01-30 16:15:37.226803: step 0, loss = 2.31 (3406.4 examples/sec; 0.038 sec/batch)
2018-01-30 16:15:40.214177: step 200, loss = 12.07 (8569.4 examples/sec; 0.015 sec/batch)
2018-01-30 16:15:44.292115: step 400, loss = 5.91 (6277.7 examples/sec; 0.020 sec/batch)
2018-01-30 16:15:49.692013: step 600, loss = 9.18 (4740.8 examples/sec; 0.027 sec/batch)
2018-01-30 16:15:55.318540: step 800, loss = 5.93 (4549.9 examples/sec; 0.028 sec/batch)
2018-01-30 16:15:59.984833: step 1000, loss = 6.24 (5486.2 examples/sec; 0.023 sec/batch)
2018-01-30 16:16:02.886217: step 1200, loss = 5.91 (8823.4 examples/sec; 0.015 sec/batch)
2018-01-30 16:16:05.857722: step 1400, loss = 4.49 (8615.2 examples/sec; 0.015 sec/batch)
2018-01-30 16:16:08.701374: step 1600, loss = 6.81 (9002.5 examples/sec; 0.014 sec/batch)
2018-01-30 16:16:12.796301: step 1800, loss = 4.51 (6251.6 examples/sec; 0.020 sec/batch)
2018-01-30 16:16:18.322000: step 2000, loss = 3.65 (4632.9 examples/sec; 0.028 sec/batch)
2018-01-30 16:16:23.876691: step 2200, loss = 4.75 (4608.7 examples/sec; 0.028 sec/batch)
2018-01-30 16:16:28.562652: step 2400, loss = 3.87 (5463.1 examples/sec; 0.023 sec/batch)
2018-01-30 16:16:31.415024: step 2600, loss = 3.17 (8975.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:16:34.352022: step 2800, loss = 3.28 (8716.4 examples/sec; 0.015 sec/batch)
2018-01-30 16:16:38.899240: step 3000, loss = 3.90 (5629.8 examples/sec; 0.023 sec/batch)
2018-01-30 16:16:44.454233: step 3200, loss = 2.88 (4608.5 examples/sec; 0.028 sec/batch)
2018-01-30 16:16:49.730696: step 3400, loss = 4.27 (4851.7 examples/sec; 0.026 sec/batch)
2018-01-30 16:16:54.917360: step 3600, loss = 3.22 (4935.7 examples/sec; 0.026 sec/batch)
2018-01-30 16:16:58.635039: step 3800, loss = 2.50 (6886.0 examples/sec; 0.019 sec/batch)
2018-01-30 16:17:01.510441: step 4000, loss = 2.74 (8903.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:17:04.391415: step 4200, loss = 2.80 (8885.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:17:07.252145: step 4400, loss = 2.74 (8948.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:17:12.270534: step 4600, loss = 2.69 (5101.2 examples/sec; 0.025 sec/batch)
2018-01-30 16:17:17.983596: step 4800, loss = 2.96 (4481.0 examples/sec; 0.029 sec/batch)
2018-01-30 16:17:23.567952: step 5000, loss = 2.62 (4584.2 examples/sec; 0.028 sec/batch)
2018-01-30 16:17:27.168539: step 5200, loss = 2.41 (7110.0 examples/sec; 0.018 sec/batch)
2018-01-30 16:17:30.124129: step 5400, loss = 2.75 (8661.6 examples/sec; 0.015 sec/batch)
2018-01-30 16:17:33.018267: step 5600, loss = 2.33 (8845.5 examples/sec; 0.014 sec/batch)
2018-01-30 16:17:45.821912: step 5800, loss = 2.50 (1999.4 examples/sec; 0.064 sec/batch)
2018-01-30 16:17:48.683738: step 6000, loss = 2.38 (8945.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:17:51.540449: step 6200, loss = 2.39 (8961.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:17:54.385271: step 6400, loss = 2.38 (8998.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:17:58.342560: step 6600, loss = 2.51 (6469.1 examples/sec; 0.020 sec/batch)
2018-01-30 16:18:03.443016: step 6800, loss = 2.30 (5019.2 examples/sec; 0.026 sec/batch)
2018-01-30 16:18:08.866102: step 7000, loss = 2.50 (4720.6 examples/sec; 0.027 sec/batch)
2018-01-30 16:18:13.601555: step 7200, loss = 2.40 (5406.0 examples/sec; 0.024 sec/batch)
2018-01-30 16:18:16.449440: step 7400, loss = 2.35 (8989.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:18:19.319997: step 7600, loss = 2.64 (8918.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:18:22.180283: step 7800, loss = 2.41 (8950.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:18:26.382121: step 8000, loss = 2.37 (6092.6 examples/sec; 0.021 sec/batch)
2018-01-30 16:18:31.541850: step 8200, loss = 2.26 (4961.5 examples/sec; 0.026 sec/batch)
2018-01-30 16:18:40.584014: step 8400, loss = 2.36 (2831.2 examples/sec; 0.045 sec/batch)
2018-01-30 16:18:43.678138: step 8600, loss = 2.27 (8273.7 examples/sec; 0.015 sec/batch)
2018-01-30 16:18:46.839684: step 8800, loss = 2.43 (8097.3 examples/sec; 0.016 sec/batch)
2018-01-30 16:18:50.193680: step 9000, loss = 2.20 (7632.7 examples/sec; 0.017 sec/batch)
2018-01-30 16:18:55.473544: step 9200, loss = 2.40 (4848.6 examples/sec; 0.026 sec/batch)
2018-01-30 16:19:00.718704: step 9400, loss = 2.45 (4880.7 examples/sec; 0.026 sec/batch)
2018-01-30 16:19:05.978888: step 9600, loss = 2.27 (4866.8 examples/sec; 0.026 sec/batch)
2018-01-30 16:19:09.493358: step 9800, loss = 2.36 (7284.2 examples/sec; 0.018 sec/batch)
2018-01-30 16:19:12.571360: step 10000, loss = 2.39 (8317.1 examples/sec; 0.015 sec/batch)
2018-01-30 16:19:15.509844: step 10200, loss = 2.19 (8712.0 examples/sec; 0.015 sec/batch)
2018-01-30 16:19:18.634847: step 10400, loss = 2.34 (8192.0 examples/sec; 0.016 sec/batch)
2018-01-30 16:19:24.039843: step 10600, loss = 2.25 (4736.4 examples/sec; 0.027 sec/batch)
2018-01-30 16:19:29.648956: step 10800, loss = 2.29 (4564.0 examples/sec; 0.028 sec/batch)
2018-01-30 16:19:34.981461: step 11000, loss = 2.28 (4800.7 examples/sec; 0.027 sec/batch)
2018-01-30 16:19:39.351572: step 11200, loss = 2.20 (5858.0 examples/sec; 0.022 sec/batch)
2018-01-30 16:19:42.243214: step 11400, loss = 2.26 (8853.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:19:45.136541: step 11600, loss = 2.33 (8847.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:19:50.094239: step 11800, loss = 2.26 (5163.7 examples/sec; 0.025 sec/batch)
2018-01-30 16:19:55.505254: step 12000, loss = 2.18 (4731.1 examples/sec; 0.027 sec/batch)
2018-01-30 16:20:00.743617: step 12200, loss = 2.37 (4887.0 examples/sec; 0.026 sec/batch)
2018-01-30 16:20:04.584893: step 12400, loss = 2.28 (6664.5 examples/sec; 0.019 sec/batch)
2018-01-30 16:20:07.453895: step 12600, loss = 2.43 (8923.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:20:10.336083: step 12800, loss = 2.23 (8882.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:20:13.213487: step 13000, loss = 2.24 (8896.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:20:18.559762: step 13200, loss = 2.35 (4788.4 examples/sec; 0.027 sec/batch)
2018-01-30 16:20:23.670686: step 13400, loss = 2.26 (5008.9 examples/sec; 0.026 sec/batch)
2018-01-30 16:20:28.669148: step 13600, loss = 2.14 (5121.6 examples/sec; 0.025 sec/batch)
2018-01-30 16:20:32.741329: step 13800, loss = 2.15 (6286.6 examples/sec; 0.020 sec/batch)
2018-01-30 16:20:37.210408: step 14000, loss = 2.19 (5728.2 examples/sec; 0.022 sec/batch)
2018-01-30 16:20:40.074893: step 14200, loss = 2.08 (8937.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:20:43.795145: step 14400, loss = 2.20 (6881.3 examples/sec; 0.019 sec/batch)
2018-01-30 16:20:49.206852: step 14600, loss = 2.28 (4730.5 examples/sec; 0.027 sec/batch)
2018-01-30 16:20:54.256139: step 14800, loss = 2.21 (5070.0 examples/sec; 0.025 sec/batch)
2018-01-30 16:20:59.427889: step 15000, loss = 2.31 (4950.0 examples/sec; 0.026 sec/batch)
2018-01-30 16:21:02.291662: step 15200, loss = 2.16 (8939.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:21:05.166060: step 15400, loss = 2.23 (8906.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:21:08.055306: step 15600, loss = 2.23 (8860.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:21:11.963242: step 15800, loss = 2.23 (6550.8 examples/sec; 0.020 sec/batch)
2018-01-30 16:21:17.152594: step 16000, loss = 2.27 (4933.2 examples/sec; 0.026 sec/batch)
2018-01-30 16:21:22.435271: step 16200, loss = 2.10 (4846.0 examples/sec; 0.026 sec/batch)
2018-01-30 16:21:27.278683: step 16400, loss = 2.10 (5285.5 examples/sec; 0.024 sec/batch)
2018-01-30 16:21:30.149299: step 16600, loss = 2.23 (8917.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:21:32.983931: step 16800, loss = 2.24 (9031.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:21:38.117832: step 17000, loss = 2.25 (4986.5 examples/sec; 0.026 sec/batch)
2018-01-30 16:21:43.743244: step 17200, loss = 2.14 (4550.8 examples/sec; 0.028 sec/batch)
2018-01-30 16:21:49.443989: step 17400, loss = 2.15 (4490.6 examples/sec; 0.029 sec/batch)
2018-01-30 16:21:53.867843: step 17600, loss = 2.12 (5786.8 examples/sec; 0.022 sec/batch)
2018-01-30 16:21:56.717529: step 17800, loss = 2.18 (8983.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:21:59.598851: step 18000, loss = 2.32 (8884.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:22:02.492184: step 18200, loss = 2.19 (8847.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:22:06.799466: step 18400, loss = 2.24 (5943.4 examples/sec; 0.022 sec/batch)
2018-01-30 16:22:12.197780: step 18600, loss = 2.17 (4742.2 examples/sec; 0.027 sec/batch)
2018-01-30 16:22:17.066936: step 18800, loss = 2.16 (5257.6 examples/sec; 0.024 sec/batch)
2018-01-30 16:22:21.672106: step 19000, loss = 2.12 (5559.0 examples/sec; 0.023 sec/batch)
2018-01-30 16:22:24.571121: step 19200, loss = 2.13 (8830.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:22:27.436799: step 19400, loss = 2.19 (8933.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:22:30.294494: step 19600, loss = 2.21 (8958.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:22:34.372311: step 19800, loss = 2.11 (6277.9 examples/sec; 0.020 sec/batch)
2018-01-30 16:22:45.908078: step 20000, loss = 2.12 (2219.2 examples/sec; 0.058 sec/batch)
2018-01-30 16:22:48.784949: step 20200, loss = 2.11 (8898.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:22:51.640933: step 20400, loss = 2.30 (8963.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:22:55.140242: step 20600, loss = 2.11 (7315.7 examples/sec; 0.017 sec/batch)
2018-01-30 16:23:00.173608: step 20800, loss = 2.28 (5086.1 examples/sec; 0.025 sec/batch)
2018-01-30 16:23:05.421454: step 21000, loss = 2.16 (4878.2 examples/sec; 0.026 sec/batch)
2018-01-30 16:23:10.613294: step 21200, loss = 2.18 (4930.8 examples/sec; 0.026 sec/batch)
2018-01-30 16:23:13.626093: step 21400, loss = 2.09 (8497.1 examples/sec; 0.015 sec/batch)
2018-01-30 16:23:16.492908: step 21600, loss = 2.11 (8929.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:23:19.370284: step 21800, loss = 2.23 (8897.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:23:22.913905: step 22000, loss = 2.04 (7224.2 examples/sec; 0.018 sec/batch)
2018-01-30 16:23:28.198585: step 22200, loss = 2.12 (4844.2 examples/sec; 0.026 sec/batch)
2018-01-30 16:23:33.381402: step 22400, loss = 2.22 (4939.4 examples/sec; 0.026 sec/batch)
2018-01-30 16:23:40.233952: step 22600, loss = 2.25 (3735.8 examples/sec; 0.034 sec/batch)
2018-01-30 16:23:43.090994: step 22800, loss = 2.17 (8960.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:23:45.971601: step 23000, loss = 2.22 (8887.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:23:49.610477: step 23200, loss = 2.21 (7035.1 examples/sec; 0.018 sec/batch)
2018-01-30 16:23:54.518509: step 23400, loss = 2.30 (5215.9 examples/sec; 0.025 sec/batch)
2018-01-30 16:23:59.635129: step 23600, loss = 2.08 (5003.3 examples/sec; 0.026 sec/batch)
2018-01-30 16:24:04.816708: step 23800, loss = 2.16 (4940.6 examples/sec; 0.026 sec/batch)
2018-01-30 16:24:07.986441: step 24000, loss = 2.12 (8076.4 examples/sec; 0.016 sec/batch)
2018-01-30 16:24:10.887039: step 24200, loss = 2.11 (8825.8 examples/sec; 0.015 sec/batch)
2018-01-30 16:24:13.744685: step 24400, loss = 2.15 (8958.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:24:16.877893: step 24600, loss = 2.02 (8170.5 examples/sec; 0.016 sec/batch)
2018-01-30 16:24:22.109101: step 24800, loss = 2.18 (4893.7 examples/sec; 0.026 sec/batch)
2018-01-30 16:24:27.554919: step 25000, loss = 2.13 (4700.9 examples/sec; 0.027 sec/batch)
2018-01-30 16:24:33.018842: step 25200, loss = 2.10 (4685.3 examples/sec; 0.027 sec/batch)
2018-01-30 16:24:37.732817: step 25400, loss = 2.22 (5430.7 examples/sec; 0.024 sec/batch)
2018-01-30 16:24:40.597154: step 25600, loss = 2.17 (8937.5 examples/sec; 0.014 sec/batch)
2018-01-30 16:24:43.453459: step 25800, loss = 2.07 (8962.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:24:48.371610: step 26000, loss = 2.08 (5205.2 examples/sec; 0.025 sec/batch)
2018-01-30 16:24:53.880734: step 26200, loss = 2.09 (4646.8 examples/sec; 0.028 sec/batch)
2018-01-30 16:24:59.610731: step 26400, loss = 2.10 (4467.7 examples/sec; 0.029 sec/batch)
2018-01-30 16:25:02.846862: step 26600, loss = 2.04 (7910.7 examples/sec; 0.016 sec/batch)
2018-01-30 16:25:05.706102: step 26800, loss = 2.12 (8953.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:25:08.562317: step 27000, loss = 2.11 (8962.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:25:11.703634: step 27200, loss = 2.15 (8149.4 examples/sec; 0.016 sec/batch)
2018-01-30 16:25:16.767335: step 27400, loss = 2.08 (5055.6 examples/sec; 0.025 sec/batch)
2018-01-30 16:25:21.866908: step 27600, loss = 2.14 (5020.0 examples/sec; 0.025 sec/batch)
2018-01-30 16:25:27.203796: step 27800, loss = 2.09 (4796.8 examples/sec; 0.027 sec/batch)
2018-01-30 16:25:30.820765: step 28000, loss = 2.14 (7077.7 examples/sec; 0.018 sec/batch)
2018-01-30 16:25:33.682985: step 28200, loss = 2.13 (8944.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:25:38.076535: step 28400, loss = 2.18 (5826.7 examples/sec; 0.022 sec/batch)
2018-01-30 16:25:42.152039: step 28600, loss = 2.18 (6281.4 examples/sec; 0.020 sec/batch)
2018-01-30 16:25:47.337888: step 28800, loss = 2.17 (4936.5 examples/sec; 0.026 sec/batch)
2018-01-30 16:25:52.420014: step 29000, loss = 2.17 (5037.3 examples/sec; 0.025 sec/batch)
2018-01-30 16:25:57.342317: step 29200, loss = 2.19 (5200.8 examples/sec; 0.025 sec/batch)
2018-01-30 16:26:00.207411: step 29400, loss = 2.11 (8935.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:26:03.081304: step 29600, loss = 2.05 (8907.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:26:05.962029: step 29800, loss = 2.11 (8886.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:26:10.151768: step 30000, loss = 2.15 (6110.2 examples/sec; 0.021 sec/batch)
2018-01-30 16:26:15.548340: step 30200, loss = 2.10 (4743.8 examples/sec; 0.027 sec/batch)
2018-01-30 16:26:20.798724: step 30400, loss = 2.19 (4875.8 examples/sec; 0.026 sec/batch)
2018-01-30 16:26:25.245656: step 30600, loss = 2.13 (5756.8 examples/sec; 0.022 sec/batch)
2018-01-30 16:26:28.099854: step 30800, loss = 2.05 (8969.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:26:30.961452: step 31000, loss = 2.06 (8946.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:26:33.825134: step 31200, loss = 2.15 (8939.5 examples/sec; 0.014 sec/batch)
2018-01-30 16:26:46.617559: step 31400, loss = 2.15 (2001.2 examples/sec; 0.064 sec/batch)
2018-01-30 16:26:49.477305: step 31600, loss = 2.10 (8951.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:26:52.336526: step 31800, loss = 2.19 (8953.5 examples/sec; 0.014 sec/batch)
2018-01-30 16:26:55.201847: step 32000, loss = 2.03 (8934.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:27:00.192505: step 32200, loss = 2.17 (5129.6 examples/sec; 0.025 sec/batch)
2018-01-30 16:27:05.437137: step 32400, loss = 2.11 (4881.2 examples/sec; 0.026 sec/batch)
2018-01-30 16:27:10.677527: step 32600, loss = 2.09 (4885.1 examples/sec; 0.026 sec/batch)
2018-01-30 16:27:14.400874: step 32800, loss = 2.06 (6875.5 examples/sec; 0.019 sec/batch)
2018-01-30 16:27:17.372292: step 33000, loss = 2.18 (8615.4 examples/sec; 0.015 sec/batch)
2018-01-30 16:27:20.286305: step 33200, loss = 2.06 (8785.1 examples/sec; 0.015 sec/batch)
2018-01-30 16:27:23.213096: step 33400, loss = 2.16 (8746.8 examples/sec; 0.015 sec/batch)
2018-01-30 16:27:28.148479: step 33600, loss = 2.11 (5187.0 examples/sec; 0.025 sec/batch)
2018-01-30 16:27:33.072698: step 33800, loss = 2.12 (5198.8 examples/sec; 0.025 sec/batch)
2018-01-30 16:27:41.216282: step 34000, loss = 2.15 (3143.6 examples/sec; 0.041 sec/batch)
2018-01-30 16:27:44.089728: step 34200, loss = 2.03 (8909.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:27:46.978547: step 34400, loss = 2.11 (8861.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:27:50.218364: step 34600, loss = 2.13 (7901.7 examples/sec; 0.016 sec/batch)
2018-01-30 16:27:55.590467: step 34800, loss = 2.08 (4765.4 examples/sec; 0.027 sec/batch)
2018-01-30 16:28:00.756237: step 35000, loss = 2.22 (4955.7 examples/sec; 0.026 sec/batch)
2018-01-30 16:28:06.067231: step 35200, loss = 2.05 (4820.2 examples/sec; 0.027 sec/batch)
2018-01-30 16:28:09.062038: step 35400, loss = 2.17 (8548.1 examples/sec; 0.015 sec/batch)
2018-01-30 16:28:11.948672: step 35600, loss = 2.09 (8868.5 examples/sec; 0.014 sec/batch)
2018-01-30 16:28:14.825355: step 35800, loss = 2.06 (8899.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:28:18.247979: step 36000, loss = 2.08 (7479.6 examples/sec; 0.017 sec/batch)
2018-01-30 16:28:23.619570: step 36200, loss = 2.10 (4765.8 examples/sec; 0.027 sec/batch)
2018-01-30 16:28:28.759440: step 36400, loss = 2.15 (4980.7 examples/sec; 0.026 sec/batch)
2018-01-30 16:28:34.134838: step 36600, loss = 2.13 (4762.4 examples/sec; 0.027 sec/batch)
2018-01-30 16:28:38.574557: step 36800, loss = 2.02 (5766.1 examples/sec; 0.022 sec/batch)
2018-01-30 16:28:41.444744: step 37000, loss = 2.17 (8919.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:28:44.336042: step 37200, loss = 2.05 (8854.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:28:49.242452: step 37400, loss = 2.05 (5217.7 examples/sec; 0.025 sec/batch)
2018-01-30 16:28:54.575162: step 37600, loss = 2.26 (4800.6 examples/sec; 0.027 sec/batch)
2018-01-30 16:28:59.714776: step 37800, loss = 2.11 (4980.9 examples/sec; 0.026 sec/batch)
2018-01-30 16:29:03.606164: step 38000, loss = 2.11 (6578.6 examples/sec; 0.019 sec/batch)
2018-01-30 16:29:06.466091: step 38200, loss = 2.05 (8951.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:29:09.364506: step 38400, loss = 2.18 (8832.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:29:12.242464: step 38600, loss = 2.06 (8895.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:29:17.230978: step 38800, loss = 2.08 (5131.8 examples/sec; 0.025 sec/batch)
2018-01-30 16:29:22.525210: step 39000, loss = 2.12 (4835.5 examples/sec; 0.026 sec/batch)
2018-01-30 16:29:27.706557: step 39200, loss = 2.04 (4940.8 examples/sec; 0.026 sec/batch)
2018-01-30 16:29:31.502777: step 39400, loss = 2.21 (6743.6 examples/sec; 0.019 sec/batch)
2018-01-30 16:29:34.360546: step 39600, loss = 2.16 (8958.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:29:38.698664: step 39800, loss = 2.19 (5901.2 examples/sec; 0.022 sec/batch)
2018-01-30 16:29:42.444716: step 40000, loss = 2.08 (6833.9 examples/sec; 0.019 sec/batch)
2018-01-30 16:29:48.053908: step 40200, loss = 2.06 (4563.9 examples/sec; 0.028 sec/batch)
2018-01-30 16:29:53.374299: step 40400, loss = 2.17 (4811.7 examples/sec; 0.027 sec/batch)
2018-01-30 16:29:58.041953: step 40600, loss = 2.12 (5484.6 examples/sec; 0.023 sec/batch)
2018-01-30 16:30:00.892291: step 40800, loss = 2.09 (8981.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:30:03.811976: step 41000, loss = 2.21 (8768.1 examples/sec; 0.015 sec/batch)
2018-01-30 16:30:06.734267: step 41200, loss = 2.21 (8760.2 examples/sec; 0.015 sec/batch)
2018-01-30 16:30:10.973827: step 41400, loss = 2.05 (6038.4 examples/sec; 0.021 sec/batch)
2018-01-30 16:30:16.339280: step 41600, loss = 2.13 (4771.3 examples/sec; 0.027 sec/batch)
2018-01-30 16:30:21.565656: step 41800, loss = 2.12 (4898.2 examples/sec; 0.026 sec/batch)
2018-01-30 16:30:26.116743: step 42000, loss = 2.05 (5625.0 examples/sec; 0.023 sec/batch)
2018-01-30 16:30:28.980370: step 42200, loss = 2.05 (8939.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:30:31.888697: step 42400, loss = 2.06 (8802.3 examples/sec; 0.015 sec/batch)
2018-01-30 16:30:34.753487: step 42600, loss = 2.08 (8936.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:30:47.667961: step 42800, loss = 2.13 (1982.3 examples/sec; 0.065 sec/batch)
2018-01-30 16:30:50.558777: step 43000, loss = 2.01 (8855.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:30:53.451694: step 43200, loss = 2.19 (8849.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:30:56.521961: step 43400, loss = 2.04 (8338.0 examples/sec; 0.015 sec/batch)
2018-01-30 16:31:01.877209: step 43600, loss = 2.03 (4780.4 examples/sec; 0.027 sec/batch)
2018-01-30 16:31:07.152535: step 43800, loss = 2.15 (4852.8 examples/sec; 0.026 sec/batch)
2018-01-30 16:31:12.423805: step 44000, loss = 2.10 (4856.5 examples/sec; 0.026 sec/batch)
2018-01-30 16:31:15.606655: step 44200, loss = 2.14 (8043.1 examples/sec; 0.016 sec/batch)
2018-01-30 16:31:18.483918: step 44400, loss = 2.04 (8897.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:31:21.380494: step 44600, loss = 2.06 (8838.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:31:24.649005: step 44800, loss = 2.09 (7832.3 examples/sec; 0.016 sec/batch)
2018-01-30 16:31:29.846328: step 45000, loss = 2.13 (4925.6 examples/sec; 0.026 sec/batch)
2018-01-30 16:31:35.201640: step 45200, loss = 2.17 (4780.3 examples/sec; 0.027 sec/batch)
2018-01-30 16:31:42.658692: step 45400, loss = 2.03 (3433.0 examples/sec; 0.037 sec/batch)
2018-01-30 16:31:45.500931: step 45600, loss = 2.16 (9007.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:31:48.407510: step 45800, loss = 2.03 (8807.6 examples/sec; 0.015 sec/batch)
2018-01-30 16:31:52.474455: step 46000, loss = 2.01 (6294.7 examples/sec; 0.020 sec/batch)
2018-01-30 16:31:57.903852: step 46200, loss = 2.18 (4715.1 examples/sec; 0.027 sec/batch)
2018-01-30 16:32:03.019339: step 46400, loss = 2.04 (5004.4 examples/sec; 0.026 sec/batch)
2018-01-30 16:32:07.759461: step 46600, loss = 2.05 (5400.7 examples/sec; 0.024 sec/batch)
2018-01-30 16:32:10.650485: step 46800, loss = 2.16 (8855.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:32:13.506632: step 47000, loss = 2.15 (8963.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:32:16.394734: step 47200, loss = 2.14 (8864.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:32:20.312896: step 47400, loss = 2.12 (6533.7 examples/sec; 0.020 sec/batch)
2018-01-30 16:32:25.640203: step 47600, loss = 2.17 (4805.4 examples/sec; 0.027 sec/batch)
2018-01-30 16:32:30.937687: step 47800, loss = 2.17 (4832.5 examples/sec; 0.026 sec/batch)
2018-01-30 16:32:35.662903: step 48000, loss = 2.07 (5417.7 examples/sec; 0.024 sec/batch)
2018-01-30 16:32:40.117458: step 48200, loss = 2.00 (5746.9 examples/sec; 0.022 sec/batch)
2018-01-30 16:32:42.965486: step 48400, loss = 2.07 (8988.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:32:45.885748: step 48600, loss = 1.99 (8766.3 examples/sec; 0.015 sec/batch)
2018-01-30 16:32:50.951763: step 48800, loss = 2.08 (5053.3 examples/sec; 0.025 sec/batch)
2018-01-30 16:32:56.087033: step 49000, loss = 2.07 (4985.1 examples/sec; 0.026 sec/batch)
2018-01-30 16:33:01.280854: step 49200, loss = 2.09 (4928.9 examples/sec; 0.026 sec/batch)
2018-01-30 16:33:05.081808: step 49400, loss = 2.13 (6735.2 examples/sec; 0.019 sec/batch)
2018-01-30 16:33:07.933843: step 49600, loss = 2.10 (8976.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:33:10.823700: step 49800, loss = 2.08 (8858.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:33:13.719787: step 50000, loss = 2.00 (8839.5 examples/sec; 0.014 sec/batch)
2018-01-30 16:33:18.961195: step 50200, loss = 2.01 (4884.2 examples/sec; 0.026 sec/batch)
2018-01-30 16:33:23.832665: step 50400, loss = 2.05 (5255.1 examples/sec; 0.024 sec/batch)
2018-01-30 16:33:29.232660: step 50600, loss = 2.09 (4740.7 examples/sec; 0.027 sec/batch)
2018-01-30 16:33:32.958309: step 50800, loss = 2.16 (6871.3 examples/sec; 0.019 sec/batch)
2018-01-30 16:33:37.334576: step 51000, loss = 2.17 (5849.7 examples/sec; 0.022 sec/batch)
2018-01-30 16:33:40.180367: step 51200, loss = 2.00 (8995.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:33:44.099145: step 51400, loss = 2.01 (6532.6 examples/sec; 0.020 sec/batch)
2018-01-30 16:33:49.590315: step 51600, loss = 2.12 (4662.0 examples/sec; 0.027 sec/batch)
2018-01-30 16:33:55.029346: step 51800, loss = 2.04 (4706.7 examples/sec; 0.027 sec/batch)
2018-01-30 16:33:59.544291: step 52000, loss = 2.15 (5670.1 examples/sec; 0.023 sec/batch)
2018-01-30 16:34:02.415352: step 52200, loss = 2.10 (8916.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:34:05.298370: step 52400, loss = 2.13 (8879.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:34:08.187098: step 52600, loss = 2.18 (8862.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:34:12.413037: step 52800, loss = 2.06 (6057.8 examples/sec; 0.021 sec/batch)
2018-01-30 16:34:17.607976: step 53000, loss = 2.09 (4927.9 examples/sec; 0.026 sec/batch)
2018-01-30 16:34:22.856109: step 53200, loss = 2.14 (4877.9 examples/sec; 0.026 sec/batch)
2018-01-30 16:34:27.443454: step 53400, loss = 2.14 (5580.6 examples/sec; 0.023 sec/batch)
2018-01-30 16:34:30.295631: step 53600, loss = 2.10 (8975.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:34:33.143968: step 53800, loss = 2.03 (8987.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:34:41.528670: step 54000, loss = 2.09 (3053.2 examples/sec; 0.042 sec/batch)
2018-01-30 16:34:46.723761: step 54200, loss = 2.09 (4927.7 examples/sec; 0.026 sec/batch)
2018-01-30 16:34:51.451743: step 54400, loss = 2.01 (5414.6 examples/sec; 0.024 sec/batch)
2018-01-30 16:34:54.312304: step 54600, loss = 2.02 (8949.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:34:57.146818: step 54800, loss = 2.02 (9031.5 examples/sec; 0.014 sec/batch)
2018-01-30 16:35:00.025331: step 55000, loss = 1.94 (8893.5 examples/sec; 0.014 sec/batch)
2018-01-30 16:35:04.335242: step 55200, loss = 2.11 (5939.8 examples/sec; 0.022 sec/batch)
2018-01-30 16:35:09.746036: step 55400, loss = 2.16 (4731.3 examples/sec; 0.027 sec/batch)
2018-01-30 16:35:15.084726: step 55600, loss = 2.09 (4795.2 examples/sec; 0.027 sec/batch)
2018-01-30 16:35:19.545544: step 55800, loss = 2.01 (5738.9 examples/sec; 0.022 sec/batch)
2018-01-30 16:35:22.403646: step 56000, loss = 2.15 (8957.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:35:25.261702: step 56200, loss = 2.07 (8957.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:35:28.122374: step 56400, loss = 2.09 (8948.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:35:32.312101: step 56600, loss = 2.11 (6110.2 examples/sec; 0.021 sec/batch)
2018-01-30 16:35:43.508148: step 56800, loss = 2.06 (2286.5 examples/sec; 0.056 sec/batch)
2018-01-30 16:35:46.368107: step 57000, loss = 2.07 (8951.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:35:49.236904: step 57200, loss = 1.99 (8923.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:35:52.095710: step 57400, loss = 2.07 (8954.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:35:56.993438: step 57600, loss = 2.07 (5226.9 examples/sec; 0.024 sec/batch)
2018-01-30 16:36:02.408043: step 57800, loss = 2.05 (4728.0 examples/sec; 0.027 sec/batch)
2018-01-30 16:36:07.476573: step 58000, loss = 2.03 (5050.8 examples/sec; 0.025 sec/batch)
2018-01-30 16:36:11.244102: step 58200, loss = 2.02 (6794.9 examples/sec; 0.019 sec/batch)
2018-01-30 16:36:14.107023: step 58400, loss = 2.05 (8941.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:36:16.959108: step 58600, loss = 2.09 (8975.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:36:19.829371: step 58800, loss = 2.14 (8919.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:36:24.455527: step 59000, loss = 2.14 (5533.8 examples/sec; 0.023 sec/batch)
2018-01-30 16:36:29.780292: step 59200, loss = 2.07 (4807.7 examples/sec; 0.027 sec/batch)
2018-01-30 16:36:34.858766: step 59400, loss = 2.13 (5040.9 examples/sec; 0.025 sec/batch)
2018-01-30 16:36:40.475168: step 59600, loss = 2.07 (4558.1 examples/sec; 0.028 sec/batch)
2018-01-30 16:36:43.337489: step 59800, loss = 2.10 (8943.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:36:46.198051: step 60000, loss = 2.11 (8949.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:36:50.394214: step 60200, loss = 2.00 (6100.8 examples/sec; 0.021 sec/batch)
2018-01-30 16:36:55.530185: step 60400, loss = 2.04 (4984.5 examples/sec; 0.026 sec/batch)
2018-01-30 16:37:00.669997: step 60600, loss = 2.28 (4980.7 examples/sec; 0.026 sec/batch)
2018-01-30 16:37:05.449421: step 60800, loss = 2.05 (5356.3 examples/sec; 0.024 sec/batch)
2018-01-30 16:37:08.293802: step 61000, loss = 2.07 (9000.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:37:11.165400: step 61200, loss = 2.03 (8914.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:37:14.044092: step 61400, loss = 2.02 (8892.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:37:18.551739: step 61600, loss = 2.03 (5679.2 examples/sec; 0.023 sec/batch)
2018-01-30 16:37:23.954450: step 61800, loss = 2.06 (4738.4 examples/sec; 0.027 sec/batch)
2018-01-30 16:37:29.119201: step 62000, loss = 2.16 (4956.7 examples/sec; 0.026 sec/batch)
2018-01-30 16:37:33.258107: step 62200, loss = 2.03 (6185.2 examples/sec; 0.021 sec/batch)
2018-01-30 16:37:37.680053: step 62400, loss = 2.01 (5789.3 examples/sec; 0.022 sec/batch)
2018-01-30 16:37:40.561748: step 62600, loss = 2.05 (8883.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:37:44.107078: step 62800, loss = 2.04 (7220.8 examples/sec; 0.018 sec/batch)
2018-01-30 16:37:49.596477: step 63000, loss = 2.11 (4663.5 examples/sec; 0.027 sec/batch)
2018-01-30 16:37:54.769670: step 63200, loss = 2.09 (4948.6 examples/sec; 0.026 sec/batch)
2018-01-30 16:37:59.833031: step 63400, loss = 2.12 (5055.9 examples/sec; 0.025 sec/batch)
2018-01-30 16:38:02.682489: step 63600, loss = 2.03 (8984.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:38:05.525522: step 63800, loss = 2.03 (9004.5 examples/sec; 0.014 sec/batch)
2018-01-30 16:38:08.395262: step 64000, loss = 2.11 (8920.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:38:12.197762: step 64200, loss = 2.08 (6732.4 examples/sec; 0.019 sec/batch)
2018-01-30 16:38:17.475107: step 64400, loss = 2.18 (4850.9 examples/sec; 0.026 sec/batch)
2018-01-30 16:38:22.773799: step 64600, loss = 2.06 (4831.4 examples/sec; 0.026 sec/batch)
2018-01-30 16:38:27.593394: step 64800, loss = 2.05 (5311.6 examples/sec; 0.024 sec/batch)
2018-01-30 16:38:30.448116: step 65000, loss = 2.06 (8967.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:38:33.304861: step 65200, loss = 2.11 (8961.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:38:38.698022: step 65400, loss = 2.11 (4746.8 examples/sec; 0.027 sec/batch)
2018-01-30 16:38:44.101784: step 65600, loss = 2.02 (4737.4 examples/sec; 0.027 sec/batch)
2018-01-30 16:38:49.574373: step 65800, loss = 1.98 (4677.9 examples/sec; 0.027 sec/batch)
2018-01-30 16:38:54.050236: step 66000, loss = 2.08 (5719.6 examples/sec; 0.022 sec/batch)
2018-01-30 16:38:56.899735: step 66200, loss = 2.03 (8984.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:38:59.827473: step 66400, loss = 2.15 (8744.0 examples/sec; 0.015 sec/batch)
2018-01-30 16:39:02.696117: step 66600, loss = 2.06 (8924.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:39:06.743471: step 66800, loss = 1.93 (6325.1 examples/sec; 0.020 sec/batch)
2018-01-30 16:39:12.052658: step 67000, loss = 2.11 (4821.8 examples/sec; 0.027 sec/batch)
2018-01-30 16:39:17.274260: step 67200, loss = 2.09 (4902.7 examples/sec; 0.026 sec/batch)
2018-01-30 16:39:21.938673: step 67400, loss = 2.12 (5488.4 examples/sec; 0.023 sec/batch)
2018-01-30 16:39:24.886985: step 67600, loss = 2.07 (8682.9 examples/sec; 0.015 sec/batch)
2018-01-30 16:39:27.808814: step 67800, loss = 2.12 (8761.6 examples/sec; 0.015 sec/batch)
2018-01-30 16:39:30.701359: step 68000, loss = 2.12 (8850.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:39:35.162761: step 68200, loss = 2.05 (5738.1 examples/sec; 0.022 sec/batch)
2018-01-30 16:39:46.233355: step 68400, loss = 2.02 (2312.4 examples/sec; 0.055 sec/batch)
2018-01-30 16:39:49.125554: step 68600, loss = 2.18 (8851.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:39:51.993145: step 68800, loss = 2.03 (8927.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:39:55.841683: step 69000, loss = 2.13 (6651.9 examples/sec; 0.019 sec/batch)
2018-01-30 16:40:01.281111: step 69200, loss = 2.20 (4706.4 examples/sec; 0.027 sec/batch)
2018-01-30 16:40:06.584502: step 69400, loss = 2.06 (4827.1 examples/sec; 0.027 sec/batch)
2018-01-30 16:40:11.374034: step 69600, loss = 2.05 (5345.0 examples/sec; 0.024 sec/batch)
2018-01-30 16:40:14.258976: step 69800, loss = 2.15 (8873.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:40:17.112900: step 70000, loss = 2.04 (8970.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:40:19.996488: step 70200, loss = 2.10 (8877.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:40:24.505209: step 70400, loss = 2.08 (5677.9 examples/sec; 0.023 sec/batch)
2018-01-30 16:40:29.644071: step 70600, loss = 2.11 (4981.6 examples/sec; 0.026 sec/batch)
2018-01-30 16:40:34.701548: step 70800, loss = 2.23 (5061.8 examples/sec; 0.025 sec/batch)
2018-01-30 16:40:40.885124: step 71000, loss = 2.03 (4140.0 examples/sec; 0.031 sec/batch)
2018-01-30 16:40:43.775030: step 71200, loss = 2.05 (8858.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:40:46.636535: step 71400, loss = 2.07 (8946.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:40:50.671673: step 71600, loss = 2.01 (6344.3 examples/sec; 0.020 sec/batch)
2018-01-30 16:40:55.936022: step 71800, loss = 2.06 (4862.9 examples/sec; 0.026 sec/batch)
2018-01-30 16:41:01.227945: step 72000, loss = 2.15 (4837.6 examples/sec; 0.026 sec/batch)
2018-01-30 16:41:05.945995: step 72200, loss = 2.07 (5426.0 examples/sec; 0.024 sec/batch)
2018-01-30 16:41:08.815440: step 72400, loss = 2.09 (8921.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:41:11.742462: step 72600, loss = 2.13 (8746.1 examples/sec; 0.015 sec/batch)
2018-01-30 16:41:14.598271: step 72800, loss = 1.98 (8964.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:41:18.911211: step 73000, loss = 2.04 (5935.6 examples/sec; 0.022 sec/batch)
2018-01-30 16:41:24.020895: step 73200, loss = 2.09 (5010.1 examples/sec; 0.026 sec/batch)
2018-01-30 16:41:29.208251: step 73400, loss = 2.03 (4935.1 examples/sec; 0.026 sec/batch)
2018-01-30 16:41:33.815190: step 73600, loss = 2.19 (5556.8 examples/sec; 0.023 sec/batch)
2018-01-30 16:41:38.167644: step 73800, loss = 2.11 (5881.7 examples/sec; 0.022 sec/batch)
2018-01-30 16:41:41.062939: step 74000, loss = 2.04 (8841.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:41:43.959157: step 74200, loss = 2.02 (8839.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:41:49.271752: step 74400, loss = 2.05 (4818.7 examples/sec; 0.027 sec/batch)
2018-01-30 16:41:54.879515: step 74600, loss = 2.06 (4565.1 examples/sec; 0.028 sec/batch)
2018-01-30 16:41:59.627209: step 74800, loss = 2.11 (5392.1 examples/sec; 0.024 sec/batch)
2018-01-30 16:42:03.243118: step 75000, loss = 2.04 (7079.8 examples/sec; 0.018 sec/batch)
2018-01-30 16:42:06.109684: step 75200, loss = 2.00 (8930.5 examples/sec; 0.014 sec/batch)
2018-01-30 16:42:08.965010: step 75400, loss = 2.07 (8965.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:42:11.842155: step 75600, loss = 1.99 (8897.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:42:17.352807: step 75800, loss = 2.04 (4645.5 examples/sec; 0.028 sec/batch)
2018-01-30 16:42:22.697071: step 76000, loss = 2.14 (4790.2 examples/sec; 0.027 sec/batch)
2018-01-30 16:42:28.279961: step 76200, loss = 2.04 (4585.4 examples/sec; 0.028 sec/batch)
2018-01-30 16:42:31.155856: step 76400, loss = 2.10 (8901.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:42:34.033456: step 76600, loss = 2.06 (8896.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:42:38.393312: step 76800, loss = 2.13 (5871.8 examples/sec; 0.022 sec/batch)
2018-01-30 16:42:43.172716: step 77000, loss = 2.03 (5356.3 examples/sec; 0.024 sec/batch)
2018-01-30 16:42:48.783667: step 77200, loss = 2.07 (4562.5 examples/sec; 0.028 sec/batch)
2018-01-30 16:42:54.346097: step 77400, loss = 2.18 (4602.3 examples/sec; 0.028 sec/batch)
2018-01-30 16:42:57.862268: step 77600, loss = 2.05 (7280.6 examples/sec; 0.018 sec/batch)
2018-01-30 16:43:00.725723: step 77800, loss = 2.08 (8940.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:43:03.585081: step 78000, loss = 2.14 (8953.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:43:06.443967: step 78200, loss = 2.15 (8954.5 examples/sec; 0.014 sec/batch)
2018-01-30 16:43:11.710750: step 78400, loss = 2.07 (4860.7 examples/sec; 0.026 sec/batch)
2018-01-30 16:43:16.969549: step 78600, loss = 2.06 (4868.0 examples/sec; 0.026 sec/batch)
2018-01-30 16:43:21.953329: step 78800, loss = 2.11 (5136.7 examples/sec; 0.025 sec/batch)
2018-01-30 16:43:25.731140: step 79000, loss = 2.03 (6776.4 examples/sec; 0.019 sec/batch)
2018-01-30 16:43:28.588946: step 79200, loss = 2.02 (8957.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:43:31.465637: step 79400, loss = 2.05 (8899.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:43:34.447112: step 79600, loss = 2.14 (8586.4 examples/sec; 0.015 sec/batch)
2018-01-30 16:43:47.741334: step 79800, loss = 2.10 (1925.6 examples/sec; 0.066 sec/batch)
2018-01-30 16:43:50.610077: step 80000, loss = 2.17 (8923.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:43:53.464354: step 80200, loss = 2.04 (8969.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:43:56.960200: step 80400, loss = 1.95 (7323.0 examples/sec; 0.017 sec/batch)
2018-01-30 16:44:02.269796: step 80600, loss = 2.15 (4821.5 examples/sec; 0.027 sec/batch)
2018-01-30 16:44:07.328330: step 80800, loss = 2.10 (5060.8 examples/sec; 0.025 sec/batch)
2018-01-30 16:44:12.621924: step 81000, loss = 2.12 (4836.0 examples/sec; 0.026 sec/batch)
2018-01-30 16:44:15.489968: step 81200, loss = 2.06 (8925.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:44:18.358147: step 81400, loss = 2.10 (8925.5 examples/sec; 0.014 sec/batch)
2018-01-30 16:44:21.216661: step 81600, loss = 2.20 (8955.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:44:24.847065: step 81800, loss = 2.12 (7051.6 examples/sec; 0.018 sec/batch)
2018-01-30 16:44:30.254949: step 82000, loss = 2.03 (4733.8 examples/sec; 0.027 sec/batch)
2018-01-30 16:44:35.276959: step 82200, loss = 2.19 (5097.6 examples/sec; 0.025 sec/batch)
2018-01-30 16:44:42.030721: step 82400, loss = 2.14 (3790.5 examples/sec; 0.034 sec/batch)
2018-01-30 16:44:44.865723: step 82600, loss = 2.14 (9030.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:44:47.718176: step 82800, loss = 2.04 (8974.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:44:51.867879: step 83000, loss = 2.00 (6169.1 examples/sec; 0.021 sec/batch)
2018-01-30 16:44:56.705772: step 83200, loss = 2.05 (5291.6 examples/sec; 0.024 sec/batch)
2018-01-30 16:45:02.122009: step 83400, loss = 2.14 (4726.5 examples/sec; 0.027 sec/batch)
2018-01-30 16:45:06.866329: step 83600, loss = 2.06 (5395.9 examples/sec; 0.024 sec/batch)
2018-01-30 16:45:09.729070: step 83800, loss = 1.98 (8942.5 examples/sec; 0.014 sec/batch)
2018-01-30 16:45:12.584324: step 84000, loss = 1.96 (8965.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:45:15.439830: step 84200, loss = 2.21 (8965.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:45:19.428149: step 84400, loss = 2.09 (6418.7 examples/sec; 0.020 sec/batch)
2018-01-30 16:45:24.744026: step 84600, loss = 2.05 (4815.8 examples/sec; 0.027 sec/batch)
2018-01-30 16:45:30.170008: step 84800, loss = 2.01 (4718.0 examples/sec; 0.027 sec/batch)
2018-01-30 16:45:34.711151: step 85000, loss = 2.00 (5637.3 examples/sec; 0.023 sec/batch)
2018-01-30 16:45:39.105255: step 85200, loss = 2.12 (5826.0 examples/sec; 0.022 sec/batch)
2018-01-30 16:45:41.976809: step 85400, loss = 1.99 (8915.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:45:45.245160: step 85600, loss = 1.99 (7832.7 examples/sec; 0.016 sec/batch)
2018-01-30 16:45:50.656203: step 85800, loss = 1.97 (4731.1 examples/sec; 0.027 sec/batch)
2018-01-30 16:45:55.975196: step 86000, loss = 2.13 (4812.9 examples/sec; 0.027 sec/batch)
2018-01-30 16:46:01.236244: step 86200, loss = 2.05 (4866.0 examples/sec; 0.026 sec/batch)
2018-01-30 16:46:04.088917: step 86400, loss = 2.07 (8974.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:46:06.921191: step 86600, loss = 2.11 (9038.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:46:09.775464: step 86800, loss = 2.13 (8969.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:46:13.551628: step 87000, loss = 2.07 (6779.4 examples/sec; 0.019 sec/batch)
2018-01-30 16:46:18.928939: step 87200, loss = 2.14 (4760.7 examples/sec; 0.027 sec/batch)
2018-01-30 16:46:24.198438: step 87400, loss = 2.09 (4858.1 examples/sec; 0.026 sec/batch)
2018-01-30 16:46:28.947204: step 87600, loss = 2.09 (5390.9 examples/sec; 0.024 sec/batch)
2018-01-30 16:46:31.805296: step 87800, loss = 2.10 (8957.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:46:34.661472: step 88000, loss = 2.11 (8963.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:46:39.086762: step 88200, loss = 2.09 (5784.9 examples/sec; 0.022 sec/batch)
2018-01-30 16:46:44.173385: step 88400, loss = 2.05 (5032.8 examples/sec; 0.025 sec/batch)
2018-01-30 16:46:49.581845: step 88600, loss = 2.12 (4733.3 examples/sec; 0.027 sec/batch)
2018-01-30 16:46:54.825707: step 88800, loss = 2.10 (4881.9 examples/sec; 0.026 sec/batch)
2018-01-30 16:46:58.189428: step 89000, loss = 2.07 (7610.6 examples/sec; 0.017 sec/batch)
2018-01-30 16:47:01.065372: step 89200, loss = 2.10 (8901.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:47:03.896635: step 89400, loss = 2.09 (9041.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:47:06.814262: step 89600, loss = 2.09 (8774.3 examples/sec; 0.015 sec/batch)
2018-01-30 16:47:11.911909: step 89800, loss = 2.16 (5021.9 examples/sec; 0.025 sec/batch)
2018-01-30 16:47:16.902971: step 90000, loss = 2.10 (5129.2 examples/sec; 0.025 sec/batch)
2018-01-30 16:47:22.032331: step 90200, loss = 2.10 (4990.9 examples/sec; 0.026 sec/batch)
2018-01-30 16:47:25.849859: step 90400, loss = 2.13 (6705.9 examples/sec; 0.019 sec/batch)
2018-01-30 16:47:28.678554: step 90600, loss = 2.01 (9050.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:47:31.538655: step 90800, loss = 2.06 (8950.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:47:34.388941: step 91000, loss = 2.21 (8981.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:47:47.123870: step 91200, loss = 2.04 (2010.2 examples/sec; 0.064 sec/batch)
2018-01-30 16:47:49.979596: step 91400, loss = 2.03 (8964.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:47:52.851804: step 91600, loss = 2.09 (8913.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:47:55.731823: step 91800, loss = 2.13 (8888.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:48:01.045228: step 92000, loss = 2.10 (4818.0 examples/sec; 0.027 sec/batch)
2018-01-30 16:48:06.690494: step 92200, loss = 2.08 (4534.8 examples/sec; 0.028 sec/batch)
2018-01-30 16:48:11.838972: step 92400, loss = 2.10 (4972.3 examples/sec; 0.026 sec/batch)
2018-01-30 16:48:14.961327: step 92600, loss = 2.04 (8198.9 examples/sec; 0.016 sec/batch)
2018-01-30 16:48:17.827177: step 92800, loss = 2.04 (8932.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:48:20.829944: step 93000, loss = 2.05 (8525.5 examples/sec; 0.015 sec/batch)
2018-01-30 16:48:24.071377: step 93200, loss = 2.07 (7897.7 examples/sec; 0.016 sec/batch)
2018-01-30 16:48:29.679771: step 93400, loss = 2.04 (4564.6 examples/sec; 0.028 sec/batch)
2018-01-30 16:48:35.364266: step 93600, loss = 2.04 (4503.5 examples/sec; 0.028 sec/batch)
2018-01-30 16:48:41.782976: step 93800, loss = 2.09 (3988.3 examples/sec; 0.032 sec/batch)
2018-01-30 16:48:44.630963: step 94000, loss = 2.14 (8988.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:48:47.467377: step 94200, loss = 2.11 (9025.5 examples/sec; 0.014 sec/batch)
2018-01-30 16:48:51.761383: step 94400, loss = 2.08 (5961.8 examples/sec; 0.021 sec/batch)
2018-01-30 16:48:57.068528: step 94600, loss = 2.05 (4823.7 examples/sec; 0.027 sec/batch)
2018-01-30 16:49:02.386358: step 94800, loss = 2.05 (4814.0 examples/sec; 0.027 sec/batch)
2018-01-30 16:49:06.676729: step 95000, loss = 2.03 (5966.8 examples/sec; 0.021 sec/batch)
2018-01-30 16:49:09.517728: step 95200, loss = 1.97 (9010.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:49:12.358612: step 95400, loss = 2.02 (9011.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:49:15.191897: step 95600, loss = 2.05 (9035.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:49:19.497650: step 95800, loss = 1.96 (5945.5 examples/sec; 0.022 sec/batch)
2018-01-30 16:49:24.718476: step 96000, loss = 2.17 (4903.4 examples/sec; 0.026 sec/batch)
2018-01-30 16:49:29.983517: step 96200, loss = 2.05 (4862.3 examples/sec; 0.026 sec/batch)
2018-01-30 16:49:34.304676: step 96400, loss = 2.05 (5924.3 examples/sec; 0.022 sec/batch)
2018-01-30 16:49:38.614911: step 96600, loss = 2.12 (5939.4 examples/sec; 0.022 sec/batch)
2018-01-30 16:49:41.452866: step 96800, loss = 2.12 (9020.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:49:44.576187: step 97000, loss = 2.04 (8196.4 examples/sec; 0.016 sec/batch)
2018-01-30 16:49:50.081735: step 97200, loss = 2.04 (4649.9 examples/sec; 0.028 sec/batch)
2018-01-30 16:49:55.447204: step 97400, loss = 1.96 (4771.3 examples/sec; 0.027 sec/batch)
2018-01-30 16:50:00.750183: step 97600, loss = 2.13 (4827.5 examples/sec; 0.027 sec/batch)
2018-01-30 16:50:03.585075: step 97800, loss = 2.01 (9030.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:50:06.464799: step 98000, loss = 2.11 (8889.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:50:09.330590: step 98200, loss = 2.04 (8933.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:50:13.035162: step 98400, loss = 2.11 (6910.4 examples/sec; 0.019 sec/batch)
2018-01-30 16:50:18.114186: step 98600, loss = 2.06 (5040.3 examples/sec; 0.025 sec/batch)
2018-01-30 16:50:23.205950: step 98800, loss = 2.05 (5027.7 examples/sec; 0.025 sec/batch)
2018-01-30 16:50:28.525211: step 99000, loss = 2.14 (4812.7 examples/sec; 0.027 sec/batch)
2018-01-30 16:50:31.403600: step 99200, loss = 2.08 (8893.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:50:34.248947: step 99400, loss = 2.12 (8997.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:50:38.814248: step 99600, loss = 2.01 (5607.5 examples/sec; 0.023 sec/batch)
2018-01-30 16:50:43.582448: step 99800, loss = 2.20 (5368.9 examples/sec; 0.024 sec/batch)
2018-01-30 16:50:48.898185: step 100000, loss = 2.03 (4815.9 examples/sec; 0.027 sec/batch)
2018-01-30 16:50:54.163326: step 100200, loss = 2.08 (4862.2 examples/sec; 0.026 sec/batch)
2018-01-30 16:50:57.935350: step 100400, loss = 2.11 (6786.8 examples/sec; 0.019 sec/batch)
2018-01-30 16:51:00.786448: step 100600, loss = 2.06 (8979.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:51:03.649757: step 100800, loss = 2.14 (8940.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:51:06.529950: step 101000, loss = 2.05 (8888.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:51:11.524792: step 101200, loss = 2.08 (5125.3 examples/sec; 0.025 sec/batch)
2018-01-30 16:51:16.843651: step 101400, loss = 2.09 (4813.1 examples/sec; 0.027 sec/batch)
2018-01-30 16:51:22.066550: step 101600, loss = 2.13 (4901.5 examples/sec; 0.026 sec/batch)
2018-01-30 16:51:25.625666: step 101800, loss = 2.07 (7192.8 examples/sec; 0.018 sec/batch)
2018-01-30 16:51:28.503918: step 102000, loss = 2.19 (8894.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:51:31.365216: step 102200, loss = 2.02 (8947.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:51:34.220825: step 102400, loss = 2.05 (8964.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:51:46.966320: step 102600, loss = 2.08 (2008.6 examples/sec; 0.064 sec/batch)
2018-01-30 16:51:49.824648: step 102800, loss = 2.08 (8956.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:51:52.671967: step 103000, loss = 2.03 (8990.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:51:55.789452: step 103200, loss = 2.09 (8211.7 examples/sec; 0.016 sec/batch)
2018-01-30 16:52:01.391458: step 103400, loss = 2.13 (4569.8 examples/sec; 0.028 sec/batch)
2018-01-30 16:52:06.805540: step 103600, loss = 2.09 (4728.4 examples/sec; 0.027 sec/batch)
2018-01-30 16:52:11.922439: step 103800, loss = 2.02 (5003.0 examples/sec; 0.026 sec/batch)
2018-01-30 16:52:14.767732: step 104000, loss = 2.10 (8997.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:52:17.636257: step 104200, loss = 2.05 (8924.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:52:20.512905: step 104400, loss = 2.01 (8899.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:52:24.197420: step 104600, loss = 2.02 (6948.0 examples/sec; 0.018 sec/batch)
2018-01-30 16:52:29.480203: step 104800, loss = 2.07 (4845.9 examples/sec; 0.026 sec/batch)
2018-01-30 16:52:35.046943: step 105000, loss = 2.07 (4598.7 examples/sec; 0.028 sec/batch)
2018-01-30 16:52:41.248563: step 105200, loss = 1.92 (4128.0 examples/sec; 0.031 sec/batch)
2018-01-30 16:52:44.097093: step 105400, loss = 2.09 (8987.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:52:46.940145: step 105600, loss = 2.15 (9004.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:52:50.915183: step 105800, loss = 2.07 (6440.2 examples/sec; 0.020 sec/batch)
2018-01-30 16:52:56.301704: step 106000, loss = 2.17 (4752.6 examples/sec; 0.027 sec/batch)
2018-01-30 16:53:01.524684: step 106200, loss = 2.06 (4901.4 examples/sec; 0.026 sec/batch)
2018-01-30 16:53:06.196884: step 106400, loss = 2.03 (5479.2 examples/sec; 0.023 sec/batch)
2018-01-30 16:53:09.085039: step 106600, loss = 1.94 (8863.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:53:11.907244: step 106800, loss = 2.06 (9070.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:53:14.750664: step 107000, loss = 2.12 (9003.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:53:19.127150: step 107200, loss = 2.03 (5849.4 examples/sec; 0.022 sec/batch)
2018-01-30 16:53:24.042635: step 107400, loss = 2.12 (5208.0 examples/sec; 0.025 sec/batch)
2018-01-30 16:53:29.250287: step 107600, loss = 2.14 (4915.8 examples/sec; 0.026 sec/batch)
2018-01-30 16:53:33.872529: step 107800, loss = 2.06 (5538.4 examples/sec; 0.023 sec/batch)
2018-01-30 16:53:38.310606: step 108000, loss = 1.98 (5768.3 examples/sec; 0.022 sec/batch)
2018-01-30 16:53:41.170417: step 108200, loss = 2.10 (8951.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:53:44.436943: step 108400, loss = 2.06 (7837.1 examples/sec; 0.016 sec/batch)
2018-01-30 16:53:49.629142: step 108600, loss = 2.05 (4930.5 examples/sec; 0.026 sec/batch)
2018-01-30 16:53:54.930784: step 108800, loss = 2.07 (4828.7 examples/sec; 0.027 sec/batch)
2018-01-30 16:54:00.279421: step 109000, loss = 2.13 (4786.3 examples/sec; 0.027 sec/batch)
2018-01-30 16:54:03.296845: step 109200, loss = 2.13 (8484.1 examples/sec; 0.015 sec/batch)
2018-01-30 16:54:06.150096: step 109400, loss = 2.11 (8972.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:54:09.001838: step 109600, loss = 2.09 (8977.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:54:12.498502: step 109800, loss = 2.10 (7321.3 examples/sec; 0.017 sec/batch)
2018-01-30 16:54:17.991807: step 110000, loss = 1.99 (4660.2 examples/sec; 0.027 sec/batch)
2018-01-30 16:54:22.950925: step 110200, loss = 2.02 (5162.2 examples/sec; 0.025 sec/batch)
2018-01-30 16:54:28.162160: step 110400, loss = 2.06 (4912.5 examples/sec; 0.026 sec/batch)
2018-01-30 16:54:31.026749: step 110600, loss = 2.17 (8936.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:54:33.870023: step 110800, loss = 1.96 (9003.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:54:38.212212: step 111000, loss = 2.11 (5895.6 examples/sec; 0.022 sec/batch)
2018-01-30 16:54:42.802643: step 111200, loss = 2.08 (5576.8 examples/sec; 0.023 sec/batch)
2018-01-30 16:54:48.160382: step 111400, loss = 2.06 (4778.1 examples/sec; 0.027 sec/batch)
2018-01-30 16:54:53.345362: step 111600, loss = 2.11 (4937.3 examples/sec; 0.026 sec/batch)
2018-01-30 16:54:57.454385: step 111800, loss = 2.08 (6230.2 examples/sec; 0.021 sec/batch)
2018-01-30 16:55:00.311900: step 112000, loss = 2.19 (8958.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:55:03.135619: step 112200, loss = 2.01 (9066.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:55:05.998571: step 112400, loss = 2.07 (8941.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:55:11.169598: step 112600, loss = 2.01 (4950.7 examples/sec; 0.026 sec/batch)
2018-01-30 16:55:16.340570: step 112800, loss = 1.99 (4950.7 examples/sec; 0.026 sec/batch)
2018-01-30 16:55:21.539540: step 113000, loss = 2.02 (4924.1 examples/sec; 0.026 sec/batch)
2018-01-30 16:55:25.207855: step 113200, loss = 2.02 (6978.7 examples/sec; 0.018 sec/batch)
2018-01-30 16:55:28.045062: step 113400, loss = 2.08 (9023.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:55:30.883777: step 113600, loss = 2.10 (9018.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:55:33.748521: step 113800, loss = 2.11 (8936.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:55:46.482370: step 114000, loss = 2.13 (2010.4 examples/sec; 0.064 sec/batch)
2018-01-30 16:55:49.336515: step 114200, loss = 2.00 (8969.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:55:52.175753: step 114400, loss = 1.94 (9016.5 examples/sec; 0.014 sec/batch)
2018-01-30 16:55:55.074384: step 114600, loss = 2.00 (8831.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:56:00.225902: step 114800, loss = 2.19 (4969.4 examples/sec; 0.026 sec/batch)
2018-01-30 16:56:05.276521: step 115000, loss = 2.15 (5068.7 examples/sec; 0.025 sec/batch)
2018-01-30 16:56:10.558973: step 115200, loss = 2.06 (4846.2 examples/sec; 0.026 sec/batch)
2018-01-30 16:56:14.226935: step 115400, loss = 2.20 (6979.4 examples/sec; 0.018 sec/batch)
2018-01-30 16:56:17.059053: step 115600, loss = 2.05 (9039.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:56:19.909630: step 115800, loss = 1.95 (8980.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:56:22.758393: step 116000, loss = 2.06 (8986.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:56:27.696599: step 116200, loss = 2.10 (5184.1 examples/sec; 0.025 sec/batch)
2018-01-30 16:56:32.711592: step 116400, loss = 2.10 (5104.7 examples/sec; 0.025 sec/batch)
2018-01-30 16:56:40.683541: step 116600, loss = 2.16 (3211.3 examples/sec; 0.040 sec/batch)
2018-01-30 16:56:43.504537: step 116800, loss = 2.11 (9074.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:56:46.353658: step 117000, loss = 2.11 (8985.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:56:49.267932: step 117200, loss = 2.07 (8784.3 examples/sec; 0.015 sec/batch)
2018-01-30 16:56:54.325356: step 117400, loss = 2.03 (5061.9 examples/sec; 0.025 sec/batch)
2018-01-30 16:56:59.375970: step 117600, loss = 2.05 (5068.7 examples/sec; 0.025 sec/batch)
2018-01-30 16:57:04.608602: step 117800, loss = 2.07 (4892.4 examples/sec; 0.026 sec/batch)
2018-01-30 16:57:08.320295: step 118000, loss = 2.15 (6897.1 examples/sec; 0.019 sec/batch)
2018-01-30 16:57:11.168666: step 118200, loss = 2.13 (8987.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:57:14.008801: step 118400, loss = 2.06 (9013.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:57:16.879658: step 118600, loss = 2.11 (8917.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:57:21.623643: step 118800, loss = 2.06 (5396.3 examples/sec; 0.024 sec/batch)
2018-01-30 16:57:26.978248: step 119000, loss = 2.18 (4780.9 examples/sec; 0.027 sec/batch)
2018-01-30 16:57:32.337257: step 119200, loss = 1.98 (4777.0 examples/sec; 0.027 sec/batch)
2018-01-30 16:57:35.978080: step 119400, loss = 2.14 (7031.4 examples/sec; 0.018 sec/batch)
2018-01-30 16:57:40.363305: step 119600, loss = 2.03 (5837.8 examples/sec; 0.022 sec/batch)
2018-01-30 16:57:43.227996: step 119800, loss = 1.99 (8936.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:57:47.413561: step 120000, loss = 2.03 (6116.3 examples/sec; 0.021 sec/batch)
2018-01-30 16:57:52.543339: step 120200, loss = 2.11 (4990.5 examples/sec; 0.026 sec/batch)
2018-01-30 16:57:57.688627: step 120400, loss = 2.00 (4975.4 examples/sec; 0.026 sec/batch)
2018-01-30 16:58:02.342006: step 120600, loss = 2.18 (5501.4 examples/sec; 0.023 sec/batch)
2018-01-30 16:58:05.169090: step 120800, loss = 2.12 (9055.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:58:08.062397: step 121000, loss = 2.10 (8848.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:58:10.901470: step 121200, loss = 2.09 (9017.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:58:14.841602: step 121400, loss = 2.00 (6497.2 examples/sec; 0.020 sec/batch)
2018-01-30 16:58:20.141665: step 121600, loss = 2.13 (4830.1 examples/sec; 0.027 sec/batch)
2018-01-30 16:58:25.327330: step 121800, loss = 2.06 (4936.7 examples/sec; 0.026 sec/batch)
2018-01-30 16:58:30.097582: step 122000, loss = 2.07 (5366.6 examples/sec; 0.024 sec/batch)
2018-01-30 16:58:32.933573: step 122200, loss = 2.10 (9026.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:58:35.786621: step 122400, loss = 2.16 (8972.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:58:40.097040: step 122600, loss = 2.03 (5939.1 examples/sec; 0.022 sec/batch)
2018-01-30 16:58:45.184871: step 122800, loss = 2.06 (5031.6 examples/sec; 0.025 sec/batch)
2018-01-30 16:58:50.563029: step 123000, loss = 1.99 (4760.0 examples/sec; 0.027 sec/batch)
2018-01-30 16:58:55.829165: step 123200, loss = 2.06 (4861.3 examples/sec; 0.026 sec/batch)
2018-01-30 16:58:59.232353: step 123400, loss = 2.04 (7522.4 examples/sec; 0.017 sec/batch)
2018-01-30 16:59:02.073548: step 123600, loss = 1.92 (9010.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:59:04.918246: step 123800, loss = 2.09 (8999.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:59:07.771117: step 124000, loss = 2.00 (8973.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:59:12.970746: step 124200, loss = 2.05 (4923.4 examples/sec; 0.026 sec/batch)
2018-01-30 16:59:18.026480: step 124400, loss = 2.06 (5063.6 examples/sec; 0.025 sec/batch)
2018-01-30 16:59:23.355056: step 124600, loss = 2.09 (4804.3 examples/sec; 0.027 sec/batch)
2018-01-30 16:59:26.994055: step 124800, loss = 2.16 (7034.9 examples/sec; 0.018 sec/batch)
2018-01-30 16:59:29.857134: step 125000, loss = 1.97 (8941.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:59:32.728779: step 125200, loss = 2.20 (8914.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:59:35.602658: step 125400, loss = 2.06 (8907.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:59:48.336662: step 125600, loss = 2.14 (2010.4 examples/sec; 0.064 sec/batch)
2018-01-30 16:59:51.169792: step 125800, loss = 2.05 (9035.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:59:54.031427: step 126000, loss = 2.14 (8945.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:59:57.486081: step 126200, loss = 2.03 (7410.3 examples/sec; 0.017 sec/batch)
2018-01-30 17:00:02.757516: step 126400, loss = 2.04 (4856.4 examples/sec; 0.026 sec/batch)
2018-01-30 17:00:08.043243: step 126600, loss = 2.10 (4843.2 examples/sec; 0.026 sec/batch)
2018-01-30 17:00:13.126499: step 126800, loss = 2.05 (5036.1 examples/sec; 0.025 sec/batch)
2018-01-30 17:00:16.006093: step 127000, loss = 2.07 (8890.1 examples/sec; 0.014 sec/batch)
2018-01-30 17:00:18.878038: step 127200, loss = 2.09 (8913.8 examples/sec; 0.014 sec/batch)
2018-01-30 17:00:21.724849: step 127400, loss = 2.16 (8992.5 examples/sec; 0.014 sec/batch)
2018-01-30 17:00:25.597442: step 127600, loss = 2.08 (6610.6 examples/sec; 0.019 sec/batch)
2018-01-30 17:00:31.042553: step 127800, loss = 2.04 (4701.5 examples/sec; 0.027 sec/batch)
2018-01-30 17:00:39.825716: step 128000, loss = 2.01 (2914.7 examples/sec; 0.044 sec/batch)
2018-01-30 17:00:42.674501: step 128200, loss = 2.08 (8986.3 examples/sec; 0.014 sec/batch)
2018-01-30 17:00:45.544688: step 128400, loss = 2.00 (8919.3 examples/sec; 0.014 sec/batch)
2018-01-30 17:00:48.381851: step 128600, loss = 2.07 (9023.1 examples/sec; 0.014 sec/batch)
2018-01-30 17:00:52.740802: step 128800, loss = 2.09 (5873.0 examples/sec; 0.022 sec/batch)
2018-01-30 17:00:57.757006: step 129000, loss = 2.03 (5103.5 examples/sec; 0.025 sec/batch)
2018-01-30 17:01:03.103158: step 129200, loss = 2.04 (4788.5 examples/sec; 0.027 sec/batch)
2018-01-30 17:01:07.457388: step 129400, loss = 2.16 (5879.3 examples/sec; 0.022 sec/batch)
2018-01-30 17:01:10.324197: step 129600, loss = 2.07 (8929.8 examples/sec; 0.014 sec/batch)
2018-01-30 17:01:13.150286: step 129800, loss = 2.13 (9058.5 examples/sec; 0.014 sec/batch)
2018-01-30 17:01:16.016089: step 130000, loss = 2.06 (8932.9 examples/sec; 0.014 sec/batch)
2018-01-30 17:01:20.093597: step 130200, loss = 2.12 (6278.3 examples/sec; 0.020 sec/batch)
2018-01-30 17:01:25.192984: step 130400, loss = 2.09 (5020.2 examples/sec; 0.025 sec/batch)
2018-01-30 17:01:30.602002: step 130600, loss = 2.02 (4732.8 examples/sec; 0.027 sec/batch)
2018-01-30 17:01:35.154125: step 130800, loss = 2.13 (5623.7 examples/sec; 0.023 sec/batch)
2018-01-30 17:01:39.469539: step 131000, loss = 2.10 (5932.2 examples/sec; 0.022 sec/batch)
2018-01-30 17:01:42.339081: step 131200, loss = 2.11 (8921.3 examples/sec; 0.014 sec/batch)
2018-01-30 17:01:45.198377: step 131400, loss = 2.09 (8953.3 examples/sec; 0.014 sec/batch)
2018-01-30 17:01:50.598757: step 131600, loss = 2.13 (4740.4 examples/sec; 0.027 sec/batch)
2018-01-30 17:01:55.726454: step 131800, loss = 2.11 (4992.5 examples/sec; 0.026 sec/batch)
2018-01-30 17:02:00.885715: step 132000, loss = 2.12 (4962.0 examples/sec; 0.026 sec/batch)
2018-01-30 17:02:04.310621: step 132200, loss = 2.03 (7474.7 examples/sec; 0.017 sec/batch)
2018-01-30 17:02:07.126932: step 132400, loss = 2.00 (9089.9 examples/sec; 0.014 sec/batch)
2018-01-30 17:02:09.972188: step 132600, loss = 1.99 (8997.4 examples/sec; 0.014 sec/batch)
2018-01-30 17:02:12.829697: step 132800, loss = 2.11 (8958.9 examples/sec; 0.014 sec/batch)
2018-01-30 17:02:17.994923: step 133000, loss = 2.16 (4956.2 examples/sec; 0.026 sec/batch)
2018-01-30 17:02:23.090955: step 133200, loss = 2.19 (5023.5 examples/sec; 0.025 sec/batch)
2018-01-30 17:02:28.141957: step 133400, loss = 2.12 (5068.3 examples/sec; 0.025 sec/batch)
2018-01-30 17:02:31.944738: step 133600, loss = 2.13 (6731.9 examples/sec; 0.019 sec/batch)
2018-01-30 17:02:34.810944: step 133800, loss = 2.06 (8931.7 examples/sec; 0.014 sec/batch)
2018-01-30 17:02:39.202134: step 134000, loss = 2.01 (5829.9 examples/sec; 0.022 sec/batch)
2018-01-30 17:02:42.738362: step 134200, loss = 2.05 (7239.4 examples/sec; 0.018 sec/batch)
2018-01-30 17:02:47.871310: step 134400, loss = 2.06 (4987.4 examples/sec; 0.026 sec/batch)
2018-01-30 17:02:53.146635: step 134600, loss = 2.08 (4852.8 examples/sec; 0.026 sec/batch)
2018-01-30 17:02:58.161328: step 134800, loss = 2.04 (5105.0 examples/sec; 0.025 sec/batch)
2018-01-30 17:03:01.174823: step 135000, loss = 2.03 (8495.1 examples/sec; 0.015 sec/batch)
2018-01-30 17:03:04.016778: step 135200, loss = 2.02 (9007.9 examples/sec; 0.014 sec/batch)
2018-01-30 17:03:06.866655: step 135400, loss = 2.02 (8982.8 examples/sec; 0.014 sec/batch)
2018-01-30 17:03:10.202135: step 135600, loss = 2.13 (7675.1 examples/sec; 0.017 sec/batch)
2018-01-30 17:03:15.350491: step 135800, loss = 2.06 (4972.5 examples/sec; 0.026 sec/batch)
2018-01-30 17:03:20.430614: step 136000, loss = 2.03 (5039.2 examples/sec; 0.025 sec/batch)
2018-01-30 17:03:25.983134: step 136200, loss = 2.13 (4610.5 examples/sec; 0.028 sec/batch)
2018-01-30 17:03:29.341374: step 136400, loss = 2.10 (7623.0 examples/sec; 0.017 sec/batch)
2018-01-30 17:03:32.248883: step 136600, loss = 2.06 (8804.8 examples/sec; 0.015 sec/batch)
2018-01-30 17:03:35.095783: step 136800, loss = 2.12 (8992.2 examples/sec; 0.014 sec/batch)
2018-01-30 17:03:47.838804: step 137000, loss = 1.97 (2008.9 examples/sec; 0.064 sec/batch)
2018-01-30 17:03:50.689682: step 137200, loss = 2.17 (8979.7 examples/sec; 0.014 sec/batch)
2018-01-30 17:03:53.539534: step 137400, loss = 2.04 (8982.9 examples/sec; 0.014 sec/batch)
2018-01-30 17:03:56.417341: step 137600, loss = 2.10 (8895.7 examples/sec; 0.014 sec/batch)
2018-01-30 17:04:00.037442: step 137800, loss = 2.01 (7071.6 examples/sec; 0.018 sec/batch)
2018-01-30 17:04:05.026135: step 138000, loss = 2.07 (5131.6 examples/sec; 0.025 sec/batch)
2018-01-30 17:04:10.323503: step 138200, loss = 2.11 (4832.6 examples/sec; 0.026 sec/batch)
2018-01-30 17:04:15.594242: step 138400, loss = 1.93 (4857.0 examples/sec; 0.026 sec/batch)
2018-01-30 17:04:18.428134: step 138600, loss = 1.98 (9033.5 examples/sec; 0.014 sec/batch)
2018-01-30 17:04:21.301435: step 138800, loss = 2.06 (8909.6 examples/sec; 0.014 sec/batch)
2018-01-30 17:04:24.141401: step 139000, loss = 1.98 (9014.2 examples/sec; 0.014 sec/batch)
2018-01-30 17:04:27.647191: step 139200, loss = 1.98 (7302.2 examples/sec; 0.018 sec/batch)
2018-01-30 17:04:32.860515: step 139400, loss = 2.04 (4910.5 examples/sec; 0.026 sec/batch)
2018-01-30 17:04:42.224651: step 139600, loss = 2.12 (2733.8 examples/sec; 0.047 sec/batch)
2018-01-30 17:04:45.088954: step 139800, loss = 2.11 (8937.6 examples/sec; 0.014 sec/batch)
2018-01-30 17:04:47.953331: step 140000, loss = 2.06 (8937.4 examples/sec; 0.014 sec/batch)
2018-01-30 17:04:50.833096: step 140200, loss = 2.03 (8889.6 examples/sec; 0.014 sec/batch)
2018-01-30 17:04:55.920447: step 140400, loss = 2.16 (5032.1 examples/sec; 0.025 sec/batch)
2018-01-30 17:05:01.095233: step 140600, loss = 2.12 (4947.1 examples/sec; 0.026 sec/batch)
2018-01-30 17:05:06.333210: step 140800, loss = 2.13 (4887.4 examples/sec; 0.026 sec/batch)
2018-01-30 17:05:09.883819: step 141000, loss = 2.20 (7210.0 examples/sec; 0.018 sec/batch)
2018-01-30 17:05:12.746450: step 141200, loss = 2.02 (8942.8 examples/sec; 0.014 sec/batch)
2018-01-30 17:05:15.609694: step 141400, loss = 1.96 (8940.9 examples/sec; 0.014 sec/batch)
2018-01-30 17:05:18.464457: step 141600, loss = 2.02 (8967.5 examples/sec; 0.014 sec/batch)
2018-01-30 17:05:23.183167: step 141800, loss = 2.10 (5425.2 examples/sec; 0.024 sec/batch)
2018-01-30 17:05:28.377958: step 142000, loss = 2.00 (4928.0 examples/sec; 0.026 sec/batch)
2018-01-30 17:05:33.989780: step 142200, loss = 2.20 (4561.8 examples/sec; 0.028 sec/batch)
2018-01-30 17:05:39.086169: step 142400, loss = 2.12 (5023.2 examples/sec; 0.025 sec/batch)
2018-01-30 17:05:41.923387: step 142600, loss = 1.97 (9022.9 examples/sec; 0.014 sec/batch)
2018-01-30 17:05:44.782593: step 142800, loss = 2.08 (8953.5 examples/sec; 0.014 sec/batch)
2018-01-30 17:05:48.935459: step 143000, loss = 2.06 (6164.4 examples/sec; 0.021 sec/batch)
2018-01-30 17:05:54.226190: step 143200, loss = 2.00 (4838.7 examples/sec; 0.026 sec/batch)
2018-01-30 17:05:59.473197: step 143400, loss = 2.07 (4879.0 examples/sec; 0.026 sec/batch)
2018-01-30 17:06:04.129960: step 143600, loss = 2.04 (5497.4 examples/sec; 0.023 sec/batch)
2018-01-30 17:06:07.018525: step 143800, loss = 1.98 (8862.5 examples/sec; 0.014 sec/batch)
2018-01-30 17:06:09.869549: step 144000, loss = 2.08 (8979.2 examples/sec; 0.014 sec/batch)
2018-01-30 17:06:12.714523: step 144200, loss = 2.07 (8998.3 examples/sec; 0.014 sec/batch)
2018-01-30 17:06:16.954793: step 144400, loss = 2.05 (6037.4 examples/sec; 0.021 sec/batch)
2018-01-30 17:06:22.157285: step 144600, loss = 2.06 (4920.7 examples/sec; 0.026 sec/batch)
2018-01-30 17:06:27.432543: step 144800, loss = 2.01 (4852.8 examples/sec; 0.026 sec/batch)
2018-01-30 17:06:31.843156: step 145000, loss = 2.02 (5804.2 examples/sec; 0.022 sec/batch)
2018-01-30 17:06:34.682306: step 145200, loss = 2.13 (9016.8 examples/sec; 0.014 sec/batch)
2018-01-30 17:06:39.109897: step 145400, loss = 2.06 (5781.9 examples/sec; 0.022 sec/batch)
2018-01-30 17:06:42.247187: step 145600, loss = 2.06 (8159.9 examples/sec; 0.016 sec/batch)
2018-01-30 17:06:47.603153: step 145800, loss = 2.10 (4779.7 examples/sec; 0.027 sec/batch)
2018-01-30 17:06:52.751442: step 146000, loss = 2.07 (4972.5 examples/sec; 0.026 sec/batch)
2018-01-30 17:06:57.714507: step 146200, loss = 2.10 (5158.1 examples/sec; 0.025 sec/batch)
2018-01-30 17:07:01.029796: step 146400, loss = 2.09 (7721.8 examples/sec; 0.017 sec/batch)
2018-01-30 17:07:03.845563: step 146600, loss = 2.01 (9091.7 examples/sec; 0.014 sec/batch)
2018-01-30 17:07:06.708045: step 146800, loss = 2.03 (8943.3 examples/sec; 0.014 sec/batch)
2018-01-30 17:07:09.560835: step 147000, loss = 2.15 (8973.7 examples/sec; 0.014 sec/batch)
2018-01-30 17:07:14.918868: step 147200, loss = 2.13 (4777.9 examples/sec; 0.027 sec/batch)
2018-01-30 17:07:20.263868: step 147400, loss = 2.13 (4789.5 examples/sec; 0.027 sec/batch)
2018-01-30 17:07:25.260068: step 147600, loss = 1.97 (5123.9 examples/sec; 0.025 sec/batch)
2018-01-30 17:07:28.676671: step 147800, loss = 2.04 (7492.8 examples/sec; 0.017 sec/batch)
2018-01-30 17:07:31.505883: step 148000, loss = 2.09 (9048.5 examples/sec; 0.014 sec/batch)
2018-01-30 17:07:34.337088: step 148200, loss = 2.04 (9042.1 examples/sec; 0.014 sec/batch)
2018-01-30 17:07:47.064301: step 148400, loss = 2.01 (2011.4 examples/sec; 0.064 sec/batch)
2018-01-30 17:07:49.912708: step 148600, loss = 2.15 (8987.5 examples/sec; 0.014 sec/batch)
2018-01-30 17:07:52.778170: step 148800, loss = 2.12 (8934.0 examples/sec; 0.014 sec/batch)
2018-01-30 17:07:55.641197: step 149000, loss = 2.07 (8941.6 examples/sec; 0.014 sec/batch)
2018-01-30 17:07:59.803245: step 149200, loss = 2.05 (6150.8 examples/sec; 0.021 sec/batch)
2018-01-30 17:08:04.979026: step 149400, loss = 2.02 (4946.1 examples/sec; 0.026 sec/batch)
2018-01-30 17:08:10.478696: step 149600, loss = 2.08 (4654.8 examples/sec; 0.027 sec/batch)
2018-01-30 17:08:14.770964: step 149800, loss = 2.11 (5964.2 examples/sec; 0.021 sec/batch)
2018-01-30 17:08:17.628979: step 150000, loss = 2.12 (8957.3 examples/sec; 0.014 sec/batch)
2018-01-30 17:08:20.488875: step 150200, loss = 2.12 (8951.4 examples/sec; 0.014 sec/batch)
2018-01-30 17:08:23.341806: step 150400, loss = 2.04 (8973.2 examples/sec; 0.014 sec/batch)
2018-01-30 17:08:27.846109: step 150600, loss = 2.01 (5683.5 examples/sec; 0.023 sec/batch)
2018-01-30 17:08:32.718819: step 150800, loss = 2.04 (5253.7 examples/sec; 0.024 sec/batch)
2018-01-30 17:08:41.374221: step 151000, loss = 2.02 (2957.7 examples/sec; 0.043 sec/batch)
2018-01-30 17:08:44.198091: step 151200, loss = 2.16 (9065.6 examples/sec; 0.014 sec/batch)
2018-01-30 17:08:47.034459: step 151400, loss = 2.07 (9025.6 examples/sec; 0.014 sec/batch)
2018-01-30 17:08:49.894479: step 151600, loss = 2.11 (8951.0 examples/sec; 0.014 sec/batch)
2018-01-30 17:08:54.720817: step 151800, loss = 2.16 (5304.2 examples/sec; 0.024 sec/batch)
2018-01-30 17:08:59.909510: step 152000, loss = 2.12 (4933.8 examples/sec; 0.026 sec/batch)
2018-01-30 17:09:05.260723: step 152200, loss = 2.09 (4784.0 examples/sec; 0.027 sec/batch)
2018-01-30 17:09:08.975295: step 152400, loss = 2.11 (6891.8 examples/sec; 0.019 sec/batch)
2018-01-30 17:09:11.829494: step 152600, loss = 2.17 (8969.2 examples/sec; 0.014 sec/batch)
2018-01-30 17:09:14.695902: step 152800, loss = 2.07 (8931.0 examples/sec; 0.014 sec/batch)
2018-01-30 17:09:17.526429: step 153000, loss = 2.19 (9044.3 examples/sec; 0.014 sec/batch)
2018-01-30 17:09:22.919037: step 153200, loss = 2.07 (4747.2 examples/sec; 0.027 sec/batch)
2018-01-30 17:09:28.544584: step 153400, loss = 2.09 (4550.7 examples/sec; 0.028 sec/batch)
2018-01-30 17:09:34.063664: step 153600, loss = 2.04 (4638.5 examples/sec; 0.028 sec/batch)
2018-01-30 17:09:39.269478: step 153800, loss = 2.04 (4917.6 examples/sec; 0.026 sec/batch)
2018-01-30 17:09:42.170141: step 154000, loss = 2.13 (8825.6 examples/sec; 0.015 sec/batch)
2018-01-30 17:09:45.049454: step 154200, loss = 2.14 (8891.0 examples/sec; 0.014 sec/batch)
2018-01-30 17:09:50.365403: step 154400, loss = 2.13 (4815.7 examples/sec; 0.027 sec/batch)
2018-01-30 17:09:55.729533: step 154600, loss = 2.09 (4772.4 examples/sec; 0.027 sec/batch)
2018-01-30 17:10:01.043687: step 154800, loss = 2.09 (4817.3 examples/sec; 0.027 sec/batch)
2018-01-30 17:10:04.319049: step 155000, loss = 2.12 (7815.9 examples/sec; 0.016 sec/batch)
2018-01-30 17:10:07.175250: step 155200, loss = 2.07 (8963.0 examples/sec; 0.014 sec/batch)
2018-01-30 17:10:10.055920: step 155400, loss = 1.95 (8886.8 examples/sec; 0.014 sec/batch)
2018-01-30 17:10:12.921276: step 155600, loss = 2.01 (8934.3 examples/sec; 0.014 sec/batch)
2018-01-30 17:10:18.288800: step 155800, loss = 2.05 (4769.4 examples/sec; 0.027 sec/batch)
2018-01-30 17:10:23.499376: step 156000, loss = 2.00 (4913.1 examples/sec; 0.026 sec/batch)
2018-01-30 17:10:28.603356: step 156200, loss = 2.04 (5015.7 examples/sec; 0.026 sec/batch)
2018-01-30 17:10:32.129573: step 156400, loss = 2.00 (7259.9 examples/sec; 0.018 sec/batch)
2018-01-30 17:10:35.004807: step 156600, loss = 1.97 (8903.6 examples/sec; 0.014 sec/batch)
2018-01-30 17:10:39.334592: step 156800, loss = 1.98 (5912.5 examples/sec; 0.022 sec/batch)
2018-01-30 17:10:43.495885: step 157000, loss = 2.07 (6151.9 examples/sec; 0.021 sec/batch)
2018-01-30 17:10:48.971071: step 157200, loss = 1.99 (4675.6 examples/sec; 0.027 sec/batch)
2018-01-30 17:10:54.271146: step 157400, loss = 2.01 (4830.1 examples/sec; 0.027 sec/batch)
2018-01-30 17:10:58.538284: step 157600, loss = 2.21 (5999.3 examples/sec; 0.021 sec/batch)
2018-01-30 17:11:01.409480: step 157800, loss = 2.06 (8916.1 examples/sec; 0.014 sec/batch)
2018-01-30 17:11:04.235724: step 158000, loss = 2.10 (9058.0 examples/sec; 0.014 sec/batch)
2018-01-30 17:11:07.097259: step 158200, loss = 2.02 (8946.2 examples/sec; 0.014 sec/batch)
2018-01-30 17:11:11.480863: step 158400, loss = 2.06 (5839.9 examples/sec; 0.022 sec/batch)
2018-01-30 17:11:16.502260: step 158600, loss = 1.97 (5098.2 examples/sec; 0.025 sec/batch)
2018-01-30 17:11:21.861491: step 158800, loss = 2.03 (4776.8 examples/sec; 0.027 sec/batch)
2018-01-30 17:11:26.295454: step 159000, loss = 2.06 (5773.6 examples/sec; 0.022 sec/batch)
2018-01-30 17:11:29.116247: step 159200, loss = 2.09 (9075.5 examples/sec; 0.014 sec/batch)
2018-01-30 17:11:31.952736: step 159400, loss = 2.06 (9025.2 examples/sec; 0.014 sec/batch)
2018-01-30 17:11:34.785804: step 159600, loss = 2.08 (9036.1 examples/sec; 0.014 sec/batch)
2018-01-30 17:11:47.558270: step 159800, loss = 2.08 (2004.3 examples/sec; 0.064 sec/batch)
2018-01-30 17:11:50.433298: step 160000, loss = 1.99 (8904.3 examples/sec; 0.014 sec/batch)
2018-01-30 17:11:53.293929: step 160200, loss = 2.02 (8949.1 examples/sec; 0.014 sec/batch)
2018-01-30 17:11:56.150371: step 160400, loss = 2.15 (8962.2 examples/sec; 0.014 sec/batch)
2018-01-30 17:12:01.380695: step 160600, loss = 2.07 (4894.5 examples/sec; 0.026 sec/batch)
2018-01-30 17:12:06.721571: step 160800, loss = 1.92 (4793.2 examples/sec; 0.027 sec/batch)
2018-01-30 17:12:12.037322: step 161000, loss = 2.02 (4815.9 examples/sec; 0.027 sec/batch)
2018-01-30 17:12:15.383153: step 161200, loss = 2.06 (7651.3 examples/sec; 0.017 sec/batch)
2018-01-30 17:12:18.243418: step 161400, loss = 2.08 (8950.2 examples/sec; 0.014 sec/batch)
2018-01-30 17:12:21.084729: step 161600, loss = 2.03 (9009.9 examples/sec; 0.014 sec/batch)
2018-01-30 17:12:24.133848: step 161800, loss = 2.06 (8395.9 examples/sec; 0.015 sec/batch)
2018-01-30 17:12:29.516898: step 162000, loss = 2.09 (4755.7 examples/sec; 0.027 sec/batch)
2018-01-30 17:12:35.008120: step 162200, loss = 2.17 (4662.0 examples/sec; 0.027 sec/batch)
2018-01-30 17:12:42.073215: step 162400, loss = 2.01 (3623.4 examples/sec; 0.035 sec/batch)
2018-01-30 17:12:44.914612: step 162600, loss = 2.01 (9009.7 examples/sec; 0.014 sec/batch)
2018-01-30 17:12:47.766730: step 162800, loss = 2.02 (8975.8 examples/sec; 0.014 sec/batch)
2018-01-30 17:12:51.607097: step 163000, loss = 2.02 (6666.0 examples/sec; 0.019 sec/batch)
2018-01-30 17:12:56.901842: step 163200, loss = 2.05 (4835.0 examples/sec; 0.026 sec/batch)
2018-01-30 17:13:02.691436: step 163400, loss = 2.21 (4421.7 examples/sec; 0.029 sec/batch)
2018-01-30 17:13:07.039354: step 163600, loss = 1.93 (5887.9 examples/sec; 0.022 sec/batch)
2018-01-30 17:13:09.881134: step 163800, loss = 2.09 (9008.4 examples/sec; 0.014 sec/batch)
2018-01-30 17:13:12.732253: step 164000, loss = 2.04 (8978.9 examples/sec; 0.014 sec/batch)
2018-01-30 17:13:15.583944: step 164200, loss = 1.91 (8977.1 examples/sec; 0.014 sec/batch)
2018-01-30 17:13:19.963447: step 164400, loss = 2.09 (5845.4 examples/sec; 0.022 sec/batch)
2018-01-30 17:13:25.035850: step 164600, loss = 2.15 (5046.9 examples/sec; 0.025 sec/batch)
2018-01-30 17:13:30.345451: step 164800, loss = 2.10 (4821.5 examples/sec; 0.027 sec/batch)
2018-01-30 17:13:34.833470: step 165000, loss = 2.14 (5704.1 examples/sec; 0.022 sec/batch)
2018-01-30 17:13:39.161876: step 165200, loss = 2.05 (5914.4 examples/sec; 0.022 sec/batch)
2018-01-30 17:13:42.003251: step 165400, loss = 2.16 (9009.7 examples/sec; 0.014 sec/batch)
2018-01-30 17:13:45.187384: step 165600, loss = 2.02 (8039.9 examples/sec; 0.016 sec/batch)
2018-01-30 17:13:50.296287: step 165800, loss = 2.13 (5010.9 examples/sec; 0.026 sec/batch)
2018-01-30 17:13:55.321817: step 166000, loss = 1.95 (5094.0 examples/sec; 0.025 sec/batch)
2018-01-30 17:14:00.366912: step 166200, loss = 2.12 (5074.2 examples/sec; 0.025 sec/batch)
2018-01-30 17:14:03.966706: step 166400, loss = 2.01 (7111.5 examples/sec; 0.018 sec/batch)
2018-01-30 17:14:06.810985: step 166600, loss = 2.01 (9000.5 examples/sec; 0.014 sec/batch)
2018-01-30 17:14:09.665130: step 166800, loss = 2.10 (8969.4 examples/sec; 0.014 sec/batch)
2018-01-30 17:14:12.541006: step 167000, loss = 2.07 (8901.6 examples/sec; 0.014 sec/batch)
2018-01-30 17:14:17.620292: step 167200, loss = 2.06 (5040.1 examples/sec; 0.025 sec/batch)
2018-01-30 17:14:22.931655: step 167400, loss = 2.04 (4819.9 examples/sec; 0.027 sec/batch)
2018-01-30 17:14:27.838155: step 167600, loss = 2.01 (5217.6 examples/sec; 0.025 sec/batch)
2018-01-30 17:14:31.686910: step 167800, loss = 2.18 (6651.5 examples/sec; 0.019 sec/batch)
2018-01-30 17:14:34.534660: step 168000, loss = 2.00 (8989.6 examples/sec; 0.014 sec/batch)
2018-01-30 17:14:38.861347: step 168200, loss = 2.00 (5916.8 examples/sec; 0.022 sec/batch)
2018-01-30 17:14:42.654619: step 168400, loss = 2.05 (6748.8 examples/sec; 0.019 sec/batch)
2018-01-30 17:14:47.733062: step 168600, loss = 2.10 (5040.9 examples/sec; 0.025 sec/batch)
2018-01-30 17:14:53.127626: step 168800, loss = 2.05 (4745.5 examples/sec; 0.027 sec/batch)
2018-01-30 17:14:57.987972: step 169000, loss = 1.97 (5267.1 examples/sec; 0.024 sec/batch)
2018-01-30 17:15:00.856139: step 169200, loss = 2.06 (8925.6 examples/sec; 0.014 sec/batch)
2018-01-30 17:15:03.686176: step 169400, loss = 2.15 (9045.8 examples/sec; 0.014 sec/batch)
2018-01-30 17:15:06.521754: step 169600, loss = 2.01 (9028.1 examples/sec; 0.014 sec/batch)
2018-01-30 17:15:10.456824: step 169800, loss = 2.13 (6505.6 examples/sec; 0.020 sec/batch)
2018-01-30 17:15:15.684332: step 170000, loss = 1.98 (4897.2 examples/sec; 0.026 sec/batch)
2018-01-30 17:15:20.825372: step 170200, loss = 2.07 (4979.5 examples/sec; 0.026 sec/batch)
2018-01-30 17:15:25.673575: step 170400, loss = 2.11 (5280.3 examples/sec; 0.024 sec/batch)
2018-01-30 17:15:28.531388: step 170600, loss = 2.14 (8957.9 examples/sec; 0.014 sec/batch)
2018-01-30 17:15:31.399554: step 170800, loss = 2.02 (8925.6 examples/sec; 0.014 sec/batch)
2018-01-30 17:15:34.265604: step 171000, loss = 2.11 (8932.2 examples/sec; 0.014 sec/batch)
2018-01-30 17:15:47.186033: step 171200, loss = 2.08 (1981.4 examples/sec; 0.065 sec/batch)
2018-01-30 17:15:50.044954: step 171400, loss = 2.05 (8954.4 examples/sec; 0.014 sec/batch)
2018-01-30 17:15:52.908757: step 171600, loss = 1.93 (8939.2 examples/sec; 0.014 sec/batch)
2018-01-30 17:15:55.757097: step 171800, loss = 1.98 (8987.7 examples/sec; 0.014 sec/batch)
2018-01-30 17:16:00.782921: step 172000, loss = 2.12 (5093.7 examples/sec; 0.025 sec/batch)
2018-01-30 17:16:05.975385: step 172200, loss = 2.09 (4930.2 examples/sec; 0.026 sec/batch)
2018-01-30 17:16:11.457223: step 172400, loss = 2.01 (4670.0 examples/sec; 0.027 sec/batch)
2018-01-30 17:16:15.295383: step 172600, loss = 2.07 (6669.9 examples/sec; 0.019 sec/batch)
2018-01-30 17:16:18.170098: step 172800, loss = 2.09 (8905.2 examples/sec; 0.014 sec/batch)
2018-01-30 17:16:21.025045: step 173000, loss = 2.09 (8966.9 examples/sec; 0.014 sec/batch)
2018-01-30 17:16:23.890794: step 173200, loss = 2.07 (8933.1 examples/sec; 0.014 sec/batch)
2018-01-30 17:16:28.609409: step 173400, loss = 2.00 (5425.3 examples/sec; 0.024 sec/batch)
2018-01-30 17:16:33.728792: step 173600, loss = 2.04 (5000.6 examples/sec; 0.026 sec/batch)
2018-01-30 17:16:42.008903: step 173800, loss = 2.04 (3091.7 examples/sec; 0.041 sec/batch)
2018-01-30 17:16:44.937743: step 174000, loss = 2.02 (8740.7 examples/sec; 0.015 sec/batch)
2018-01-30 17:16:49.196327: step 174200, loss = 2.07 (6011.4 examples/sec; 0.021 sec/batch)
2018-01-30 17:29:38.396016: step 174400, loss = 2.06 (33.3 examples/sec; 3.846 sec/batch)
2018-01-30 17:29:42.005960: step 174600, loss = 2.01 (7091.5 examples/sec; 0.018 sec/batch)
2018-01-30 17:29:45.153345: step 174800, loss = 2.05 (8133.7 examples/sec; 0.016 sec/batch)
2018-01-30 17:29:48.365590: step 175000, loss = 2.10 (7969.5 examples/sec; 0.016 sec/batch)
2018-01-30 17:29:54.410630: step 175200, loss = 2.00 (4234.9 examples/sec; 0.030 sec/batch)
2018-01-30 17:30:00.890370: step 175400, loss = 2.08 (3950.8 examples/sec; 0.032 sec/batch)
2018-01-30 17:30:06.739929: step 175600, loss = 2.09 (4376.4 examples/sec; 0.029 sec/batch)
2018-01-30 17:30:10.936729: step 175800, loss = 2.15 (6099.9 examples/sec; 0.021 sec/batch)
2018-01-30 17:30:14.224729: step 176000, loss = 2.12 (7785.9 examples/sec; 0.016 sec/batch)
2018-01-30 17:30:17.399015: step 176200, loss = 2.15 (8064.8 examples/sec; 0.016 sec/batch)
2018-01-30 17:30:21.362079: step 176400, loss = 2.07 (6459.6 examples/sec; 0.020 sec/batch)
2018-01-30 17:30:34.944651: step 176600, loss = 2.14 (1884.8 examples/sec; 0.068 sec/batch)
2018-01-30 17:30:37.817037: step 176800, loss = 2.07 (8912.5 examples/sec; 0.014 sec/batch)
2018-01-30 17:30:40.750073: step 177000, loss = 2.02 (8728.2 examples/sec; 0.015 sec/batch)
2018-01-30 17:30:43.682656: step 177200, loss = 2.02 (8729.5 examples/sec; 0.015 sec/batch)
2018-01-30 17:30:48.155799: step 177400, loss = 2.08 (5723.0 examples/sec; 0.022 sec/batch)
2018-01-30 17:30:53.465521: step 177600, loss = 2.09 (4821.3 examples/sec; 0.027 sec/batch)
2018-01-30 17:30:58.729931: step 177800, loss = 2.04 (4862.8 examples/sec; 0.026 sec/batch)
2018-01-30 17:31:03.228950: step 178000, loss = 2.10 (5690.1 examples/sec; 0.022 sec/batch)
2018-01-30 17:31:06.188233: step 178200, loss = 2.08 (8650.7 examples/sec; 0.015 sec/batch)
2018-01-30 17:31:09.104503: step 178400, loss = 2.13 (8778.3 examples/sec; 0.015 sec/batch)
2018-01-30 17:31:12.079834: step 178600, loss = 2.09 (8604.1 examples/sec; 0.015 sec/batch)
2018-01-30 17:31:16.555380: step 178800, loss = 2.07 (5720.0 examples/sec; 0.022 sec/batch)
2018-01-30 17:31:21.592611: step 179000, loss = 2.24 (5082.2 examples/sec; 0.025 sec/batch)
2018-01-30 17:31:26.947443: step 179200, loss = 2.11 (4780.7 examples/sec; 0.027 sec/batch)
2018-01-30 17:31:33.311692: step 179400, loss = 2.10 (4022.5 examples/sec; 0.032 sec/batch)
2018-01-30 17:31:36.229744: step 179600, loss = 2.06 (8773.0 examples/sec; 0.015 sec/batch)
2018-01-30 17:31:39.180161: step 179800, loss = 2.12 (8676.7 examples/sec; 0.015 sec/batch)
2018-01-30 17:31:43.538904: step 180000, loss = 2.02 (5873.3 examples/sec; 0.022 sec/batch)
2018-01-30 17:31:48.988997: step 180200, loss = 2.13 (4697.2 examples/sec; 0.027 sec/batch)
2018-01-30 17:31:54.242168: step 180400, loss = 2.00 (4873.2 examples/sec; 0.026 sec/batch)
2018-01-30 17:31:58.458857: step 180600, loss = 2.08 (6071.1 examples/sec; 0.021 sec/batch)
2018-01-30 17:32:01.397337: step 180800, loss = 2.02 (8712.0 examples/sec; 0.015 sec/batch)
2018-01-30 17:32:04.349136: step 181000, loss = 2.04 (8684.4 examples/sec; 0.015 sec/batch)
2018-01-30 17:32:07.328593: step 181200, loss = 2.06 (8580.7 examples/sec; 0.015 sec/batch)
2018-01-30 17:32:11.989769: step 181400, loss = 2.02 (5492.2 examples/sec; 0.023 sec/batch)
2018-01-30 17:32:17.086718: step 181600, loss = 2.15 (5022.6 examples/sec; 0.025 sec/batch)
2018-01-30 17:32:22.316874: step 181800, loss = 2.12 (4894.7 examples/sec; 0.026 sec/batch)
2018-01-30 17:32:26.539659: step 182000, loss = 2.01 (6062.3 examples/sec; 0.021 sec/batch)
2018-01-30 17:32:31.572409: step 182200, loss = 2.00 (5086.7 examples/sec; 0.025 sec/batch)
2018-01-30 17:32:34.585056: step 182400, loss = 2.14 (8497.5 examples/sec; 0.015 sec/batch)
2018-01-30 17:32:38.934114: step 182600, loss = 2.03 (5886.3 examples/sec; 0.022 sec/batch)
2018-01-30 17:32:44.374112: step 182800, loss = 2.06 (4705.9 examples/sec; 0.027 sec/batch)
2018-01-30 17:32:49.623155: step 183000, loss = 2.09 (4877.1 examples/sec; 0.026 sec/batch)
2018-01-30 17:32:54.154953: step 183200, loss = 2.05 (5649.0 examples/sec; 0.023 sec/batch)
2018-01-30 17:32:57.074533: step 183400, loss = 2.05 (8768.4 examples/sec; 0.015 sec/batch)
2018-01-30 17:32:59.992492: step 183600, loss = 2.01 (8773.3 examples/sec; 0.015 sec/batch)
2018-01-30 17:33:02.943751: step 183800, loss = 2.02 (8674.3 examples/sec; 0.015 sec/batch)
2018-01-30 17:33:07.330250: step 184000, loss = 1.91 (5836.1 examples/sec; 0.022 sec/batch)
2018-01-30 17:33:12.420682: step 184200, loss = 2.07 (5029.0 examples/sec; 0.025 sec/batch)
2018-01-30 17:33:17.458082: step 184400, loss = 2.00 (5082.0 examples/sec; 0.025 sec/batch)
2018-01-30 17:33:22.272394: step 184600, loss = 1.97 (5317.5 examples/sec; 0.024 sec/batch)
2018-01-30 17:33:25.206954: step 184800, loss = 2.01 (8723.6 examples/sec; 0.015 sec/batch)
2018-01-30 17:33:30.157246: step 185000, loss = 2.10 (5171.4 examples/sec; 0.025 sec/batch)
2018-01-30 17:33:33.801373: step 185200, loss = 2.13 (7025.0 examples/sec; 0.018 sec/batch)
2018-01-30 17:33:39.097907: step 185400, loss = 2.10 (4833.3 examples/sec; 0.026 sec/batch)
2018-01-30 17:33:44.368743: step 185600, loss = 2.01 (4856.9 examples/sec; 0.026 sec/batch)
2018-01-30 17:33:49.469492: step 185800, loss = 2.04 (5018.9 examples/sec; 0.026 sec/batch)
2018-01-30 17:33:52.513590: step 186000, loss = 2.07 (8409.7 examples/sec; 0.015 sec/batch)
2018-01-30 17:33:55.449114: step 186200, loss = 2.06 (8720.8 examples/sec; 0.015 sec/batch)
2018-01-30 17:33:58.373588: step 186400, loss = 1.99 (8753.7 examples/sec; 0.015 sec/batch)
2018-01-30 17:34:01.806516: step 186600, loss = 1.92 (7457.2 examples/sec; 0.017 sec/batch)
2018-01-30 17:34:07.199575: step 186800, loss = 2.00 (4746.8 examples/sec; 0.027 sec/batch)
2018-01-30 17:34:12.996851: step 187000, loss = 2.09 (4415.9 examples/sec; 0.029 sec/batch)
2018-01-30 17:34:17.934666: step 187200, loss = 2.13 (5184.5 examples/sec; 0.025 sec/batch)
2018-01-30 17:34:20.881856: step 187400, loss = 2.18 (8686.2 examples/sec; 0.015 sec/batch)
2018-01-30 17:34:23.812158: step 187600, loss = 2.06 (8736.3 examples/sec; 0.015 sec/batch)
2018-01-30 17:34:26.793198: step 187800, loss = 2.07 (8587.6 examples/sec; 0.015 sec/batch)
2018-01-30 17:34:40.026049: step 188000, loss = 2.04 (1934.6 examples/sec; 0.066 sec/batch)
2018-01-30 17:34:42.948235: step 188200, loss = 2.07 (8760.6 examples/sec; 0.015 sec/batch)
2018-01-30 17:34:45.870421: step 188400, loss = 2.05 (8760.6 examples/sec; 0.015 sec/batch)
2018-01-30 17:34:49.292662: step 188600, loss = 2.13 (7480.5 examples/sec; 0.017 sec/batch)
2018-01-30 17:34:54.536409: step 188800, loss = 2.06 (4882.0 examples/sec; 0.026 sec/batch)
2018-01-30 17:34:59.943236: step 189000, loss = 2.22 (4734.8 examples/sec; 0.027 sec/batch)
2018-01-30 17:35:04.896891: step 189200, loss = 2.05 (5167.9 examples/sec; 0.025 sec/batch)
2018-01-30 17:35:08.178490: step 189400, loss = 2.04 (7801.1 examples/sec; 0.016 sec/batch)
2018-01-30 17:35:11.350990: step 189600, loss = 2.01 (8069.3 examples/sec; 0.016 sec/batch)
2018-01-30 17:35:14.816326: step 189800, loss = 2.03 (7387.5 examples/sec; 0.017 sec/batch)
2018-01-30 17:35:18.736082: step 190000, loss = 2.09 (6531.0 examples/sec; 0.020 sec/batch)
2018-01-30 17:35:24.155667: step 190200, loss = 2.15 (4723.6 examples/sec; 0.027 sec/batch)
2018-01-30 17:35:33.328699: step 190400, loss = 2.16 (2790.8 examples/sec; 0.046 sec/batch)
2018-01-30 17:35:36.262896: step 190600, loss = 2.20 (8724.7 examples/sec; 0.015 sec/batch)
2018-01-30 17:35:39.201711: step 190800, loss = 2.14 (8711.0 examples/sec; 0.015 sec/batch)
2018-01-30 17:35:42.297236: step 191000, loss = 1.96 (8270.0 examples/sec; 0.015 sec/batch)
2018-01-30 17:35:47.962054: step 191200, loss = 1.98 (4519.1 examples/sec; 0.028 sec/batch)
2018-01-30 17:35:53.621755: step 191400, loss = 2.09 (4523.2 examples/sec; 0.028 sec/batch)
2018-01-30 17:35:58.762928: step 191600, loss = 2.04 (4979.4 examples/sec; 0.026 sec/batch)
2018-01-30 17:36:01.799857: step 191800, loss = 2.06 (8429.6 examples/sec; 0.015 sec/batch)
2018-01-30 17:36:04.666154: step 192000, loss = 2.10 (8931.4 examples/sec; 0.014 sec/batch)
2018-01-30 17:36:07.542570: step 192200, loss = 2.16 (8900.0 examples/sec; 0.014 sec/batch)
2018-01-30 17:36:10.923534: step 192400, loss = 2.15 (7571.8 examples/sec; 0.017 sec/batch)
2018-01-30 17:36:16.150466: step 192600, loss = 2.04 (4897.7 examples/sec; 0.026 sec/batch)
2018-01-30 17:36:21.380621: step 192800, loss = 2.10 (4894.7 examples/sec; 0.026 sec/batch)
2018-01-30 17:36:26.838036: step 193000, loss = 2.07 (4690.9 examples/sec; 0.027 sec/batch)
2018-01-30 17:36:31.326548: step 193200, loss = 1.97 (5703.4 examples/sec; 0.022 sec/batch)
2018-01-30 17:36:34.217481: step 193400, loss = 1.99 (8855.3 examples/sec; 0.014 sec/batch)
2018-01-30 17:36:37.092657: step 193600, loss = 2.16 (8903.8 examples/sec; 0.014 sec/batch)
2018-01-30 17:36:41.893035: step 193800, loss = 2.05 (5332.9 examples/sec; 0.024 sec/batch)
2018-01-30 17:36:47.271181: step 194000, loss = 2.03 (4760.0 examples/sec; 0.027 sec/batch)
2018-01-30 17:36:52.163077: step 194200, loss = 2.14 (5233.1 examples/sec; 0.024 sec/batch)
2018-01-30 17:36:56.319769: step 194400, loss = 2.06 (6158.7 examples/sec; 0.021 sec/batch)
2018-01-30 17:36:59.163823: step 194600, loss = 2.05 (9001.2 examples/sec; 0.014 sec/batch)
2018-01-30 17:37:02.023503: step 194800, loss = 2.14 (8952.1 examples/sec; 0.014 sec/batch)
2018-01-30 17:37:04.872259: step 195000, loss = 2.03 (8986.4 examples/sec; 0.014 sec/batch)
2018-01-30 17:37:09.691101: step 195200, loss = 2.06 (5312.5 examples/sec; 0.024 sec/batch)
2018-01-30 17:37:14.985724: step 195400, loss = 2.06 (4835.1 examples/sec; 0.026 sec/batch)
2018-01-30 17:37:20.140133: step 195600, loss = 2.09 (4966.6 examples/sec; 0.026 sec/batch)
2018-01-30 17:37:24.049797: step 195800, loss = 2.15 (6547.9 examples/sec; 0.020 sec/batch)
2018-01-30 17:37:26.909289: step 196000, loss = 2.17 (8952.6 examples/sec; 0.014 sec/batch)
2018-01-30 17:37:31.320493: step 196200, loss = 2.03 (5803.4 examples/sec; 0.022 sec/batch)
2018-01-30 17:37:34.988596: step 196400, loss = 2.04 (6979.1 examples/sec; 0.018 sec/batch)
2018-01-30 17:37:40.270411: step 196600, loss = 2.08 (4846.8 examples/sec; 0.026 sec/batch)
2018-01-30 17:37:45.616346: step 196800, loss = 2.11 (4788.7 examples/sec; 0.027 sec/batch)
2018-01-30 17:37:50.525756: step 197000, loss = 2.08 (5214.5 examples/sec; 0.025 sec/batch)
2018-01-30 17:37:53.376920: step 197200, loss = 2.03 (8978.8 examples/sec; 0.014 sec/batch)
2018-01-30 17:37:56.238613: step 197400, loss = 2.06 (8945.8 examples/sec; 0.014 sec/batch)
2018-01-30 17:37:59.084275: step 197600, loss = 2.10 (8996.2 examples/sec; 0.014 sec/batch)
2018-01-30 17:38:02.842075: step 197800, loss = 2.16 (6812.5 examples/sec; 0.019 sec/batch)
2018-01-30 17:38:08.144149: step 198000, loss = 2.15 (4828.3 examples/sec; 0.027 sec/batch)
2018-01-30 17:38:13.206635: step 198200, loss = 2.03 (5056.8 examples/sec; 0.025 sec/batch)
2018-01-30 17:38:18.312308: step 198400, loss = 2.09 (5014.0 examples/sec; 0.026 sec/batch)
2018-01-30 17:38:21.151396: step 198600, loss = 1.99 (9017.0 examples/sec; 0.014 sec/batch)
2018-01-30 17:38:24.019065: step 198800, loss = 2.13 (8927.1 examples/sec; 0.014 sec/batch)
2018-01-30 17:38:26.885596: step 199000, loss = 2.07 (8930.7 examples/sec; 0.014 sec/batch)
2018-01-30 17:38:39.818875: step 199200, loss = 1.98 (1979.4 examples/sec; 0.065 sec/batch)
2018-01-30 17:38:42.679870: step 199400, loss = 2.01 (8947.9 examples/sec; 0.014 sec/batch)
2018-01-30 17:38:45.532000: step 199600, loss = 2.03 (8975.7 examples/sec; 0.014 sec/batch)
2018-01-30 17:38:48.393602: step 199800, loss = 2.01 (8946.0 examples/sec; 0.014 sec/batch)
