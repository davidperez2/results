Images -> gris
pas de distored_inputs
f=tf.Variable(initial_value=tf.range(-10,10,20/576,dtype=tf.float32),trainable=True)



EVALUATION

2018-01-30 15:00:28.383939: precision @ 1 = 0.083
2018-01-30 15:00:56.707465: precision @ 1 = 0.083
2018-01-30 15:01:24.317493: precision @ 1 = 0.188
2018-01-30 15:01:52.057845: precision @ 1 = 0.188
2018-01-30 15:02:12.828950: precision @ 1 = 0.188
2018-01-30 15:02:40.546637: precision @ 1 = 0.191
2018-01-30 15:03:06.233838: precision @ 1 = 0.191
2018-01-30 15:03:35.015982: precision @ 1 = 0.187
2018-01-30 15:04:02.909286: precision @ 1 = 0.187
2018-01-30 15:04:30.114212: precision @ 1 = 0.187
2018-01-30 15:04:57.043593: precision @ 1 = 0.187
2018-01-30 15:05:24.786747: precision @ 1 = 0.186
2018-01-30 15:05:52.762297: precision @ 1 = 0.186
2018-01-30 15:06:13.192864: precision @ 1 = 0.186
2018-01-30 15:06:40.313760: precision @ 1 = 0.185
2018-01-30 15:07:06.342870: precision @ 1 = 0.185
2018-01-30 15:07:33.876659: precision @ 1 = 0.184
2018-01-30 15:08:01.692342: precision @ 1 = 0.184
2018-01-30 15:08:29.298606: precision @ 1 = 0.186
2018-01-30 15:08:56.662727: precision @ 1 = 0.186
2018-01-30 15:09:24.209923: precision @ 1 = 0.186
2018-01-30 15:09:51.938318: precision @ 1 = 0.186
2018-01-30 15:10:12.960541: precision @ 1 = 0.186
2018-01-30 15:10:40.569558: precision @ 1 = 0.187
2018-01-30 15:11:06.144093: precision @ 1 = 0.187
2018-01-30 15:11:34.104461: precision @ 1 = 0.189
2018-01-30 15:12:01.602374: precision @ 1 = 0.189
2018-01-30 15:12:28.718394: precision @ 1 = 0.191
2018-01-30 15:12:55.953301: precision @ 1 = 0.191
2018-01-30 15:13:23.530399: precision @ 1 = 0.190
2018-01-30 15:13:51.223188: precision @ 1 = 0.190
2018-01-30 15:14:12.262724: precision @ 1 = 0.190
2018-01-30 15:14:39.717644: precision @ 1 = 0.190
2018-01-30 15:15:06.297502: precision @ 1 = 0.190
2018-01-30 15:15:34.160233: precision @ 1 = 0.188
2018-01-30 15:16:01.965705: precision @ 1 = 0.188
2018-01-30 15:16:29.892377: precision @ 1 = 0.187
2018-01-30 15:16:57.622494: precision @ 1 = 0.187
2018-01-30 15:17:25.077465: precision @ 1 = 0.187
2018-01-30 15:17:52.209467: precision @ 1 = 0.187
2018-01-30 15:18:13.023324: precision @ 1 = 0.187
2018-01-30 15:18:40.760722: precision @ 1 = 0.186
2018-01-30 15:19:06.463376: precision @ 1 = 0.186
2018-01-30 15:19:33.179422: precision @ 1 = 0.184
2018-01-30 15:20:01.331931: precision @ 1 = 0.184
2018-01-30 15:20:29.058016: precision @ 1 = 0.184
2018-01-30 15:20:57.007801: precision @ 1 = 0.184
2018-01-30 15:21:24.766032: precision @ 1 = 0.183
2018-01-30 15:21:53.123218: precision @ 1 = 0.183
2018-01-30 15:22:13.508551: precision @ 1 = 0.183
2018-01-30 15:22:41.032452: precision @ 1 = 0.182
2018-01-30 15:23:06.780485: precision @ 1 = 0.182
2018-01-30 15:23:34.323282: precision @ 1 = 0.181
2018-01-30 15:24:02.012855: precision @ 1 = 0.181
2018-01-30 15:24:29.754138: precision @ 1 = 0.181
2018-01-30 15:24:57.750857: precision @ 1 = 0.181
2018-01-30 15:25:25.279739: precision @ 1 = 0.182
2018-01-30 15:25:53.427506: precision @ 1 = 0.182
2018-01-30 15:26:13.710316: precision @ 1 = 0.182
2018-01-30 15:26:41.748698: precision @ 1 = 0.181
2018-01-30 15:27:07.428290: precision @ 1 = 0.181
2018-01-30 15:27:35.337143: precision @ 1 = 0.181
2018-01-30 15:28:03.565217: precision @ 1 = 0.181
2018-01-30 15:28:30.914753: precision @ 1 = 0.181
2018-01-30 15:28:58.714006: precision @ 1 = 0.181
2018-01-30 15:29:26.745213: precision @ 1 = 0.180
2018-01-30 15:29:53.966123: precision @ 1 = 0.180
2018-01-30 15:30:13.923256: precision @ 1 = 0.180
2018-01-30 15:30:41.400150: precision @ 1 = 0.179
2018-01-30 15:31:07.222043: precision @ 1 = 0.179
2018-01-30 15:31:34.679439: precision @ 1 = 0.180
2018-01-30 15:32:02.020535: precision @ 1 = 0.180
2018-01-30 15:32:29.609486: precision @ 1 = 0.179
2018-01-30 15:32:57.025208: precision @ 1 = 0.179
2018-01-30 15:33:24.454972: precision @ 1 = 0.179
2018-01-30 15:33:52.722631: precision @ 1 = 0.179
2018-01-30 15:34:13.396526: precision @ 1 = 0.179
2018-01-30 15:34:41.264914: precision @ 1 = 0.178
2018-01-30 15:35:06.892915: precision @ 1 = 0.178
2018-01-30 15:35:35.173014: precision @ 1 = 0.179
2018-01-30 15:36:03.159412: precision @ 1 = 0.179
2018-01-30 15:36:30.882108: precision @ 1 = 0.180
2018-01-30 15:36:58.940253: precision @ 1 = 0.180
2018-01-30 15:37:27.123865: precision @ 1 = 0.182
2018-01-30 15:37:54.879671: precision @ 1 = 0.182
2018-01-30 15:38:15.102543: precision @ 1 = 0.182
2018-01-30 15:38:42.712826: precision @ 1 = 0.182
2018-01-30 15:39:07.759572: precision @ 1 = 0.182
2018-01-30 15:39:34.883130: precision @ 1 = 0.182
2018-01-30 15:40:01.969469: precision @ 1 = 0.182
2018-01-30 15:40:29.911525: precision @ 1 = 0.182
2018-01-30 15:40:57.017209: precision @ 1 = 0.181
2018-01-30 15:41:24.799124: precision @ 1 = 0.181
2018-01-30 15:41:52.463416: precision @ 1 = 0.181
2018-01-30 15:42:13.307209: precision @ 1 = 0.181
2018-01-30 15:42:40.793981: precision @ 1 = 0.180
2018-01-30 15:43:06.929475: precision @ 1 = 0.180
2018-01-30 15:43:34.249823: precision @ 1 = 0.181
2018-01-30 15:44:01.063424: precision @ 1 = 0.181
2018-01-30 15:44:28.625796: precision @ 1 = 0.180
2018-01-30 15:44:56.220578: precision @ 1 = 0.180
2018-01-30 15:45:23.683507: precision @ 1 = 0.180
2018-01-30 15:45:51.467669: precision @ 1 = 0.180
2018-01-30 15:46:12.839963: precision @ 1 = 0.180
2018-01-30 15:46:40.345310: precision @ 1 = 0.179
2018-01-30 15:47:06.629386: precision @ 1 = 0.179
2018-01-30 15:47:34.134967: precision @ 1 = 0.179
2018-01-30 15:48:01.598837: precision @ 1 = 0.179
2018-01-30 15:48:29.026257: precision @ 1 = 0.180
2018-01-30 15:48:57.005941: precision @ 1 = 0.180
2018-01-30 15:49:24.766927: precision @ 1 = 0.179
2018-01-30 15:49:52.851548: precision @ 1 = 0.179
2018-01-30 15:50:13.666706: precision @ 1 = 0.179
2018-01-30 15:50:41.338142: precision @ 1 = 0.179
2018-01-30 15:51:07.411354: precision @ 1 = 0.179
2018-01-30 15:51:35.127286: precision @ 1 = 0.179
2018-01-30 15:52:02.540789: precision @ 1 = 0.179
2018-01-30 15:52:30.036075: precision @ 1 = 0.179
2018-01-30 15:52:57.930761: precision @ 1 = 0.179
2018-01-30 15:53:25.520974: precision @ 1 = 0.179
2018-01-30 15:53:53.506592: precision @ 1 = 0.179
2018-01-30 15:54:14.138491: precision @ 1 = 0.179
2018-01-30 15:54:41.585006: precision @ 1 = 0.179
2018-01-30 15:55:07.526199: precision @ 1 = 0.179
2018-01-30 15:55:34.780497: precision @ 1 = 0.179
2018-01-30 15:56:02.667630: precision @ 1 = 0.179
2018-01-30 15:56:30.591079: precision @ 1 = 0.179
2018-01-30 15:56:58.734465: precision @ 1 = 0.179
2018-01-30 15:57:26.257350: precision @ 1 = 0.179
2018-01-30 15:57:53.948906: precision @ 1 = 0.179
2018-01-30 15:58:14.177404: precision @ 1 = 0.179
2018-01-30 15:58:41.761925: precision @ 1 = 0.179
2018-01-30 15:59:07.727400: precision @ 1 = 0.179
2018-01-30 15:59:35.700000: precision @ 1 = 0.179
2018-01-30 16:00:03.708144: precision @ 1 = 0.179
2018-01-30 16:00:31.881511: precision @ 1 = 0.179
2018-01-30 16:00:59.643756: precision @ 1 = 0.179
2018-01-30 16:01:27.133506: precision @ 1 = 0.179
2018-01-30 16:01:53.910702: precision @ 1 = 0.179
2018-01-30 16:02:14.093196: precision @ 1 = 0.179
2018-01-30 16:02:41.771316: precision @ 1 = 0.179
2018-01-30 16:03:07.332805: precision @ 1 = 0.179
2018-01-30 16:03:35.883830: precision @ 1 = 0.179
2018-01-30 16:04:03.288930: precision @ 1 = 0.179
2018-01-30 16:04:30.712429: precision @ 1 = 0.179
2018-01-30 16:04:58.578783: precision @ 1 = 0.179
2018-01-30 16:05:26.499559: precision @ 1 = 0.179
2018-01-30 16:05:53.772407: precision @ 1 = 0.179
2018-01-30 16:06:14.131445: precision @ 1 = 0.179
2018-01-30 16:06:41.775635: precision @ 1 = 0.179
2018-01-30 16:07:07.591597: precision @ 1 = 0.179
2018-01-30 16:07:36.022415: precision @ 1 = 0.179
2018-01-30 16:08:03.457300: precision @ 1 = 0.179
2018-01-30 16:08:32.104397: precision @ 1 = 0.179
2018-01-30 16:08:59.451235: precision @ 1 = 0.179
2018-01-30 16:09:27.065951: precision @ 1 = 0.179
2018-01-30 16:09:55.052095: precision @ 1 = 0.179
2018-01-30 16:10:14.687492: precision @ 1 = 0.179
2018-01-30 16:10:34.845819: precision @ 1 = 0.179
2018-01-30 16:10:54.247890: precision @ 1 = 0.179
2018-01-30 16:11:13.595246: precision @ 1 = 0.179
2018-01-30 16:11:32.854747: precision @ 1 = 0.179
2018-01-30 16:11:52.183143: precision @ 1 = 0.179
2018-01-30 16:12:11.504610: precision @ 1 = 0.179
2018-01-30 16:12:30.925174: precision @ 1 = 0.179



LOSS (Train)

2018-01-30 15:00:05.444688: step 0, loss = 2.31 (3478.9 examples/sec; 0.037 sec/batch)
2018-01-30 15:00:08.484067: step 200, loss = 49.03 (8422.8 examples/sec; 0.015 sec/batch)
2018-01-30 15:00:11.653651: step 400, loss = 37.75 (8076.8 examples/sec; 0.016 sec/batch)
2018-01-30 15:00:17.218172: step 600, loss = 27.05 (4600.6 examples/sec; 0.028 sec/batch)
2018-01-30 15:00:22.183607: step 800, loss = 25.14 (5155.6 examples/sec; 0.025 sec/batch)
2018-01-30 15:00:27.548674: step 1000, loss = 16.26 (4771.6 examples/sec; 0.027 sec/batch)
2018-01-30 15:00:30.819357: step 1200, loss = 28.76 (7827.1 examples/sec; 0.016 sec/batch)
2018-01-30 15:00:33.683501: step 1400, loss = 17.51 (8938.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:00:36.518092: step 1600, loss = 17.18 (9031.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:00:39.445007: step 1800, loss = 17.07 (8746.4 examples/sec; 0.015 sec/batch)
2018-01-30 15:00:44.678112: step 2000, loss = 15.51 (4891.9 examples/sec; 0.026 sec/batch)
2018-01-30 15:00:49.885385: step 2200, loss = 14.97 (4916.2 examples/sec; 0.026 sec/batch)
2018-01-30 15:00:54.929733: step 2400, loss = 15.14 (5075.0 examples/sec; 0.025 sec/batch)
2018-01-30 15:00:58.627689: step 2600, loss = 10.55 (6922.7 examples/sec; 0.018 sec/batch)
2018-01-30 15:01:01.509599: step 2800, loss = 17.82 (8883.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:01:05.914174: step 3000, loss = 12.28 (5812.1 examples/sec; 0.022 sec/batch)
2018-01-30 15:01:09.685907: step 3200, loss = 8.34 (6787.3 examples/sec; 0.019 sec/batch)
2018-01-30 15:01:15.334742: step 3400, loss = 8.87 (4531.9 examples/sec; 0.028 sec/batch)
2018-01-30 15:01:20.727737: step 3600, loss = 9.07 (4746.9 examples/sec; 0.027 sec/batch)
2018-01-30 15:01:25.206402: step 3800, loss = 10.14 (5716.0 examples/sec; 0.022 sec/batch)
2018-01-30 15:01:28.077160: step 4000, loss = 7.51 (8917.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:01:30.913679: step 4200, loss = 9.92 (9025.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:01:33.777588: step 4400, loss = 7.09 (8938.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:01:37.932548: step 4600, loss = 8.29 (6161.3 examples/sec; 0.021 sec/batch)
2018-01-30 15:01:43.186247: step 4800, loss = 6.49 (4872.8 examples/sec; 0.026 sec/batch)
2018-01-30 15:01:48.404276: step 5000, loss = 5.66 (4906.1 examples/sec; 0.026 sec/batch)
2018-01-30 15:01:53.028313: step 5200, loss = 6.19 (5536.3 examples/sec; 0.023 sec/batch)
2018-01-30 15:01:55.885741: step 5400, loss = 4.76 (8959.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:01:58.713698: step 5600, loss = 6.10 (9052.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:02:01.585076: step 5800, loss = 5.40 (8915.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:02:14.437380: step 6000, loss = 3.68 (1991.9 examples/sec; 0.064 sec/batch)
2018-01-30 15:02:17.329715: step 6200, loss = 3.66 (8851.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:02:20.188472: step 6400, loss = 3.50 (8954.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:02:23.041672: step 6600, loss = 4.25 (8972.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:02:27.679855: step 6800, loss = 3.11 (5519.4 examples/sec; 0.023 sec/batch)
2018-01-30 15:02:32.719959: step 7000, loss = 3.04 (5079.3 examples/sec; 0.025 sec/batch)
2018-01-30 15:02:38.465063: step 7200, loss = 3.03 (4456.0 examples/sec; 0.029 sec/batch)
2018-01-30 15:02:42.332329: step 7400, loss = 2.66 (6619.7 examples/sec; 0.019 sec/batch)
2018-01-30 15:02:45.212207: step 7600, loss = 3.01 (8889.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:02:48.045168: step 7800, loss = 2.94 (9036.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:02:50.893170: step 8000, loss = 2.66 (8988.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:02:55.744723: step 8200, loss = 2.65 (5276.7 examples/sec; 0.024 sec/batch)
2018-01-30 15:03:01.168198: step 8400, loss = 2.58 (4720.2 examples/sec; 0.027 sec/batch)
2018-01-30 15:03:09.025902: step 8600, loss = 2.45 (3257.9 examples/sec; 0.039 sec/batch)
2018-01-30 15:03:11.869733: step 8800, loss = 2.59 (9001.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:03:14.765463: step 9000, loss = 2.61 (8840.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:03:18.315310: step 9200, loss = 2.40 (7211.6 examples/sec; 0.018 sec/batch)
2018-01-30 15:03:23.383897: step 9400, loss = 2.47 (5050.7 examples/sec; 0.025 sec/batch)
2018-01-30 15:03:28.895342: step 9600, loss = 2.61 (4644.9 examples/sec; 0.028 sec/batch)
2018-01-30 15:03:34.312361: step 9800, loss = 2.39 (4725.8 examples/sec; 0.027 sec/batch)
2018-01-30 15:03:37.583067: step 10000, loss = 2.47 (7827.1 examples/sec; 0.016 sec/batch)
2018-01-30 15:03:40.494583: step 10200, loss = 2.36 (8792.7 examples/sec; 0.015 sec/batch)
2018-01-30 15:03:43.344150: step 10400, loss = 2.56 (8983.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:03:46.435579: step 10600, loss = 2.53 (8281.0 examples/sec; 0.015 sec/batch)
2018-01-30 15:03:51.528682: step 10800, loss = 2.34 (5026.4 examples/sec; 0.025 sec/batch)
2018-01-30 15:03:56.782133: step 11000, loss = 2.44 (4873.0 examples/sec; 0.026 sec/batch)
2018-01-30 15:04:02.158158: step 11200, loss = 2.53 (4761.9 examples/sec; 0.027 sec/batch)
2018-01-30 15:04:06.922363: step 11400, loss = 2.53 (5373.4 examples/sec; 0.024 sec/batch)
2018-01-30 15:04:09.788433: step 11600, loss = 2.34 (8932.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:04:12.651090: step 11800, loss = 2.44 (8942.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:04:17.394794: step 12000, loss = 2.28 (5396.6 examples/sec; 0.024 sec/batch)
2018-01-30 15:04:22.973992: step 12200, loss = 2.35 (4588.5 examples/sec; 0.028 sec/batch)
2018-01-30 15:04:28.154965: step 12400, loss = 2.38 (4941.2 examples/sec; 0.026 sec/batch)
2018-01-30 15:04:32.129118: step 12600, loss = 2.30 (6441.6 examples/sec; 0.020 sec/batch)
2018-01-30 15:04:34.995050: step 12800, loss = 2.30 (8932.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:04:37.882188: step 13000, loss = 2.41 (8866.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:04:40.759497: step 13200, loss = 2.40 (8897.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:04:46.258348: step 13400, loss = 2.31 (4655.5 examples/sec; 0.027 sec/batch)
2018-01-30 15:04:51.742787: step 13600, loss = 2.23 (4667.8 examples/sec; 0.027 sec/batch)
2018-01-30 15:04:57.152979: step 13800, loss = 2.31 (4731.8 examples/sec; 0.027 sec/batch)
2018-01-30 15:05:00.004946: step 14000, loss = 2.24 (8976.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:05:02.884185: step 14200, loss = 2.44 (8891.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:05:07.279692: step 14400, loss = 2.26 (5824.1 examples/sec; 0.022 sec/batch)
2018-01-30 15:05:12.019167: step 14600, loss = 2.41 (5401.4 examples/sec; 0.024 sec/batch)
2018-01-30 15:05:17.374470: step 14800, loss = 2.26 (4780.3 examples/sec; 0.027 sec/batch)
2018-01-30 15:05:22.649587: step 15000, loss = 2.30 (4853.0 examples/sec; 0.026 sec/batch)
2018-01-30 15:05:26.529970: step 15200, loss = 2.22 (6597.3 examples/sec; 0.019 sec/batch)
2018-01-30 15:05:29.393192: step 15400, loss = 2.17 (8941.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:05:32.254066: step 15600, loss = 2.18 (8948.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:05:35.113156: step 15800, loss = 2.35 (8953.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:05:39.946868: step 16000, loss = 2.36 (5296.1 examples/sec; 0.024 sec/batch)
2018-01-30 15:05:45.202407: step 16200, loss = 2.21 (4871.1 examples/sec; 0.026 sec/batch)
2018-01-30 15:05:50.500201: step 16400, loss = 2.16 (4832.2 examples/sec; 0.026 sec/batch)
2018-01-30 15:05:54.370661: step 16600, loss = 2.30 (6614.2 examples/sec; 0.019 sec/batch)
2018-01-30 15:05:57.200219: step 16800, loss = 2.27 (9047.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:06:00.079051: step 17000, loss = 2.28 (8892.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:06:03.015788: step 17200, loss = 2.33 (8717.2 examples/sec; 0.015 sec/batch)
2018-01-30 15:06:15.778034: step 17400, loss = 2.29 (2005.9 examples/sec; 0.064 sec/batch)
2018-01-30 15:06:18.656851: step 17600, loss = 2.36 (8892.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:06:21.523066: step 17800, loss = 2.44 (8931.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:06:24.673263: step 18000, loss = 2.36 (8126.5 examples/sec; 0.016 sec/batch)
2018-01-30 15:06:30.110678: step 18200, loss = 2.30 (4708.1 examples/sec; 0.027 sec/batch)
2018-01-30 15:06:36.038796: step 18400, loss = 2.13 (4318.4 examples/sec; 0.030 sec/batch)
2018-01-30 15:06:40.740470: step 18600, loss = 2.18 (5444.9 examples/sec; 0.024 sec/batch)
2018-01-30 15:06:43.634387: step 18800, loss = 2.08 (8846.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:06:46.479094: step 19000, loss = 2.21 (8999.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:06:49.347526: step 19200, loss = 2.21 (8924.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:06:52.961688: step 19400, loss = 2.26 (7083.2 examples/sec; 0.018 sec/batch)
2018-01-30 15:06:58.351457: step 19600, loss = 2.20 (4749.7 examples/sec; 0.027 sec/batch)
2018-01-30 15:07:03.497975: step 19800, loss = 2.11 (4974.2 examples/sec; 0.026 sec/batch)
2018-01-30 15:07:10.376426: step 20000, loss = 2.15 (3721.8 examples/sec; 0.034 sec/batch)
2018-01-30 15:07:13.221144: step 20200, loss = 2.23 (8999.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:07:16.104476: step 20400, loss = 2.17 (8878.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:07:20.588647: step 20600, loss = 2.26 (5709.0 examples/sec; 0.022 sec/batch)
2018-01-30 15:07:26.023124: step 20800, loss = 2.18 (4710.7 examples/sec; 0.027 sec/batch)
2018-01-30 15:07:31.347684: step 21000, loss = 2.23 (4807.9 examples/sec; 0.027 sec/batch)
2018-01-30 15:07:35.375921: step 21200, loss = 2.32 (6355.1 examples/sec; 0.020 sec/batch)
2018-01-30 15:07:38.218797: step 21400, loss = 2.21 (9005.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:07:41.059623: step 21600, loss = 2.10 (9011.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:07:43.907194: step 21800, loss = 2.28 (8990.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:07:48.503780: step 22000, loss = 2.16 (5569.4 examples/sec; 0.023 sec/batch)
2018-01-30 15:07:53.807208: step 22200, loss = 2.16 (4827.1 examples/sec; 0.027 sec/batch)
2018-01-30 15:07:58.958364: step 22400, loss = 2.27 (4969.8 examples/sec; 0.026 sec/batch)
2018-01-30 15:08:03.143390: step 22600, loss = 2.11 (6117.0 examples/sec; 0.021 sec/batch)
2018-01-30 15:08:07.514249: step 22800, loss = 2.16 (5857.0 examples/sec; 0.022 sec/batch)
2018-01-30 15:08:10.369826: step 23000, loss = 2.26 (8964.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:08:13.826155: step 23200, loss = 2.13 (7406.7 examples/sec; 0.017 sec/batch)
2018-01-30 15:08:19.486703: step 23400, loss = 2.17 (4522.5 examples/sec; 0.028 sec/batch)
2018-01-30 15:08:24.770138: step 23600, loss = 2.23 (4845.3 examples/sec; 0.026 sec/batch)
2018-01-30 15:08:29.604923: step 23800, loss = 2.09 (5295.0 examples/sec; 0.024 sec/batch)
2018-01-30 15:08:32.477986: step 24000, loss = 2.12 (8910.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:08:35.323626: step 24200, loss = 2.18 (8996.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:08:38.168219: step 24400, loss = 2.03 (8999.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:08:41.652292: step 24600, loss = 2.16 (7347.7 examples/sec; 0.017 sec/batch)
2018-01-30 15:08:47.107459: step 24800, loss = 2.21 (4692.8 examples/sec; 0.027 sec/batch)
2018-01-30 15:08:52.487755: step 25000, loss = 2.23 (4758.1 examples/sec; 0.027 sec/batch)
2018-01-30 15:08:57.398590: step 25200, loss = 2.22 (5213.0 examples/sec; 0.025 sec/batch)
2018-01-30 15:09:00.307064: step 25400, loss = 2.02 (8801.9 examples/sec; 0.015 sec/batch)
2018-01-30 15:09:03.175121: step 25600, loss = 2.22 (8925.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:09:07.730378: step 25800, loss = 2.12 (5619.9 examples/sec; 0.023 sec/batch)
2018-01-30 15:09:12.965856: step 26000, loss = 2.15 (4889.7 examples/sec; 0.026 sec/batch)
2018-01-30 15:09:18.490036: step 26200, loss = 2.18 (4634.2 examples/sec; 0.028 sec/batch)
2018-01-30 15:09:23.584857: step 26400, loss = 2.23 (5024.7 examples/sec; 0.025 sec/batch)
2018-01-30 15:09:26.810249: step 26600, loss = 2.33 (7937.0 examples/sec; 0.016 sec/batch)
2018-01-30 15:09:29.670804: step 26800, loss = 2.17 (8949.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:09:32.538982: step 27000, loss = 2.09 (8925.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:09:35.780841: step 27200, loss = 2.28 (7896.7 examples/sec; 0.016 sec/batch)
2018-01-30 15:09:41.011258: step 27400, loss = 2.12 (4894.4 examples/sec; 0.026 sec/batch)
2018-01-30 15:09:46.315220: step 27600, loss = 2.17 (4826.6 examples/sec; 0.027 sec/batch)
2018-01-30 15:09:51.547652: step 27800, loss = 2.07 (4892.6 examples/sec; 0.026 sec/batch)
2018-01-30 15:09:54.640494: step 28000, loss = 2.20 (8277.2 examples/sec; 0.015 sec/batch)
2018-01-30 15:09:57.486339: step 28200, loss = 2.21 (8995.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:10:00.373899: step 28400, loss = 2.12 (8865.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:10:03.604837: step 28600, loss = 2.09 (7923.4 examples/sec; 0.016 sec/batch)
2018-01-30 15:10:16.040684: step 28800, loss = 2.12 (2058.6 examples/sec; 0.062 sec/batch)
2018-01-30 15:10:18.886540: step 29000, loss = 2.13 (8995.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:10:21.757451: step 29200, loss = 2.19 (8917.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:10:25.332060: step 29400, loss = 2.37 (7161.6 examples/sec; 0.018 sec/batch)
2018-01-30 15:10:30.752059: step 29600, loss = 2.10 (4723.2 examples/sec; 0.027 sec/batch)
2018-01-30 15:10:36.283215: step 29800, loss = 2.17 (4628.3 examples/sec; 0.028 sec/batch)
2018-01-30 15:10:41.038358: step 30000, loss = 2.17 (5383.6 examples/sec; 0.024 sec/batch)
2018-01-30 15:10:43.891801: step 30200, loss = 2.17 (8971.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:10:46.754319: step 30400, loss = 2.17 (8943.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:10:49.601108: step 30600, loss = 2.08 (8992.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:10:53.441418: step 30800, loss = 2.07 (6666.1 examples/sec; 0.019 sec/batch)
2018-01-30 15:10:59.005866: step 31000, loss = 2.16 (4600.6 examples/sec; 0.028 sec/batch)
2018-01-30 15:11:07.693546: step 31200, loss = 2.12 (2946.7 examples/sec; 0.043 sec/batch)
2018-01-30 15:11:10.562732: step 31400, loss = 2.10 (8922.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:11:13.431573: step 31600, loss = 2.09 (8923.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:11:16.292553: step 31800, loss = 2.15 (8948.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:11:20.639269: step 32000, loss = 2.09 (5889.5 examples/sec; 0.022 sec/batch)
2018-01-30 15:11:26.166983: step 32200, loss = 2.10 (4631.2 examples/sec; 0.028 sec/batch)
2018-01-30 15:11:31.648765: step 32400, loss = 2.13 (4670.0 examples/sec; 0.027 sec/batch)
2018-01-30 15:11:35.589649: step 32600, loss = 2.22 (6496.0 examples/sec; 0.020 sec/batch)
2018-01-30 15:11:38.460410: step 32800, loss = 2.08 (8917.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:11:41.335767: step 33000, loss = 2.13 (8903.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:11:44.194672: step 33200, loss = 2.15 (8954.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:11:48.827028: step 33400, loss = 2.13 (5526.3 examples/sec; 0.023 sec/batch)
2018-01-30 15:11:54.229208: step 33600, loss = 2.13 (4738.8 examples/sec; 0.027 sec/batch)
2018-01-30 15:11:59.595238: step 33800, loss = 2.10 (4770.8 examples/sec; 0.027 sec/batch)
2018-01-30 15:12:03.439833: step 34000, loss = 2.13 (6658.7 examples/sec; 0.019 sec/batch)
2018-01-30 15:12:07.805103: step 34200, loss = 2.14 (5864.5 examples/sec; 0.022 sec/batch)
2018-01-30 15:12:10.669595: step 34400, loss = 2.10 (8937.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:12:14.611354: step 34600, loss = 2.10 (6494.6 examples/sec; 0.020 sec/batch)
2018-01-30 15:12:19.657475: step 34800, loss = 2.24 (5073.2 examples/sec; 0.025 sec/batch)
2018-01-30 15:12:25.670277: step 35000, loss = 2.15 (4257.6 examples/sec; 0.030 sec/batch)
2018-01-30 15:12:29.980403: step 35200, loss = 2.04 (5939.5 examples/sec; 0.022 sec/batch)
2018-01-30 15:12:32.866293: step 35400, loss = 2.08 (8870.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:12:35.731922: step 35600, loss = 2.10 (8933.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:12:38.585921: step 35800, loss = 2.17 (8969.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:12:43.397378: step 36000, loss = 2.13 (5320.6 examples/sec; 0.024 sec/batch)
2018-01-30 15:12:48.691924: step 36200, loss = 2.11 (4835.2 examples/sec; 0.026 sec/batch)
2018-01-30 15:12:54.321141: step 36400, loss = 2.08 (4547.7 examples/sec; 0.028 sec/batch)
2018-01-30 15:12:57.919625: step 36600, loss = 2.15 (7114.1 examples/sec; 0.018 sec/batch)
2018-01-30 15:13:00.788938: step 36800, loss = 2.10 (8922.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:13:03.664890: step 37000, loss = 2.02 (8901.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:13:09.114838: step 37200, loss = 1.95 (4697.3 examples/sec; 0.027 sec/batch)
2018-01-30 15:13:14.531353: step 37400, loss = 2.25 (4726.3 examples/sec; 0.027 sec/batch)
2018-01-30 15:13:19.812319: step 37600, loss = 2.14 (4847.6 examples/sec; 0.026 sec/batch)
2018-01-30 15:13:24.476995: step 37800, loss = 2.04 (5488.1 examples/sec; 0.023 sec/batch)
2018-01-30 15:13:27.356169: step 38000, loss = 2.21 (8891.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:13:30.227703: step 38200, loss = 2.15 (8915.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:13:33.078987: step 38400, loss = 2.12 (8978.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:13:37.475749: step 38600, loss = 2.05 (5822.5 examples/sec; 0.022 sec/batch)
2018-01-30 15:13:42.802180: step 38800, loss = 2.04 (4806.2 examples/sec; 0.027 sec/batch)
2018-01-30 15:13:48.022788: step 39000, loss = 2.22 (4903.6 examples/sec; 0.026 sec/batch)
2018-01-30 15:13:52.418320: step 39200, loss = 2.11 (5824.1 examples/sec; 0.022 sec/batch)
2018-01-30 15:13:55.281476: step 39400, loss = 2.14 (8941.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:13:58.154116: step 39600, loss = 2.13 (8911.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:14:01.004721: step 39800, loss = 2.13 (8980.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:14:13.783927: step 40000, loss = 2.14 (2003.3 examples/sec; 0.064 sec/batch)
2018-01-30 15:14:16.628126: step 40200, loss = 2.15 (9000.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:14:19.512869: step 40400, loss = 1.99 (8874.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:14:22.371893: step 40600, loss = 2.14 (8954.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:14:27.135664: step 40800, loss = 2.22 (5373.9 examples/sec; 0.024 sec/batch)
2018-01-30 15:14:32.232727: step 41000, loss = 1.99 (5022.5 examples/sec; 0.025 sec/batch)
2018-01-30 15:14:38.125578: step 41200, loss = 2.19 (4344.2 examples/sec; 0.029 sec/batch)
2018-01-30 15:14:41.716428: step 41400, loss = 2.12 (7129.2 examples/sec; 0.018 sec/batch)
2018-01-30 15:14:44.613294: step 41600, loss = 2.14 (8837.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:14:47.468511: step 41800, loss = 2.25 (8966.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:14:50.359886: step 42000, loss = 2.08 (8853.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:14:55.457849: step 42200, loss = 2.19 (5021.6 examples/sec; 0.025 sec/batch)
2018-01-30 15:15:00.508940: step 42400, loss = 2.07 (5068.2 examples/sec; 0.025 sec/batch)
2018-01-30 15:15:08.396222: step 42600, loss = 2.14 (3245.7 examples/sec; 0.039 sec/batch)
2018-01-30 15:15:11.277518: step 42800, loss = 2.19 (8884.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:15:14.141367: step 43000, loss = 2.07 (8939.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:15:17.047978: step 43200, loss = 2.04 (8807.5 examples/sec; 0.015 sec/batch)
2018-01-30 15:15:22.194117: step 43400, loss = 2.05 (4974.6 examples/sec; 0.026 sec/batch)
2018-01-30 15:15:27.421822: step 43600, loss = 2.12 (4897.0 examples/sec; 0.026 sec/batch)
2018-01-30 15:15:32.717371: step 43800, loss = 2.19 (4834.2 examples/sec; 0.026 sec/batch)
2018-01-30 15:15:36.242905: step 44000, loss = 2.02 (7261.3 examples/sec; 0.018 sec/batch)
2018-01-30 15:15:39.090169: step 44200, loss = 2.14 (8991.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:15:41.965543: step 44400, loss = 2.08 (8903.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:15:44.824951: step 44600, loss = 2.10 (8952.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:15:49.952965: step 44800, loss = 2.08 (4992.2 examples/sec; 0.026 sec/batch)
2018-01-30 15:15:55.260788: step 45000, loss = 2.18 (4823.1 examples/sec; 0.027 sec/batch)
2018-01-30 15:16:00.367362: step 45200, loss = 2.04 (5013.1 examples/sec; 0.026 sec/batch)
2018-01-30 15:16:03.992251: step 45400, loss = 2.08 (7062.3 examples/sec; 0.018 sec/batch)
2018-01-30 15:16:08.371400: step 45600, loss = 2.09 (5845.9 examples/sec; 0.022 sec/batch)
2018-01-30 15:16:11.234797: step 45800, loss = 2.11 (8940.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:16:15.454558: step 46000, loss = 2.07 (6066.7 examples/sec; 0.021 sec/batch)
2018-01-30 15:16:20.875835: step 46200, loss = 2.01 (4722.1 examples/sec; 0.027 sec/batch)
2018-01-30 15:16:26.467856: step 46400, loss = 2.09 (4578.0 examples/sec; 0.028 sec/batch)
2018-01-30 15:16:30.939335: step 46600, loss = 2.21 (5725.2 examples/sec; 0.022 sec/batch)
2018-01-30 15:16:33.856871: step 46800, loss = 2.04 (8774.5 examples/sec; 0.015 sec/batch)
2018-01-30 15:16:36.716151: step 47000, loss = 2.06 (8953.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:16:39.550547: step 47200, loss = 2.19 (9031.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:16:43.746154: step 47400, loss = 1.98 (6101.6 examples/sec; 0.021 sec/batch)
2018-01-30 15:16:48.877855: step 47600, loss = 2.10 (4988.6 examples/sec; 0.026 sec/batch)
2018-01-30 15:16:54.298695: step 47800, loss = 2.13 (4722.5 examples/sec; 0.027 sec/batch)
2018-01-30 15:16:58.664579: step 48000, loss = 2.03 (5863.6 examples/sec; 0.022 sec/batch)
2018-01-30 15:17:01.512064: step 48200, loss = 2.06 (8990.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:17:05.867264: step 48400, loss = 1.99 (5878.0 examples/sec; 0.022 sec/batch)
2018-01-30 15:17:09.017970: step 48600, loss = 2.17 (8125.2 examples/sec; 0.016 sec/batch)
2018-01-30 15:17:14.586963: step 48800, loss = 2.10 (4596.9 examples/sec; 0.028 sec/batch)
2018-01-30 15:17:19.756473: step 49000, loss = 2.06 (4952.1 examples/sec; 0.026 sec/batch)
2018-01-30 15:17:24.968078: step 49200, loss = 2.06 (4912.1 examples/sec; 0.026 sec/batch)
2018-01-30 15:17:27.874351: step 49400, loss = 2.05 (8808.5 examples/sec; 0.015 sec/batch)
2018-01-30 15:17:30.731311: step 49600, loss = 2.14 (8960.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:17:33.586409: step 49800, loss = 2.06 (8966.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:17:36.888626: step 50000, loss = 2.08 (7752.4 examples/sec; 0.017 sec/batch)
2018-01-30 15:17:42.492092: step 50200, loss = 2.02 (4568.6 examples/sec; 0.028 sec/batch)
2018-01-30 15:17:47.928360: step 50400, loss = 2.01 (4709.1 examples/sec; 0.027 sec/batch)
2018-01-30 15:17:52.851668: step 50600, loss = 2.14 (5199.8 examples/sec; 0.025 sec/batch)
2018-01-30 15:17:55.699120: step 50800, loss = 2.16 (8990.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:17:58.541332: step 51000, loss = 2.15 (9007.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:18:01.416709: step 51200, loss = 2.20 (8903.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:18:14.211120: step 51400, loss = 2.07 (2000.9 examples/sec; 0.064 sec/batch)
2018-01-30 15:18:17.056932: step 51600, loss = 1.97 (8995.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:18:19.939415: step 51800, loss = 1.98 (8881.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:18:22.803192: step 52000, loss = 2.20 (8939.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:18:27.387222: step 52200, loss = 2.06 (5584.6 examples/sec; 0.023 sec/batch)
2018-01-30 15:18:32.689238: step 52400, loss = 2.11 (4828.4 examples/sec; 0.027 sec/batch)
2018-01-30 15:18:37.851123: step 52600, loss = 2.03 (4959.4 examples/sec; 0.026 sec/batch)
2018-01-30 15:18:41.939030: step 52800, loss = 2.08 (6262.4 examples/sec; 0.020 sec/batch)
2018-01-30 15:18:44.804618: step 53000, loss = 2.06 (8933.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:18:47.662235: step 53200, loss = 2.11 (8958.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:18:50.490635: step 53400, loss = 2.10 (9051.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:18:54.693286: step 53600, loss = 2.06 (6091.4 examples/sec; 0.021 sec/batch)
2018-01-30 15:19:00.185956: step 53800, loss = 2.12 (4660.8 examples/sec; 0.027 sec/batch)
2018-01-30 15:19:08.445803: step 54000, loss = 2.15 (3099.3 examples/sec; 0.041 sec/batch)
2018-01-30 15:19:11.287468: step 54200, loss = 2.16 (9008.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:19:14.132411: step 54400, loss = 2.08 (8998.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:19:17.023160: step 54600, loss = 2.10 (8855.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:19:22.292553: step 54800, loss = 2.18 (4858.2 examples/sec; 0.026 sec/batch)
2018-01-30 15:19:27.678656: step 55000, loss = 2.08 (4753.0 examples/sec; 0.027 sec/batch)
2018-01-30 15:19:33.353716: step 55200, loss = 2.11 (4511.0 examples/sec; 0.028 sec/batch)
2018-01-30 15:19:36.250447: step 55400, loss = 2.08 (8837.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:19:39.092614: step 55600, loss = 2.18 (9007.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:19:41.952625: step 55800, loss = 2.14 (8951.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:19:45.433798: step 56000, loss = 2.11 (7353.8 examples/sec; 0.017 sec/batch)
2018-01-30 15:19:50.428803: step 56200, loss = 1.98 (5125.1 examples/sec; 0.025 sec/batch)
2018-01-30 15:19:55.822870: step 56400, loss = 2.07 (4746.0 examples/sec; 0.027 sec/batch)
2018-01-30 15:20:00.828337: step 56600, loss = 2.07 (5114.4 examples/sec; 0.025 sec/batch)
2018-01-30 15:20:03.926615: step 56800, loss = 2.12 (8262.7 examples/sec; 0.015 sec/batch)
2018-01-30 15:20:08.300023: step 57000, loss = 2.05 (5853.6 examples/sec; 0.022 sec/batch)
2018-01-30 15:20:11.144384: step 57200, loss = 2.00 (9000.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:20:15.932328: step 57400, loss = 2.13 (5346.8 examples/sec; 0.024 sec/batch)
2018-01-30 15:20:20.933532: step 57600, loss = 2.09 (5118.8 examples/sec; 0.025 sec/batch)
2018-01-30 15:20:25.969822: step 57800, loss = 2.01 (5083.1 examples/sec; 0.025 sec/batch)
2018-01-30 15:20:30.337523: step 58000, loss = 2.00 (5861.2 examples/sec; 0.022 sec/batch)
2018-01-30 15:20:33.148321: step 58200, loss = 2.16 (9107.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:20:36.165356: step 58400, loss = 2.11 (8485.2 examples/sec; 0.015 sec/batch)
2018-01-30 15:20:39.019682: step 58600, loss = 2.16 (8968.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:20:43.517980: step 58800, loss = 2.10 (5691.0 examples/sec; 0.022 sec/batch)
2018-01-30 15:20:48.477499: step 59000, loss = 2.09 (5161.8 examples/sec; 0.025 sec/batch)
2018-01-30 15:20:53.815240: step 59200, loss = 2.17 (4796.0 examples/sec; 0.027 sec/batch)
2018-01-30 15:20:58.163895: step 59400, loss = 2.10 (5886.9 examples/sec; 0.022 sec/batch)
2018-01-30 15:21:01.039316: step 59600, loss = 2.13 (8903.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:21:03.898842: step 59800, loss = 1.98 (8952.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:21:08.703418: step 60000, loss = 2.05 (5328.3 examples/sec; 0.024 sec/batch)
2018-01-30 15:21:13.621881: step 60200, loss = 2.01 (5204.9 examples/sec; 0.025 sec/batch)
2018-01-30 15:21:19.279978: step 60400, loss = 2.23 (4524.5 examples/sec; 0.028 sec/batch)
2018-01-30 15:21:24.641018: step 60600, loss = 2.07 (4775.2 examples/sec; 0.027 sec/batch)
2018-01-30 15:21:27.589235: step 60800, loss = 2.10 (8683.2 examples/sec; 0.015 sec/batch)
2018-01-30 15:21:30.435783: step 61000, loss = 1.95 (8993.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:21:33.277065: step 61200, loss = 2.02 (9010.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:21:36.621102: step 61400, loss = 2.09 (7655.4 examples/sec; 0.017 sec/batch)
2018-01-30 15:21:41.624710: step 61600, loss = 2.11 (5116.3 examples/sec; 0.025 sec/batch)
2018-01-30 15:21:46.877171: step 61800, loss = 2.25 (4873.9 examples/sec; 0.026 sec/batch)
2018-01-30 15:21:51.810129: step 62000, loss = 2.09 (5189.6 examples/sec; 0.025 sec/batch)
2018-01-30 15:21:55.224029: step 62200, loss = 2.04 (7498.8 examples/sec; 0.017 sec/batch)
2018-01-30 15:21:58.116976: step 62400, loss = 2.12 (8849.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:22:00.969508: step 62600, loss = 2.02 (8974.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:22:03.827075: step 62800, loss = 2.01 (8958.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:22:16.608834: step 63000, loss = 2.29 (2002.9 examples/sec; 0.064 sec/batch)
2018-01-30 15:22:19.481647: step 63200, loss = 1.99 (8911.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:22:22.332626: step 63400, loss = 2.07 (8979.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:22:25.972398: step 63600, loss = 2.05 (7033.4 examples/sec; 0.018 sec/batch)
2018-01-30 15:22:31.343248: step 63800, loss = 2.09 (4766.5 examples/sec; 0.027 sec/batch)
2018-01-30 15:22:36.880204: step 64000, loss = 2.00 (4623.5 examples/sec; 0.028 sec/batch)
2018-01-30 15:22:41.571506: step 64200, loss = 2.05 (5456.9 examples/sec; 0.023 sec/batch)
2018-01-30 15:22:44.431547: step 64400, loss = 2.11 (8950.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:22:47.261927: step 64600, loss = 2.13 (9044.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:22:50.126168: step 64800, loss = 2.05 (8937.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:22:53.911550: step 65000, loss = 2.07 (6762.9 examples/sec; 0.019 sec/batch)
2018-01-30 15:22:59.156868: step 65200, loss = 2.06 (4880.5 examples/sec; 0.026 sec/batch)
2018-01-30 15:23:08.132766: step 65400, loss = 2.16 (2852.1 examples/sec; 0.045 sec/batch)
2018-01-30 15:23:11.007970: step 65600, loss = 2.06 (8903.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:23:13.841030: step 65800, loss = 2.06 (9036.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:23:16.697389: step 66000, loss = 2.06 (8962.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:23:21.254724: step 66200, loss = 2.01 (5617.3 examples/sec; 0.023 sec/batch)
2018-01-30 15:23:26.442303: step 66400, loss = 2.04 (4934.9 examples/sec; 0.026 sec/batch)
2018-01-30 15:23:31.538302: step 66600, loss = 1.96 (5023.5 examples/sec; 0.025 sec/batch)
2018-01-30 15:23:35.822297: step 66800, loss = 2.02 (5975.7 examples/sec; 0.021 sec/batch)
2018-01-30 15:23:38.689298: step 67000, loss = 2.13 (8929.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:23:41.539988: step 67200, loss = 2.03 (8980.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:23:44.383153: step 67400, loss = 1.95 (9004.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:23:48.969210: step 67600, loss = 2.08 (5582.1 examples/sec; 0.023 sec/batch)
2018-01-30 15:23:54.198672: step 67800, loss = 2.04 (4895.3 examples/sec; 0.026 sec/batch)
2018-01-30 15:23:59.472681: step 68000, loss = 2.00 (4854.0 examples/sec; 0.026 sec/batch)
2018-01-30 15:24:03.544603: step 68200, loss = 2.05 (6287.0 examples/sec; 0.020 sec/batch)
2018-01-30 15:24:07.893175: step 68400, loss = 2.16 (5887.0 examples/sec; 0.022 sec/batch)
2018-01-30 15:24:10.742120: step 68600, loss = 2.19 (8985.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:24:14.315386: step 68800, loss = 2.11 (7164.3 examples/sec; 0.018 sec/batch)
2018-01-30 15:24:19.319701: step 69000, loss = 2.13 (5115.6 examples/sec; 0.025 sec/batch)
2018-01-30 15:24:24.700342: step 69200, loss = 2.02 (4757.8 examples/sec; 0.027 sec/batch)
2018-01-30 15:24:29.847897: step 69400, loss = 2.04 (4973.2 examples/sec; 0.026 sec/batch)
2018-01-30 15:24:32.707505: step 69600, loss = 2.11 (8952.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:24:35.562085: step 69800, loss = 2.07 (8968.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:24:38.417154: step 70000, loss = 2.11 (8966.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:24:41.927421: step 70200, loss = 2.04 (7292.9 examples/sec; 0.018 sec/batch)
2018-01-30 15:24:47.170770: step 70400, loss = 2.16 (4882.4 examples/sec; 0.026 sec/batch)
2018-01-30 15:24:52.290750: step 70600, loss = 2.09 (5000.0 examples/sec; 0.026 sec/batch)
2018-01-30 15:24:57.327490: step 70800, loss = 2.13 (5082.7 examples/sec; 0.025 sec/batch)
2018-01-30 15:25:00.406108: step 71000, loss = 2.14 (8315.4 examples/sec; 0.015 sec/batch)
2018-01-30 15:25:03.271276: step 71200, loss = 2.04 (8934.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:25:07.700751: step 71400, loss = 2.12 (5779.5 examples/sec; 0.022 sec/batch)
2018-01-30 15:25:12.249255: step 71600, loss = 2.09 (5628.2 examples/sec; 0.023 sec/batch)
2018-01-30 15:25:17.642176: step 71800, loss = 2.15 (4747.0 examples/sec; 0.027 sec/batch)
2018-01-30 15:25:22.934816: step 72000, loss = 2.10 (4836.9 examples/sec; 0.026 sec/batch)
2018-01-30 15:25:26.902640: step 72200, loss = 1.96 (6451.9 examples/sec; 0.020 sec/batch)
2018-01-30 15:25:29.752403: step 72400, loss = 2.05 (8983.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:25:32.581291: step 72600, loss = 1.93 (9049.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:25:35.438779: step 72800, loss = 2.14 (8958.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:25:39.916202: step 73000, loss = 2.04 (5717.6 examples/sec; 0.022 sec/batch)
2018-01-30 15:25:45.076050: step 73200, loss = 2.19 (4961.4 examples/sec; 0.026 sec/batch)
2018-01-30 15:25:50.389488: step 73400, loss = 2.01 (4818.0 examples/sec; 0.027 sec/batch)
2018-01-30 15:25:54.705185: step 73600, loss = 2.04 (5931.8 examples/sec; 0.022 sec/batch)
2018-01-30 15:25:57.598543: step 73800, loss = 2.05 (8847.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:26:00.457229: step 74000, loss = 2.01 (8955.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:26:03.319531: step 74200, loss = 2.05 (8943.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:26:16.027881: step 74400, loss = 2.21 (2014.4 examples/sec; 0.064 sec/batch)
2018-01-30 15:26:18.901182: step 74600, loss = 2.07 (8909.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:26:21.794489: step 74800, loss = 2.09 (8848.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:26:24.810339: step 75000, loss = 2.05 (8488.5 examples/sec; 0.015 sec/batch)
2018-01-30 15:26:29.945619: step 75200, loss = 2.13 (4985.1 examples/sec; 0.026 sec/batch)
2018-01-30 15:26:35.130745: step 75400, loss = 2.11 (4937.2 examples/sec; 0.026 sec/batch)
2018-01-30 15:26:40.345198: step 75600, loss = 2.13 (4909.4 examples/sec; 0.026 sec/batch)
2018-01-30 15:26:43.853429: step 75800, loss = 2.05 (7297.1 examples/sec; 0.018 sec/batch)
2018-01-30 15:26:46.715764: step 76000, loss = 1.97 (8943.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:26:49.570196: step 76200, loss = 2.05 (8968.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:26:52.435807: step 76400, loss = 2.07 (8933.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:26:57.417545: step 76600, loss = 2.10 (5138.8 examples/sec; 0.025 sec/batch)
2018-01-30 15:27:02.598597: step 76800, loss = 1.96 (4941.1 examples/sec; 0.026 sec/batch)
2018-01-30 15:27:10.529876: step 77000, loss = 2.09 (3227.7 examples/sec; 0.040 sec/batch)
2018-01-30 15:27:13.395110: step 77200, loss = 1.99 (8934.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:27:16.260294: step 77400, loss = 2.03 (8934.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:27:19.894779: step 77600, loss = 2.05 (7043.6 examples/sec; 0.018 sec/batch)
2018-01-30 15:27:25.209569: step 77800, loss = 1.94 (4816.7 examples/sec; 0.027 sec/batch)
2018-01-30 15:27:30.279528: step 78000, loss = 2.02 (5049.4 examples/sec; 0.025 sec/batch)
2018-01-30 15:27:35.407709: step 78200, loss = 2.08 (4992.0 examples/sec; 0.026 sec/batch)
2018-01-30 15:27:38.315151: step 78400, loss = 2.11 (8805.0 examples/sec; 0.015 sec/batch)
2018-01-30 15:27:41.195725: step 78600, loss = 2.06 (8887.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:27:44.062444: step 78800, loss = 2.09 (8930.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:27:47.444094: step 79000, loss = 2.12 (7570.3 examples/sec; 0.017 sec/batch)
2018-01-30 15:27:52.518736: step 79200, loss = 2.09 (5044.7 examples/sec; 0.025 sec/batch)
2018-01-30 15:27:57.759069: step 79400, loss = 2.10 (4885.2 examples/sec; 0.026 sec/batch)
2018-01-30 15:28:02.924245: step 79600, loss = 2.11 (4956.3 examples/sec; 0.026 sec/batch)
2018-01-30 15:28:07.584197: step 79800, loss = 2.02 (5493.6 examples/sec; 0.023 sec/batch)
2018-01-30 15:28:10.462246: step 80000, loss = 2.06 (8894.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:28:13.350881: step 80200, loss = 2.10 (8862.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:28:17.891329: step 80400, loss = 2.05 (5638.2 examples/sec; 0.023 sec/batch)
2018-01-30 15:28:23.666891: step 80600, loss = 2.05 (4432.5 examples/sec; 0.029 sec/batch)
2018-01-30 15:28:28.674108: step 80800, loss = 2.20 (5112.6 examples/sec; 0.025 sec/batch)
2018-01-30 15:28:32.617912: step 81000, loss = 2.09 (6491.2 examples/sec; 0.020 sec/batch)
2018-01-30 15:28:35.492252: step 81200, loss = 2.05 (8906.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:28:38.346034: step 81400, loss = 2.06 (8970.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:28:41.199819: step 81600, loss = 2.13 (8970.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:28:46.077431: step 81800, loss = 2.12 (5248.5 examples/sec; 0.024 sec/batch)
2018-01-30 15:28:51.112421: step 82000, loss = 2.08 (5084.4 examples/sec; 0.025 sec/batch)
2018-01-30 15:28:56.692662: step 82200, loss = 2.12 (4587.6 examples/sec; 0.028 sec/batch)
2018-01-30 15:29:00.480484: step 82400, loss = 2.08 (6758.5 examples/sec; 0.019 sec/batch)
2018-01-30 15:29:03.341615: step 82600, loss = 2.12 (8947.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:29:07.773200: step 82800, loss = 1.97 (5776.7 examples/sec; 0.022 sec/batch)
2018-01-30 15:29:11.677170: step 83000, loss = 1.99 (6557.4 examples/sec; 0.020 sec/batch)
2018-01-30 15:29:17.063864: step 83200, loss = 2.05 (4752.5 examples/sec; 0.027 sec/batch)
2018-01-30 15:29:21.939194: step 83400, loss = 2.05 (5250.9 examples/sec; 0.024 sec/batch)
2018-01-30 15:29:26.917106: step 83600, loss = 2.11 (5142.7 examples/sec; 0.025 sec/batch)
2018-01-30 15:29:29.774065: step 83800, loss = 2.05 (8960.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:29:32.680199: step 84000, loss = 2.06 (8809.0 examples/sec; 0.015 sec/batch)
2018-01-30 15:29:35.538652: step 84200, loss = 2.06 (8955.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:29:39.168945: step 84400, loss = 2.01 (7051.8 examples/sec; 0.018 sec/batch)
2018-01-30 15:29:44.628327: step 84600, loss = 2.02 (4689.2 examples/sec; 0.027 sec/batch)
2018-01-30 15:29:50.027654: step 84800, loss = 2.01 (4741.3 examples/sec; 0.027 sec/batch)
2018-01-30 15:29:54.827382: step 85000, loss = 2.03 (5333.6 examples/sec; 0.024 sec/batch)
2018-01-30 15:29:57.718044: step 85200, loss = 2.06 (8856.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:30:00.596688: step 85400, loss = 2.09 (8893.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:30:03.467568: step 85600, loss = 2.13 (8917.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:30:16.249512: step 85800, loss = 2.08 (2002.8 examples/sec; 0.064 sec/batch)
2018-01-30 15:30:19.139747: step 86000, loss = 2.08 (8857.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:30:22.045201: step 86200, loss = 1.94 (8811.0 examples/sec; 0.015 sec/batch)
2018-01-30 15:30:25.104185: step 86400, loss = 2.08 (8368.8 examples/sec; 0.015 sec/batch)
2018-01-30 15:30:30.552256: step 86600, loss = 2.08 (4698.9 examples/sec; 0.027 sec/batch)
2018-01-30 15:30:35.825165: step 86800, loss = 2.10 (4855.0 examples/sec; 0.026 sec/batch)
2018-01-30 15:30:41.069531: step 87000, loss = 2.01 (4881.4 examples/sec; 0.026 sec/batch)
2018-01-30 15:30:44.156428: step 87200, loss = 2.04 (8293.1 examples/sec; 0.015 sec/batch)
2018-01-30 15:30:47.042560: step 87400, loss = 1.94 (8870.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:30:49.929293: step 87600, loss = 2.09 (8868.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:30:53.249362: step 87800, loss = 2.09 (7710.7 examples/sec; 0.017 sec/batch)
2018-01-30 15:30:58.501839: step 88000, loss = 2.04 (4873.9 examples/sec; 0.026 sec/batch)
2018-01-30 15:31:03.850210: step 88200, loss = 2.02 (4786.5 examples/sec; 0.027 sec/batch)
2018-01-30 15:31:11.043490: step 88400, loss = 2.02 (3558.9 examples/sec; 0.036 sec/batch)
2018-01-30 15:31:13.919486: step 88600, loss = 2.10 (8901.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:31:16.775118: step 88800, loss = 2.19 (8964.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:31:20.906222: step 89000, loss = 2.03 (6196.9 examples/sec; 0.021 sec/batch)
2018-01-30 15:31:26.208258: step 89200, loss = 2.10 (4828.3 examples/sec; 0.027 sec/batch)
2018-01-30 15:31:31.620090: step 89400, loss = 2.06 (4730.4 examples/sec; 0.027 sec/batch)
2018-01-30 15:31:36.071993: step 89600, loss = 2.23 (5750.3 examples/sec; 0.022 sec/batch)
2018-01-30 15:31:38.936899: step 89800, loss = 2.02 (8935.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:31:41.820293: step 90000, loss = 2.10 (8878.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:31:44.642694: step 90200, loss = 2.04 (9070.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:31:49.467755: step 90400, loss = 2.13 (5305.6 examples/sec; 0.024 sec/batch)
2018-01-30 15:31:54.922393: step 90600, loss = 2.04 (4693.3 examples/sec; 0.027 sec/batch)
2018-01-30 15:32:00.171929: step 90800, loss = 2.11 (4876.6 examples/sec; 0.026 sec/batch)
2018-01-30 15:32:03.920207: step 91000, loss = 2.03 (6829.8 examples/sec; 0.019 sec/batch)
2018-01-30 15:32:08.281816: step 91200, loss = 2.02 (5869.4 examples/sec; 0.022 sec/batch)
2018-01-30 15:32:11.148227: step 91400, loss = 2.12 (8931.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:32:15.042088: step 91600, loss = 2.12 (6574.5 examples/sec; 0.019 sec/batch)
2018-01-30 15:32:20.310126: step 91800, loss = 2.06 (4859.5 examples/sec; 0.026 sec/batch)
2018-01-30 15:32:25.710729: step 92000, loss = 2.03 (4740.2 examples/sec; 0.027 sec/batch)
2018-01-30 15:32:30.401051: step 92200, loss = 2.10 (5458.0 examples/sec; 0.023 sec/batch)
2018-01-30 15:32:33.250093: step 92400, loss = 2.07 (8985.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:32:36.098466: step 92600, loss = 2.15 (8987.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:32:38.968406: step 92800, loss = 2.16 (8920.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:32:42.935797: step 93000, loss = 2.03 (6452.6 examples/sec; 0.020 sec/batch)
2018-01-30 15:32:48.406289: step 93200, loss = 2.09 (4679.7 examples/sec; 0.027 sec/batch)
2018-01-30 15:32:53.673410: step 93400, loss = 2.05 (4860.3 examples/sec; 0.026 sec/batch)
2018-01-30 15:32:58.174852: step 93600, loss = 2.04 (5687.1 examples/sec; 0.023 sec/batch)
2018-01-30 15:33:01.040121: step 93800, loss = 2.15 (8934.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:33:03.905791: step 94000, loss = 2.00 (8933.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:33:08.756302: step 94200, loss = 2.09 (5277.8 examples/sec; 0.024 sec/batch)
2018-01-30 15:33:14.014040: step 94400, loss = 2.09 (4869.0 examples/sec; 0.026 sec/batch)
2018-01-30 15:33:19.322263: step 94600, loss = 2.04 (4822.7 examples/sec; 0.027 sec/batch)
2018-01-30 15:33:24.673745: step 94800, loss = 2.07 (4783.7 examples/sec; 0.027 sec/batch)
2018-01-30 15:33:27.554923: step 95000, loss = 2.12 (8885.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:33:30.420320: step 95200, loss = 2.04 (8934.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:33:33.311991: step 95400, loss = 2.06 (8853.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:33:37.169327: step 95600, loss = 2.09 (6636.7 examples/sec; 0.019 sec/batch)
2018-01-30 15:33:42.274513: step 95800, loss = 1.98 (5014.5 examples/sec; 0.026 sec/batch)
2018-01-30 15:33:47.359646: step 96000, loss = 2.05 (5034.3 examples/sec; 0.025 sec/batch)
2018-01-30 15:33:52.237083: step 96200, loss = 2.14 (5248.7 examples/sec; 0.024 sec/batch)
2018-01-30 15:33:55.356862: step 96400, loss = 2.10 (8205.7 examples/sec; 0.016 sec/batch)
2018-01-30 15:33:58.234873: step 96600, loss = 2.01 (8895.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:34:01.132817: step 96800, loss = 1.99 (8833.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:34:14.068473: step 97000, loss = 1.94 (1979.0 examples/sec; 0.065 sec/batch)
2018-01-30 15:34:17.063685: step 97200, loss = 2.06 (8547.0 examples/sec; 0.015 sec/batch)
2018-01-30 15:34:19.992199: step 97400, loss = 2.11 (8741.6 examples/sec; 0.015 sec/batch)
2018-01-30 15:34:22.874503: step 97600, loss = 2.10 (8881.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:34:27.152035: step 97800, loss = 2.09 (5984.8 examples/sec; 0.021 sec/batch)
2018-01-30 15:34:32.490894: step 98000, loss = 2.02 (4795.0 examples/sec; 0.027 sec/batch)
2018-01-30 15:34:37.550976: step 98200, loss = 2.09 (5059.2 examples/sec; 0.025 sec/batch)
2018-01-30 15:34:42.199739: step 98400, loss = 1.99 (5506.8 examples/sec; 0.023 sec/batch)
2018-01-30 15:34:45.075378: step 98600, loss = 2.11 (8902.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:34:47.920962: step 98800, loss = 2.09 (8996.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:34:50.783239: step 99000, loss = 2.04 (8943.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:34:55.176976: step 99200, loss = 2.10 (5826.5 examples/sec; 0.022 sec/batch)
2018-01-30 15:35:00.399692: step 99400, loss = 2.09 (4901.7 examples/sec; 0.026 sec/batch)
2018-01-30 15:35:08.836047: step 99600, loss = 2.09 (3034.5 examples/sec; 0.042 sec/batch)
2018-01-30 15:35:11.691111: step 99800, loss = 1.96 (8966.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:35:14.556237: step 100000, loss = 2.15 (8935.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:35:17.413629: step 100200, loss = 2.05 (8959.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:35:22.156644: step 100400, loss = 2.00 (5397.4 examples/sec; 0.024 sec/batch)
2018-01-30 15:35:27.599387: step 100600, loss = 2.16 (4703.5 examples/sec; 0.027 sec/batch)
2018-01-30 15:35:32.579409: step 100800, loss = 2.11 (5140.5 examples/sec; 0.025 sec/batch)
2018-01-30 15:35:36.598005: step 101000, loss = 2.15 (6370.4 examples/sec; 0.020 sec/batch)
2018-01-30 15:35:39.467497: step 101200, loss = 2.06 (8921.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:35:42.354676: step 101400, loss = 2.03 (8866.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:35:45.229872: step 101600, loss = 2.00 (8903.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:35:49.734837: step 101800, loss = 2.11 (5682.6 examples/sec; 0.023 sec/batch)
2018-01-30 15:35:55.157132: step 102000, loss = 2.14 (4721.2 examples/sec; 0.027 sec/batch)
2018-01-30 15:36:00.245753: step 102200, loss = 2.05 (5030.8 examples/sec; 0.025 sec/batch)
2018-01-30 15:36:05.946036: step 102400, loss = 1.95 (4491.0 examples/sec; 0.029 sec/batch)
2018-01-30 15:36:08.821822: step 102600, loss = 2.10 (8901.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:36:11.682898: step 102800, loss = 2.04 (8947.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:36:15.031950: step 103000, loss = 2.02 (7644.0 examples/sec; 0.017 sec/batch)
2018-01-30 15:36:20.483024: step 103200, loss = 2.03 (4696.3 examples/sec; 0.027 sec/batch)
2018-01-30 15:36:25.518219: step 103400, loss = 2.13 (5084.2 examples/sec; 0.025 sec/batch)
2018-01-30 15:36:30.960242: step 103600, loss = 2.08 (4704.1 examples/sec; 0.027 sec/batch)
2018-01-30 15:36:33.798355: step 103800, loss = 2.12 (9020.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:36:36.649737: step 104000, loss = 2.01 (8978.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:36:39.565839: step 104200, loss = 2.20 (8778.8 examples/sec; 0.015 sec/batch)
2018-01-30 15:36:43.121801: step 104400, loss = 2.09 (7199.2 examples/sec; 0.018 sec/batch)
2018-01-30 15:36:48.244714: step 104600, loss = 2.16 (4997.2 examples/sec; 0.026 sec/batch)
2018-01-30 15:36:53.844802: step 104800, loss = 2.07 (4571.4 examples/sec; 0.028 sec/batch)
2018-01-30 15:36:58.658974: step 105000, loss = 2.01 (5317.6 examples/sec; 0.024 sec/batch)
2018-01-30 15:37:01.697151: step 105200, loss = 2.01 (8426.1 examples/sec; 0.015 sec/batch)
2018-01-30 15:37:06.160110: step 105400, loss = 2.04 (5736.1 examples/sec; 0.022 sec/batch)
2018-01-30 15:37:09.034794: step 105600, loss = 2.07 (8905.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:37:13.525381: step 105800, loss = 2.07 (5700.8 examples/sec; 0.022 sec/batch)
2018-01-30 15:37:18.933413: step 106000, loss = 2.12 (4733.7 examples/sec; 0.027 sec/batch)
2018-01-30 15:37:23.990368: step 106200, loss = 1.99 (5062.3 examples/sec; 0.025 sec/batch)
2018-01-30 15:37:28.244139: step 106400, loss = 2.07 (6018.2 examples/sec; 0.021 sec/batch)
2018-01-30 15:37:31.118420: step 106600, loss = 2.06 (8906.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:37:33.990683: step 106800, loss = 2.07 (8912.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:37:36.858505: step 107000, loss = 2.04 (8926.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:37:41.149194: step 107200, loss = 2.12 (5966.4 examples/sec; 0.021 sec/batch)
2018-01-30 15:37:46.438785: step 107400, loss = 2.10 (4839.7 examples/sec; 0.026 sec/batch)
2018-01-30 15:37:51.760621: step 107600, loss = 2.02 (4810.4 examples/sec; 0.027 sec/batch)
2018-01-30 15:37:56.055527: step 107800, loss = 2.09 (5960.5 examples/sec; 0.021 sec/batch)
2018-01-30 15:37:58.922383: step 108000, loss = 2.03 (8929.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:38:01.782504: step 108200, loss = 2.13 (8950.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:38:13.992962: step 108400, loss = 2.15 (2096.6 examples/sec; 0.061 sec/batch)
2018-01-30 15:38:17.372441: step 108600, loss = 2.02 (7575.1 examples/sec; 0.017 sec/batch)
2018-01-30 15:38:20.248447: step 108800, loss = 2.07 (8901.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:38:23.101200: step 109000, loss = 2.03 (8973.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:38:26.047353: step 109200, loss = 2.01 (8689.3 examples/sec; 0.015 sec/batch)
2018-01-30 15:38:31.284000: step 109400, loss = 2.06 (4888.6 examples/sec; 0.026 sec/batch)
2018-01-30 15:38:36.664650: step 109600, loss = 2.02 (4757.8 examples/sec; 0.027 sec/batch)
2018-01-30 15:38:42.104223: step 109800, loss = 1.97 (4706.3 examples/sec; 0.027 sec/batch)
2018-01-30 15:38:45.275232: step 110000, loss = 2.07 (8073.1 examples/sec; 0.016 sec/batch)
2018-01-30 15:38:48.151445: step 110200, loss = 2.02 (8900.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:38:50.975645: step 110400, loss = 2.05 (9064.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:38:54.010400: step 110600, loss = 2.06 (8435.6 examples/sec; 0.015 sec/batch)
2018-01-30 15:38:59.428559: step 110800, loss = 2.11 (4724.9 examples/sec; 0.027 sec/batch)
2018-01-30 15:39:09.149110: step 111000, loss = 2.03 (2633.6 examples/sec; 0.049 sec/batch)
2018-01-30 15:39:12.018064: step 111200, loss = 1.99 (8923.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:39:14.916183: step 111400, loss = 2.09 (8833.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:39:17.827935: step 111600, loss = 2.09 (8792.0 examples/sec; 0.015 sec/batch)
2018-01-30 15:39:22.703645: step 111800, loss = 1.92 (5250.5 examples/sec; 0.024 sec/batch)
2018-01-30 15:39:27.971774: step 112000, loss = 2.06 (4859.4 examples/sec; 0.026 sec/batch)
2018-01-30 15:39:33.735036: step 112200, loss = 2.16 (4441.9 examples/sec; 0.029 sec/batch)
2018-01-30 15:39:37.122615: step 112400, loss = 2.00 (7557.0 examples/sec; 0.017 sec/batch)
2018-01-30 15:39:40.009723: step 112600, loss = 2.16 (8867.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:39:42.883249: step 112800, loss = 2.12 (8908.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:39:45.827792: step 113000, loss = 2.13 (8694.0 examples/sec; 0.015 sec/batch)
2018-01-30 15:39:51.734048: step 113200, loss = 2.17 (4334.4 examples/sec; 0.030 sec/batch)
2018-01-30 15:39:56.892664: step 113400, loss = 2.04 (4962.6 examples/sec; 0.026 sec/batch)
2018-01-30 15:40:02.250747: step 113600, loss = 2.06 (4777.8 examples/sec; 0.027 sec/batch)
2018-01-30 15:40:06.561251: step 113800, loss = 2.01 (5939.0 examples/sec; 0.022 sec/batch)
2018-01-30 15:40:09.415221: step 114000, loss = 2.17 (8970.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:40:12.277707: step 114200, loss = 2.19 (8943.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:40:17.197625: step 114400, loss = 2.09 (5203.3 examples/sec; 0.025 sec/batch)
2018-01-30 15:40:22.422785: step 114600, loss = 2.06 (4899.4 examples/sec; 0.026 sec/batch)
2018-01-30 15:40:27.599487: step 114800, loss = 2.01 (4945.2 examples/sec; 0.026 sec/batch)
2018-01-30 15:40:31.507932: step 115000, loss = 2.10 (6549.9 examples/sec; 0.020 sec/batch)
2018-01-30 15:40:34.409405: step 115200, loss = 2.03 (8823.1 examples/sec; 0.015 sec/batch)
2018-01-30 15:40:37.273899: step 115400, loss = 2.11 (8937.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:40:40.180027: step 115600, loss = 2.10 (8809.0 examples/sec; 0.015 sec/batch)
2018-01-30 15:40:45.326779: step 115800, loss = 2.14 (4974.0 examples/sec; 0.026 sec/batch)
2018-01-30 15:40:50.881653: step 116000, loss = 2.05 (4608.6 examples/sec; 0.028 sec/batch)
2018-01-30 15:40:56.595291: step 116200, loss = 2.04 (4480.5 examples/sec; 0.029 sec/batch)
2018-01-30 15:40:59.673513: step 116400, loss = 2.10 (8316.5 examples/sec; 0.015 sec/batch)
2018-01-30 15:41:02.529029: step 116600, loss = 2.12 (8965.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:41:06.959502: step 116800, loss = 2.12 (5778.2 examples/sec; 0.022 sec/batch)
2018-01-30 15:41:11.517669: step 117000, loss = 2.14 (5616.3 examples/sec; 0.023 sec/batch)
2018-01-30 15:41:16.834365: step 117200, loss = 2.06 (4815.0 examples/sec; 0.027 sec/batch)
2018-01-30 15:41:22.042299: step 117400, loss = 2.08 (4915.6 examples/sec; 0.026 sec/batch)
2018-01-30 15:41:26.128824: step 117600, loss = 2.10 (6264.5 examples/sec; 0.020 sec/batch)
2018-01-30 15:41:28.998129: step 117800, loss = 2.05 (8922.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:41:31.849870: step 118000, loss = 2.14 (8977.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:41:34.692567: step 118200, loss = 2.13 (9005.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:41:39.371762: step 118400, loss = 2.12 (5471.0 examples/sec; 0.023 sec/batch)
2018-01-30 15:41:44.560896: step 118600, loss = 2.05 (4933.4 examples/sec; 0.026 sec/batch)
2018-01-30 15:41:49.751421: step 118800, loss = 2.01 (4932.1 examples/sec; 0.026 sec/batch)
2018-01-30 15:41:53.889438: step 119000, loss = 2.14 (6186.5 examples/sec; 0.021 sec/batch)
2018-01-30 15:41:56.758275: step 119200, loss = 2.17 (8923.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:41:59.647958: step 119400, loss = 2.18 (8859.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:42:02.516056: step 119600, loss = 2.01 (8925.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:42:15.323641: step 119800, loss = 2.09 (1998.8 examples/sec; 0.064 sec/batch)
2018-01-30 15:42:18.149065: step 120000, loss = 2.04 (9060.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:42:21.025248: step 120200, loss = 2.04 (8900.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:42:23.884502: step 120400, loss = 2.04 (8953.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:42:29.182089: step 120600, loss = 2.02 (4832.4 examples/sec; 0.026 sec/batch)
2018-01-30 15:42:34.721439: step 120800, loss = 2.01 (4621.5 examples/sec; 0.028 sec/batch)
2018-01-30 15:42:39.911343: step 121000, loss = 2.05 (4932.7 examples/sec; 0.026 sec/batch)
2018-01-30 15:42:43.192757: step 121200, loss = 2.02 (7801.5 examples/sec; 0.016 sec/batch)
2018-01-30 15:42:46.042182: step 121400, loss = 2.05 (8984.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:42:48.919511: step 121600, loss = 2.06 (8897.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:42:51.942612: step 121800, loss = 1.97 (8468.1 examples/sec; 0.015 sec/batch)
2018-01-30 15:42:56.907585: step 122000, loss = 2.09 (5156.1 examples/sec; 0.025 sec/batch)
2018-01-30 15:43:02.299570: step 122200, loss = 2.11 (4747.8 examples/sec; 0.027 sec/batch)
2018-01-30 15:43:09.817636: step 122400, loss = 2.07 (3405.1 examples/sec; 0.038 sec/batch)
2018-01-30 15:43:12.691147: step 122600, loss = 2.02 (8909.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:43:15.571864: step 122800, loss = 2.05 (8886.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:43:18.911273: step 123000, loss = 2.07 (7666.0 examples/sec; 0.017 sec/batch)
2018-01-30 15:43:24.656840: step 123200, loss = 1.95 (4455.6 examples/sec; 0.029 sec/batch)
2018-01-30 15:43:30.007930: step 123400, loss = 2.17 (4784.1 examples/sec; 0.027 sec/batch)
2018-01-30 15:43:34.851218: step 123600, loss = 2.11 (5285.7 examples/sec; 0.024 sec/batch)
2018-01-30 15:43:37.743143: step 123800, loss = 2.06 (8852.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:43:40.597463: step 124000, loss = 2.06 (8968.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:43:43.447350: step 124200, loss = 2.10 (8982.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:43:47.561289: step 124400, loss = 2.14 (6222.7 examples/sec; 0.021 sec/batch)
2018-01-30 15:43:52.702012: step 124600, loss = 1.95 (4979.8 examples/sec; 0.026 sec/batch)
2018-01-30 15:43:58.698935: step 124800, loss = 2.09 (4268.9 examples/sec; 0.030 sec/batch)
2018-01-30 15:44:02.713481: step 125000, loss = 2.01 (6376.8 examples/sec; 0.020 sec/batch)
2018-01-30 15:44:07.069704: step 125200, loss = 2.09 (5876.7 examples/sec; 0.022 sec/batch)
2018-01-30 15:44:09.977860: step 125400, loss = 1.97 (8802.8 examples/sec; 0.015 sec/batch)
2018-01-30 15:44:13.839250: step 125600, loss = 2.04 (6629.7 examples/sec; 0.019 sec/batch)
2018-01-30 15:44:19.152937: step 125800, loss = 2.10 (4817.7 examples/sec; 0.027 sec/batch)
2018-01-30 15:44:24.512650: step 126000, loss = 2.00 (4776.4 examples/sec; 0.027 sec/batch)
2018-01-30 15:44:29.282116: step 126200, loss = 2.06 (5367.5 examples/sec; 0.024 sec/batch)
2018-01-30 15:44:32.156635: step 126400, loss = 1.98 (8905.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:44:35.017475: step 126600, loss = 1.96 (8948.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:44:37.873113: step 126800, loss = 2.06 (8964.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:44:41.944846: step 127000, loss = 2.00 (6287.2 examples/sec; 0.020 sec/batch)
2018-01-30 15:44:47.521204: step 127200, loss = 2.08 (4590.8 examples/sec; 0.028 sec/batch)
2018-01-30 15:44:52.908874: step 127400, loss = 2.07 (4751.6 examples/sec; 0.027 sec/batch)
2018-01-30 15:44:57.327096: step 127600, loss = 2.09 (5794.2 examples/sec; 0.022 sec/batch)
2018-01-30 15:45:00.195536: step 127800, loss = 2.08 (8924.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:45:03.067568: step 128000, loss = 1.99 (8913.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:45:07.794964: step 128200, loss = 2.02 (5415.2 examples/sec; 0.024 sec/batch)
2018-01-30 15:45:13.355231: step 128400, loss = 2.12 (4604.1 examples/sec; 0.028 sec/batch)
2018-01-30 15:45:18.448793: step 128600, loss = 2.07 (5026.0 examples/sec; 0.025 sec/batch)
2018-01-30 15:45:23.909611: step 128800, loss = 2.06 (4687.9 examples/sec; 0.027 sec/batch)
2018-01-30 15:45:26.801431: step 129000, loss = 2.19 (8852.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:45:29.672087: step 129200, loss = 2.12 (8917.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:45:32.541373: step 129400, loss = 2.06 (8922.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:45:36.105723: step 129600, loss = 2.14 (7182.2 examples/sec; 0.018 sec/batch)
2018-01-30 15:45:41.529630: step 129800, loss = 2.07 (4719.8 examples/sec; 0.027 sec/batch)
2018-01-30 15:45:46.642164: step 130000, loss = 2.13 (5007.3 examples/sec; 0.026 sec/batch)
2018-01-30 15:45:51.733322: step 130200, loss = 2.19 (5028.3 examples/sec; 0.025 sec/batch)
2018-01-30 15:45:54.570988: step 130400, loss = 2.16 (9021.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:45:57.468696: step 130600, loss = 2.13 (8834.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:46:00.350214: step 130800, loss = 2.02 (8884.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:46:03.966894: step 131000, loss = 1.98 (7078.3 examples/sec; 0.018 sec/batch)
2018-01-30 15:46:15.921400: step 131200, loss = 2.10 (2141.5 examples/sec; 0.060 sec/batch)
2018-01-30 15:46:18.807505: step 131400, loss = 2.11 (8870.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:46:21.665215: step 131600, loss = 2.08 (8958.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:46:25.167660: step 131800, loss = 2.12 (7309.2 examples/sec; 0.018 sec/batch)
2018-01-30 15:46:30.607435: step 132000, loss = 2.04 (4706.1 examples/sec; 0.027 sec/batch)
2018-01-30 15:46:36.225232: step 132200, loss = 2.08 (4556.9 examples/sec; 0.028 sec/batch)
2018-01-30 15:46:40.923497: step 132400, loss = 2.12 (5448.8 examples/sec; 0.023 sec/batch)
2018-01-30 15:46:43.786970: step 132600, loss = 2.11 (8940.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:46:46.647547: step 132800, loss = 1.97 (8949.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:46:49.526788: step 133000, loss = 2.07 (8891.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:46:53.584549: step 133200, loss = 2.03 (6308.9 examples/sec; 0.020 sec/batch)
2018-01-30 15:46:58.814960: step 133400, loss = 2.11 (4894.5 examples/sec; 0.026 sec/batch)
2018-01-30 15:47:03.872121: step 133600, loss = 2.02 (5062.1 examples/sec; 0.025 sec/batch)
2018-01-30 15:47:10.421449: step 133800, loss = 2.09 (3908.8 examples/sec; 0.033 sec/batch)
2018-01-30 15:47:13.314320: step 134000, loss = 2.03 (8849.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:47:16.175515: step 134200, loss = 2.17 (8947.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:47:20.563048: step 134400, loss = 2.08 (5834.7 examples/sec; 0.022 sec/batch)
2018-01-30 15:47:25.869085: step 134600, loss = 2.11 (4824.7 examples/sec; 0.027 sec/batch)
2018-01-30 15:47:31.318709: step 134800, loss = 1.98 (4697.6 examples/sec; 0.027 sec/batch)
2018-01-30 15:47:35.459781: step 135000, loss = 2.09 (6182.0 examples/sec; 0.021 sec/batch)
2018-01-30 15:47:38.318315: step 135200, loss = 2.07 (8955.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:47:41.149931: step 135400, loss = 2.12 (9040.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:47:44.013184: step 135600, loss = 2.09 (8940.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:47:48.564931: step 135800, loss = 1.98 (5624.2 examples/sec; 0.023 sec/batch)
2018-01-30 15:47:54.005579: step 136000, loss = 2.10 (4705.3 examples/sec; 0.027 sec/batch)
2018-01-30 15:47:59.330015: step 136200, loss = 2.20 (4808.0 examples/sec; 0.027 sec/batch)
2018-01-30 15:48:03.294419: step 136400, loss = 2.12 (6457.5 examples/sec; 0.020 sec/batch)
2018-01-30 15:48:07.654825: step 136600, loss = 2.09 (5871.0 examples/sec; 0.022 sec/batch)
2018-01-30 15:48:10.541008: step 136800, loss = 2.01 (8869.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:48:14.269600: step 137000, loss = 2.01 (6865.9 examples/sec; 0.019 sec/batch)
2018-01-30 15:48:19.836503: step 137200, loss = 2.06 (4598.6 examples/sec; 0.028 sec/batch)
2018-01-30 15:48:25.295055: step 137400, loss = 2.06 (4689.9 examples/sec; 0.027 sec/batch)
2018-01-30 15:48:29.939652: step 137600, loss = 2.00 (5511.8 examples/sec; 0.023 sec/batch)
2018-01-30 15:48:32.818190: step 137800, loss = 2.04 (8893.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:48:35.687712: step 138000, loss = 2.11 (8921.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:48:38.549137: step 138200, loss = 2.01 (8946.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:48:42.789450: step 138400, loss = 2.14 (6037.3 examples/sec; 0.021 sec/batch)
2018-01-30 15:48:47.983135: step 138600, loss = 1.96 (4929.1 examples/sec; 0.026 sec/batch)
2018-01-30 15:48:53.207925: step 138800, loss = 2.03 (4899.7 examples/sec; 0.026 sec/batch)
2018-01-30 15:48:57.814055: step 139000, loss = 1.98 (5557.8 examples/sec; 0.023 sec/batch)
2018-01-30 15:49:00.690635: step 139200, loss = 2.01 (8899.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:49:03.578671: step 139400, loss = 2.09 (8864.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:49:07.998340: step 139600, loss = 2.22 (5792.3 examples/sec; 0.022 sec/batch)
2018-01-30 15:49:13.322951: step 139800, loss = 2.19 (4807.9 examples/sec; 0.027 sec/batch)
2018-01-30 15:49:18.927672: step 140000, loss = 2.12 (4567.6 examples/sec; 0.028 sec/batch)
2018-01-30 15:49:24.192173: step 140200, loss = 2.15 (4862.8 examples/sec; 0.026 sec/batch)
2018-01-30 15:49:27.341666: step 140400, loss = 2.07 (8128.3 examples/sec; 0.016 sec/batch)
2018-01-30 15:49:30.231113: step 140600, loss = 2.01 (8859.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:49:33.140541: step 140800, loss = 2.10 (8799.0 examples/sec; 0.015 sec/batch)
2018-01-30 15:49:36.445728: step 141000, loss = 1.99 (7745.4 examples/sec; 0.017 sec/batch)
2018-01-30 15:49:41.938228: step 141200, loss = 2.08 (4660.9 examples/sec; 0.027 sec/batch)
2018-01-30 15:49:47.207727: step 141400, loss = 2.06 (4858.1 examples/sec; 0.026 sec/batch)
2018-01-30 15:49:52.041467: step 141600, loss = 2.07 (5296.1 examples/sec; 0.024 sec/batch)
2018-01-30 15:49:55.291134: step 141800, loss = 2.01 (7877.7 examples/sec; 0.016 sec/batch)
2018-01-30 15:49:58.163835: step 142000, loss = 2.03 (8911.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:50:01.031117: step 142200, loss = 2.10 (8928.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:50:04.083727: step 142400, loss = 2.06 (8386.3 examples/sec; 0.015 sec/batch)
2018-01-30 15:50:16.791364: step 142600, loss = 2.05 (2014.5 examples/sec; 0.064 sec/batch)
2018-01-30 15:50:19.688071: step 142800, loss = 2.07 (8837.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:50:22.559627: step 143000, loss = 2.11 (8915.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:50:26.259216: step 143200, loss = 2.01 (6919.7 examples/sec; 0.018 sec/batch)
2018-01-30 15:50:31.514485: step 143400, loss = 2.10 (4871.3 examples/sec; 0.026 sec/batch)
2018-01-30 15:50:36.813288: step 143600, loss = 2.09 (4831.3 examples/sec; 0.026 sec/batch)
2018-01-30 15:50:41.806942: step 143800, loss = 1.98 (5126.5 examples/sec; 0.025 sec/batch)
2018-01-30 15:50:44.684814: step 144000, loss = 2.09 (8895.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:50:47.622771: step 144200, loss = 2.12 (8713.5 examples/sec; 0.015 sec/batch)
2018-01-30 15:50:50.492011: step 144400, loss = 2.05 (8922.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:50:54.499265: step 144600, loss = 2.04 (6388.4 examples/sec; 0.020 sec/batch)
2018-01-30 15:50:59.540760: step 144800, loss = 2.14 (5077.9 examples/sec; 0.025 sec/batch)
2018-01-30 15:51:04.600542: step 145000, loss = 2.00 (5059.5 examples/sec; 0.025 sec/batch)
2018-01-30 15:51:11.383177: step 145200, loss = 2.02 (3774.3 examples/sec; 0.034 sec/batch)
2018-01-30 15:51:14.210780: step 145400, loss = 1.99 (9053.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:51:17.076450: step 145600, loss = 2.14 (8933.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:51:21.359366: step 145800, loss = 2.03 (5977.2 examples/sec; 0.021 sec/batch)
2018-01-30 15:51:26.792899: step 146000, loss = 2.03 (4711.5 examples/sec; 0.027 sec/batch)
2018-01-30 15:51:32.028566: step 146200, loss = 2.20 (4889.5 examples/sec; 0.026 sec/batch)
2018-01-30 15:51:36.319581: step 146400, loss = 2.05 (5966.0 examples/sec; 0.021 sec/batch)
2018-01-30 15:51:39.193574: step 146600, loss = 2.14 (8907.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:51:42.075642: step 146800, loss = 2.08 (8882.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:51:44.946349: step 147000, loss = 2.04 (8917.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:51:49.438853: step 147200, loss = 2.05 (5698.4 examples/sec; 0.022 sec/batch)
2018-01-30 15:51:54.919689: step 147400, loss = 2.05 (4670.8 examples/sec; 0.027 sec/batch)
2018-01-30 15:52:00.311432: step 147600, loss = 2.06 (4748.0 examples/sec; 0.027 sec/batch)
2018-01-30 15:52:04.258274: step 147800, loss = 2.12 (6486.2 examples/sec; 0.020 sec/batch)
2018-01-30 15:52:08.618064: step 148000, loss = 2.04 (5871.8 examples/sec; 0.022 sec/batch)
2018-01-30 15:52:11.475012: step 148200, loss = 2.05 (8960.6 examples/sec; 0.014 sec/batch)
2018-01-30 15:52:15.209191: step 148400, loss = 2.12 (6855.6 examples/sec; 0.019 sec/batch)
2018-01-30 15:52:21.016077: step 148600, loss = 2.14 (4408.6 examples/sec; 0.029 sec/batch)
2018-01-30 15:52:26.288211: step 148800, loss = 2.01 (4855.7 examples/sec; 0.026 sec/batch)
2018-01-30 15:52:30.771521: step 149000, loss = 2.11 (5710.1 examples/sec; 0.022 sec/batch)
2018-01-30 15:52:33.620724: step 149200, loss = 2.06 (8985.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:52:36.492564: step 149400, loss = 2.02 (8914.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:52:39.357669: step 149600, loss = 2.04 (8935.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:52:43.310479: step 149800, loss = 2.10 (6476.4 examples/sec; 0.020 sec/batch)
2018-01-30 15:52:48.910141: step 150000, loss = 2.16 (4571.7 examples/sec; 0.028 sec/batch)
2018-01-30 15:52:53.928455: step 150200, loss = 2.14 (5101.3 examples/sec; 0.025 sec/batch)
2018-01-30 15:52:58.628032: step 150400, loss = 2.13 (5447.3 examples/sec; 0.023 sec/batch)
2018-01-30 15:53:01.518366: step 150600, loss = 2.01 (8857.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:53:04.398191: step 150800, loss = 2.01 (8889.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:53:09.045140: step 151000, loss = 1.99 (5509.0 examples/sec; 0.023 sec/batch)
2018-01-30 15:53:14.474025: step 151200, loss = 2.04 (4715.5 examples/sec; 0.027 sec/batch)
2018-01-30 15:53:19.853491: step 151400, loss = 2.00 (4758.8 examples/sec; 0.027 sec/batch)
2018-01-30 15:53:24.979956: step 151600, loss = 2.09 (4993.7 examples/sec; 0.026 sec/batch)
2018-01-30 15:53:28.099161: step 151800, loss = 2.09 (8207.2 examples/sec; 0.016 sec/batch)
2018-01-30 15:53:30.933614: step 152000, loss = 2.14 (9031.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:53:33.814706: step 152200, loss = 1.94 (8885.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:53:36.982690: step 152400, loss = 2.09 (8080.8 examples/sec; 0.016 sec/batch)
2018-01-30 15:53:42.201694: step 152600, loss = 2.11 (4905.1 examples/sec; 0.026 sec/batch)
2018-01-30 15:53:47.752563: step 152800, loss = 2.02 (4611.9 examples/sec; 0.028 sec/batch)
2018-01-30 15:53:53.209684: step 153000, loss = 2.10 (4691.1 examples/sec; 0.027 sec/batch)
2018-01-30 15:53:56.204580: step 153200, loss = 2.13 (8547.9 examples/sec; 0.015 sec/batch)
2018-01-30 15:53:59.035830: step 153400, loss = 2.07 (9041.9 examples/sec; 0.014 sec/batch)
2018-01-30 15:54:01.866823: step 153600, loss = 2.11 (9042.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:54:14.695197: step 153800, loss = 2.04 (1995.6 examples/sec; 0.064 sec/batch)
2018-01-30 15:54:17.685591: step 154000, loss = 2.02 (8560.7 examples/sec; 0.015 sec/batch)
2018-01-30 15:54:20.796580: step 154200, loss = 2.10 (8228.9 examples/sec; 0.016 sec/batch)
2018-01-30 15:54:23.652128: step 154400, loss = 2.06 (8965.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:54:27.891612: step 154600, loss = 2.09 (6038.5 examples/sec; 0.021 sec/batch)
2018-01-30 15:54:33.337261: step 154800, loss = 1.96 (4701.0 examples/sec; 0.027 sec/batch)
2018-01-30 15:54:38.599476: step 155000, loss = 2.05 (4864.9 examples/sec; 0.026 sec/batch)
2018-01-30 15:54:42.836741: step 155200, loss = 2.10 (6041.6 examples/sec; 0.021 sec/batch)
2018-01-30 15:54:45.688236: step 155400, loss = 2.07 (8977.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:54:48.565267: step 155600, loss = 2.02 (8898.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:54:51.405690: step 155800, loss = 2.03 (9012.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:54:55.766792: step 156000, loss = 2.09 (5870.1 examples/sec; 0.022 sec/batch)
2018-01-30 15:55:00.580452: step 156200, loss = 2.02 (5318.2 examples/sec; 0.024 sec/batch)
2018-01-30 15:55:09.337869: step 156400, loss = 2.22 (2923.2 examples/sec; 0.044 sec/batch)
2018-01-30 15:55:12.187351: step 156600, loss = 1.97 (8984.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:55:15.011272: step 156800, loss = 2.17 (9065.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:55:18.000852: step 157000, loss = 2.03 (8563.1 examples/sec; 0.015 sec/batch)
2018-01-30 15:55:23.558854: step 157200, loss = 2.07 (4606.0 examples/sec; 0.028 sec/batch)
2018-01-30 15:55:28.641330: step 157400, loss = 2.10 (5036.9 examples/sec; 0.025 sec/batch)
2018-01-30 15:55:34.112160: step 157600, loss = 1.99 (4679.4 examples/sec; 0.027 sec/batch)
2018-01-30 15:55:37.363282: step 157800, loss = 2.07 (7874.2 examples/sec; 0.016 sec/batch)
2018-01-30 15:55:40.193535: step 158000, loss = 2.07 (9045.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:55:43.065399: step 158200, loss = 1.99 (8914.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:55:46.336540: step 158400, loss = 2.06 (7826.0 examples/sec; 0.016 sec/batch)
2018-01-30 15:55:51.334631: step 158600, loss = 2.05 (5122.0 examples/sec; 0.025 sec/batch)
2018-01-30 15:55:56.492354: step 158800, loss = 2.11 (4963.4 examples/sec; 0.026 sec/batch)
2018-01-30 15:56:01.820955: step 159000, loss = 2.09 (4804.3 examples/sec; 0.027 sec/batch)
2018-01-30 15:56:06.650295: step 159200, loss = 1.97 (5300.9 examples/sec; 0.024 sec/batch)
2018-01-30 15:56:09.500022: step 159400, loss = 2.00 (8983.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:56:12.380241: step 159600, loss = 2.02 (8888.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:56:16.503279: step 159800, loss = 2.01 (6209.0 examples/sec; 0.021 sec/batch)
2018-01-30 15:56:21.748645: step 160000, loss = 2.05 (4880.5 examples/sec; 0.026 sec/batch)
2018-01-30 15:56:27.130342: step 160200, loss = 2.14 (4756.9 examples/sec; 0.027 sec/batch)
2018-01-30 15:56:31.552446: step 160400, loss = 2.00 (5789.1 examples/sec; 0.022 sec/batch)
2018-01-30 15:56:34.403320: step 160600, loss = 1.99 (8979.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:56:37.254349: step 160800, loss = 2.09 (8979.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:56:40.133243: step 161000, loss = 2.10 (8892.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:56:44.330571: step 161200, loss = 2.11 (6099.1 examples/sec; 0.021 sec/batch)
2018-01-30 15:56:49.303161: step 161400, loss = 2.05 (5148.2 examples/sec; 0.025 sec/batch)
2018-01-30 15:56:54.484295: step 161600, loss = 1.97 (4941.0 examples/sec; 0.026 sec/batch)
2018-01-30 15:56:59.264722: step 161800, loss = 2.07 (5355.2 examples/sec; 0.024 sec/batch)
2018-01-30 15:57:02.107878: step 162000, loss = 2.10 (9004.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:57:06.566735: step 162200, loss = 2.00 (5741.4 examples/sec; 0.022 sec/batch)
2018-01-30 15:57:09.476548: step 162400, loss = 2.03 (8797.8 examples/sec; 0.015 sec/batch)
2018-01-30 15:57:14.595886: step 162600, loss = 2.04 (5000.6 examples/sec; 0.026 sec/batch)
2018-01-30 15:57:19.987114: step 162800, loss = 2.06 (4748.5 examples/sec; 0.027 sec/batch)
2018-01-30 15:57:25.331159: step 163000, loss = 2.02 (4790.4 examples/sec; 0.027 sec/batch)
2018-01-30 15:57:28.666354: step 163200, loss = 2.00 (7675.7 examples/sec; 0.017 sec/batch)
2018-01-30 15:57:31.509225: step 163400, loss = 2.08 (9005.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:57:34.358087: step 163600, loss = 2.12 (8986.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:57:37.304936: step 163800, loss = 2.01 (8687.2 examples/sec; 0.015 sec/batch)
2018-01-30 15:57:42.644535: step 164000, loss = 1.96 (4794.4 examples/sec; 0.027 sec/batch)
2018-01-30 15:57:47.961938: step 164200, loss = 2.03 (4814.4 examples/sec; 0.027 sec/batch)
2018-01-30 15:57:53.093600: step 164400, loss = 2.01 (4988.6 examples/sec; 0.026 sec/batch)
2018-01-30 15:57:56.386013: step 164600, loss = 1.99 (7775.5 examples/sec; 0.016 sec/batch)
2018-01-30 15:57:59.258743: step 164800, loss = 2.11 (8911.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:58:02.120521: step 165000, loss = 2.13 (8945.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:58:15.007804: step 165200, loss = 2.07 (1986.5 examples/sec; 0.064 sec/batch)
2018-01-30 15:58:17.873006: step 165400, loss = 2.07 (8934.8 examples/sec; 0.014 sec/batch)
2018-01-30 15:58:20.742212: step 165600, loss = 2.09 (8922.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:58:23.626024: step 165800, loss = 2.03 (8877.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:58:27.899998: step 166000, loss = 2.11 (5989.7 examples/sec; 0.021 sec/batch)
2018-01-30 15:58:33.062543: step 166200, loss = 1.97 (4958.8 examples/sec; 0.026 sec/batch)
2018-01-30 15:58:38.469531: step 166400, loss = 2.08 (4734.6 examples/sec; 0.027 sec/batch)
2018-01-30 15:58:42.853349: step 166600, loss = 2.16 (5839.7 examples/sec; 0.022 sec/batch)
2018-01-30 15:58:45.711978: step 166800, loss = 2.08 (8955.3 examples/sec; 0.014 sec/batch)
2018-01-30 15:58:48.601668: step 167000, loss = 2.05 (8859.1 examples/sec; 0.014 sec/batch)
2018-01-30 15:58:51.458003: step 167200, loss = 2.11 (8962.5 examples/sec; 0.014 sec/batch)
2018-01-30 15:58:55.421008: step 167400, loss = 2.07 (6459.7 examples/sec; 0.020 sec/batch)
2018-01-30 15:59:00.550636: step 167600, loss = 1.95 (4990.6 examples/sec; 0.026 sec/batch)
2018-01-30 15:59:09.544160: step 167800, loss = 2.09 (2846.5 examples/sec; 0.045 sec/batch)
2018-01-30 15:59:12.398752: step 168000, loss = 2.04 (8968.0 examples/sec; 0.014 sec/batch)
2018-01-30 15:59:15.268246: step 168200, loss = 2.10 (8921.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:59:18.132308: step 168400, loss = 2.03 (8938.4 examples/sec; 0.014 sec/batch)
2018-01-30 15:59:22.950036: step 168600, loss = 2.13 (5313.7 examples/sec; 0.024 sec/batch)
2018-01-30 15:59:28.421447: step 168800, loss = 2.08 (4678.9 examples/sec; 0.027 sec/batch)
2018-01-30 15:59:33.561915: step 169000, loss = 2.10 (4980.1 examples/sec; 0.026 sec/batch)
2018-01-30 15:59:37.388915: step 169200, loss = 2.13 (6689.3 examples/sec; 0.019 sec/batch)
2018-01-30 15:59:40.275155: step 169400, loss = 2.11 (8869.7 examples/sec; 0.014 sec/batch)
2018-01-30 15:59:43.131286: step 169600, loss = 2.04 (8963.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:59:46.030442: step 169800, loss = 2.04 (8830.2 examples/sec; 0.014 sec/batch)
2018-01-30 15:59:50.742686: step 170000, loss = 2.08 (5432.7 examples/sec; 0.024 sec/batch)
2018-01-30 15:59:56.082462: step 170200, loss = 2.10 (4794.2 examples/sec; 0.027 sec/batch)
2018-01-30 16:00:01.411717: step 170400, loss = 2.06 (4803.7 examples/sec; 0.027 sec/batch)
2018-01-30 16:00:06.886437: step 170600, loss = 2.13 (4676.0 examples/sec; 0.027 sec/batch)
2018-01-30 16:00:09.784293: step 170800, loss = 2.08 (8834.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:00:12.643767: step 171000, loss = 2.10 (8952.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:00:16.465824: step 171200, loss = 2.03 (6698.0 examples/sec; 0.019 sec/batch)
2018-01-30 16:00:21.618539: step 171400, loss = 1.97 (4968.3 examples/sec; 0.026 sec/batch)
2018-01-30 16:00:26.816436: step 171600, loss = 2.11 (4925.1 examples/sec; 0.026 sec/batch)
2018-01-30 16:00:31.868399: step 171800, loss = 2.14 (5067.3 examples/sec; 0.025 sec/batch)
2018-01-30 16:00:34.747757: step 172000, loss = 2.04 (8890.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:00:37.619854: step 172200, loss = 2.14 (8913.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:00:40.511920: step 172400, loss = 2.03 (8851.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:00:43.807779: step 172600, loss = 2.19 (7767.3 examples/sec; 0.016 sec/batch)
2018-01-30 16:00:49.208466: step 172800, loss = 2.11 (4740.1 examples/sec; 0.027 sec/batch)
2018-01-30 16:00:54.405389: step 173000, loss = 1.96 (4926.0 examples/sec; 0.026 sec/batch)
2018-01-30 16:00:59.675009: step 173200, loss = 2.15 (4858.0 examples/sec; 0.026 sec/batch)
2018-01-30 16:01:02.570881: step 173400, loss = 2.07 (8840.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:01:07.006242: step 173600, loss = 2.02 (5771.8 examples/sec; 0.022 sec/batch)
2018-01-30 16:01:09.853449: step 173800, loss = 2.12 (8991.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:01:14.697664: step 174000, loss = 2.11 (5284.7 examples/sec; 0.024 sec/batch)
2018-01-30 16:01:19.906500: step 174200, loss = 2.03 (4914.7 examples/sec; 0.026 sec/batch)
2018-01-30 16:01:25.371595: step 174400, loss = 2.02 (4684.3 examples/sec; 0.027 sec/batch)
2018-01-30 16:01:29.086881: step 174600, loss = 1.99 (6890.5 examples/sec; 0.019 sec/batch)
2018-01-30 16:01:31.930326: step 174800, loss = 2.08 (9003.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:01:34.787968: step 175000, loss = 2.08 (8958.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:01:37.615553: step 175200, loss = 2.07 (9053.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:01:42.993622: step 175400, loss = 2.04 (4760.1 examples/sec; 0.027 sec/batch)
2018-01-30 16:01:48.690642: step 175600, loss = 1.96 (4493.6 examples/sec; 0.028 sec/batch)
2018-01-30 16:01:54.066969: step 175800, loss = 2.04 (4761.6 examples/sec; 0.027 sec/batch)
2018-01-30 16:01:56.942160: step 176000, loss = 2.08 (8903.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:01:59.823465: step 176200, loss = 2.07 (8884.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:02:02.684503: step 176400, loss = 2.14 (8947.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:02:15.533086: step 176600, loss = 2.10 (1992.4 examples/sec; 0.064 sec/batch)
2018-01-30 16:02:18.390701: step 176800, loss = 2.03 (8958.5 examples/sec; 0.014 sec/batch)
2018-01-30 16:02:21.260096: step 177000, loss = 1.98 (8921.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:02:24.103669: step 177200, loss = 2.10 (9002.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:02:28.566660: step 177400, loss = 2.03 (5736.1 examples/sec; 0.022 sec/batch)
2018-01-30 16:02:33.952473: step 177600, loss = 2.03 (4753.2 examples/sec; 0.027 sec/batch)
2018-01-30 16:02:39.116400: step 177800, loss = 2.17 (4957.5 examples/sec; 0.026 sec/batch)
2018-01-30 16:02:43.262445: step 178000, loss = 2.09 (6174.6 examples/sec; 0.021 sec/batch)
2018-01-30 16:02:46.114759: step 178200, loss = 2.03 (8975.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:02:48.958505: step 178400, loss = 2.03 (9002.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:02:51.834225: step 178600, loss = 2.09 (8902.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:02:56.734840: step 178800, loss = 2.14 (5223.8 examples/sec; 0.025 sec/batch)
2018-01-30 16:03:01.959910: step 179000, loss = 2.15 (4899.5 examples/sec; 0.026 sec/batch)
2018-01-30 16:03:09.882899: step 179200, loss = 2.05 (3231.1 examples/sec; 0.040 sec/batch)
2018-01-30 16:03:12.723911: step 179400, loss = 2.04 (9010.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:03:15.567668: step 179600, loss = 2.09 (9002.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:03:18.856704: step 179800, loss = 2.08 (7783.4 examples/sec; 0.016 sec/batch)
2018-01-30 16:03:24.326946: step 180000, loss = 1.95 (4679.9 examples/sec; 0.027 sec/batch)
2018-01-30 16:03:29.949929: step 180200, loss = 2.00 (4552.7 examples/sec; 0.028 sec/batch)
2018-01-30 16:03:35.415030: step 180400, loss = 1.98 (4684.3 examples/sec; 0.027 sec/batch)
2018-01-30 16:03:38.511494: step 180600, loss = 2.05 (8267.5 examples/sec; 0.015 sec/batch)
2018-01-30 16:03:41.389997: step 180800, loss = 2.07 (8893.5 examples/sec; 0.014 sec/batch)
2018-01-30 16:03:44.236771: step 181000, loss = 2.04 (8992.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:03:47.366967: step 181200, loss = 2.04 (8178.4 examples/sec; 0.016 sec/batch)
2018-01-30 16:03:52.587273: step 181400, loss = 2.00 (4903.9 examples/sec; 0.026 sec/batch)
2018-01-30 16:03:58.130043: step 181600, loss = 1.99 (4618.6 examples/sec; 0.028 sec/batch)
2018-01-30 16:04:03.417955: step 181800, loss = 2.10 (4841.2 examples/sec; 0.026 sec/batch)
2018-01-30 16:04:07.781435: step 182000, loss = 1.99 (5866.9 examples/sec; 0.022 sec/batch)
2018-01-30 16:04:10.648394: step 182200, loss = 2.08 (8929.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:04:13.531279: step 182400, loss = 2.08 (8880.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:04:18.155921: step 182600, loss = 2.07 (5535.6 examples/sec; 0.023 sec/batch)
2018-01-30 16:04:23.712097: step 182800, loss = 2.13 (4607.5 examples/sec; 0.028 sec/batch)
2018-01-30 16:04:29.126636: step 183000, loss = 2.02 (4728.0 examples/sec; 0.027 sec/batch)
2018-01-30 16:04:32.759429: step 183200, loss = 2.15 (7046.9 examples/sec; 0.018 sec/batch)
2018-01-30 16:04:35.609806: step 183400, loss = 2.06 (8981.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:04:38.479513: step 183600, loss = 2.12 (8920.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:04:41.345795: step 183800, loss = 2.00 (8931.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:04:46.480404: step 184000, loss = 1.95 (4985.8 examples/sec; 0.026 sec/batch)
2018-01-30 16:04:51.652644: step 184200, loss = 2.11 (4949.5 examples/sec; 0.026 sec/batch)
2018-01-30 16:04:56.583141: step 184400, loss = 1.94 (5192.2 examples/sec; 0.025 sec/batch)
2018-01-30 16:05:00.490108: step 184600, loss = 2.09 (6552.4 examples/sec; 0.020 sec/batch)
2018-01-30 16:05:03.357709: step 184800, loss = 2.08 (8927.3 examples/sec; 0.014 sec/batch)
2018-01-30 16:05:07.690730: step 185000, loss = 2.09 (5908.1 examples/sec; 0.022 sec/batch)
2018-01-30 16:05:11.618586: step 185200, loss = 2.03 (6517.6 examples/sec; 0.020 sec/batch)
2018-01-30 16:05:16.879362: step 185400, loss = 2.14 (4866.2 examples/sec; 0.026 sec/batch)
2018-01-30 16:05:22.347646: step 185600, loss = 2.05 (4681.5 examples/sec; 0.027 sec/batch)
2018-01-30 16:05:27.124624: step 185800, loss = 2.11 (5359.0 examples/sec; 0.024 sec/batch)
2018-01-30 16:05:30.007941: step 186000, loss = 1.98 (8878.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:05:32.853367: step 186200, loss = 2.06 (8996.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:05:35.700735: step 186400, loss = 2.08 (8990.8 examples/sec; 0.014 sec/batch)
2018-01-30 16:05:39.712741: step 186600, loss = 2.01 (6380.8 examples/sec; 0.020 sec/batch)
2018-01-30 16:05:45.388459: step 186800, loss = 2.05 (4510.4 examples/sec; 0.028 sec/batch)
2018-01-30 16:05:50.570798: step 187000, loss = 2.07 (4939.9 examples/sec; 0.026 sec/batch)
2018-01-30 16:05:54.984120: step 187200, loss = 2.04 (5800.6 examples/sec; 0.022 sec/batch)
2018-01-30 16:05:57.854335: step 187400, loss = 1.97 (8919.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:06:00.832706: step 187600, loss = 1.95 (8595.3 examples/sec; 0.015 sec/batch)
2018-01-30 16:06:03.695579: step 187800, loss = 2.04 (8942.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:06:16.620343: step 188000, loss = 2.03 (1980.7 examples/sec; 0.065 sec/batch)
2018-01-30 16:06:19.487539: step 188200, loss = 2.22 (8928.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:06:22.336239: step 188400, loss = 2.08 (8986.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:06:25.461401: step 188600, loss = 2.09 (8191.6 examples/sec; 0.016 sec/batch)
2018-01-30 16:06:30.811502: step 188800, loss = 1.99 (4785.0 examples/sec; 0.027 sec/batch)
2018-01-30 16:06:36.186830: step 189000, loss = 2.00 (4762.5 examples/sec; 0.027 sec/batch)
2018-01-30 16:06:41.243902: step 189200, loss = 2.01 (5062.2 examples/sec; 0.025 sec/batch)
2018-01-30 16:06:44.367874: step 189400, loss = 2.09 (8194.7 examples/sec; 0.016 sec/batch)
2018-01-30 16:06:47.232069: step 189600, loss = 2.04 (8937.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:06:50.071553: step 189800, loss = 2.09 (9015.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:06:53.152724: step 190000, loss = 2.06 (8308.5 examples/sec; 0.015 sec/batch)
2018-01-30 16:06:58.435868: step 190200, loss = 2.00 (4845.6 examples/sec; 0.026 sec/batch)
2018-01-30 16:07:03.571254: step 190400, loss = 2.10 (4985.0 examples/sec; 0.026 sec/batch)
2018-01-30 16:07:10.935791: step 190600, loss = 1.95 (3476.1 examples/sec; 0.037 sec/batch)
2018-01-30 16:07:13.808800: step 190800, loss = 1.98 (8910.5 examples/sec; 0.014 sec/batch)
2018-01-30 16:07:16.681468: step 191000, loss = 2.07 (8911.6 examples/sec; 0.014 sec/batch)
2018-01-30 16:07:20.317151: step 191200, loss = 2.06 (7041.3 examples/sec; 0.018 sec/batch)
2018-01-30 16:07:25.598164: step 191400, loss = 2.08 (4847.6 examples/sec; 0.026 sec/batch)
2018-01-30 16:07:30.552846: step 191600, loss = 2.13 (5166.8 examples/sec; 0.025 sec/batch)
2018-01-30 16:07:35.584869: step 191800, loss = 2.08 (5087.4 examples/sec; 0.025 sec/batch)
2018-01-30 16:07:38.721818: step 192000, loss = 2.12 (8160.8 examples/sec; 0.016 sec/batch)
2018-01-30 16:07:41.587218: step 192200, loss = 2.22 (8934.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:07:44.398500: step 192400, loss = 2.10 (9106.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:07:47.710168: step 192600, loss = 2.02 (7730.2 examples/sec; 0.017 sec/batch)
2018-01-30 16:07:53.116420: step 192800, loss = 2.14 (4735.3 examples/sec; 0.027 sec/batch)
2018-01-30 16:07:58.093508: step 193000, loss = 2.04 (5143.6 examples/sec; 0.025 sec/batch)
2018-01-30 16:08:03.566687: step 193200, loss = 2.07 (4677.4 examples/sec; 0.027 sec/batch)
2018-01-30 16:08:07.937784: step 193400, loss = 2.04 (5856.7 examples/sec; 0.022 sec/batch)
2018-01-30 16:08:10.812535: step 193600, loss = 2.10 (8905.1 examples/sec; 0.014 sec/batch)
2018-01-30 16:08:13.655434: step 193800, loss = 2.04 (9004.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:08:18.370612: step 194000, loss = 2.14 (5429.3 examples/sec; 0.024 sec/batch)
2018-01-30 16:08:23.601554: step 194200, loss = 2.14 (4894.0 examples/sec; 0.026 sec/batch)
2018-01-30 16:08:28.803457: step 194400, loss = 2.09 (4921.3 examples/sec; 0.026 sec/batch)
2018-01-30 16:08:33.103130: step 194600, loss = 2.03 (5953.9 examples/sec; 0.021 sec/batch)
2018-01-30 16:08:35.983040: step 194800, loss = 2.10 (8889.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:08:38.877665: step 195000, loss = 2.10 (8844.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:08:41.735315: step 195200, loss = 2.13 (8958.4 examples/sec; 0.014 sec/batch)
2018-01-30 16:08:45.968112: step 195400, loss = 1.95 (6048.0 examples/sec; 0.021 sec/batch)
2018-01-30 16:08:51.573719: step 195600, loss = 2.18 (4566.9 examples/sec; 0.028 sec/batch)
2018-01-30 16:08:56.954443: step 195800, loss = 2.11 (4757.7 examples/sec; 0.027 sec/batch)
2018-01-30 16:09:00.939917: step 196000, loss = 2.07 (6423.3 examples/sec; 0.020 sec/batch)
2018-01-30 16:09:03.794780: step 196200, loss = 1.99 (8967.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:09:08.239268: step 196400, loss = 1.96 (5759.9 examples/sec; 0.022 sec/batch)
2018-01-30 16:09:11.821057: step 196600, loss = 2.01 (7147.3 examples/sec; 0.018 sec/batch)
2018-01-30 16:09:17.166082: step 196800, loss = 2.19 (4789.5 examples/sec; 0.027 sec/batch)
2018-01-30 16:09:22.490445: step 197000, loss = 2.07 (4808.1 examples/sec; 0.027 sec/batch)
2018-01-30 16:09:27.479002: step 197200, loss = 2.07 (5131.7 examples/sec; 0.025 sec/batch)
2018-01-30 16:09:30.342948: step 197400, loss = 2.02 (8938.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:09:33.202313: step 197600, loss = 2.10 (8953.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:09:36.071645: step 197800, loss = 2.12 (8921.9 examples/sec; 0.014 sec/batch)
2018-01-30 16:09:39.852864: step 198000, loss = 2.13 (6770.3 examples/sec; 0.019 sec/batch)
2018-01-30 16:09:45.034689: step 198200, loss = 2.06 (4940.3 examples/sec; 0.026 sec/batch)
2018-01-30 16:09:50.278020: step 198400, loss = 2.01 (4882.4 examples/sec; 0.026 sec/batch)
2018-01-30 16:09:55.317693: step 198600, loss = 2.04 (5079.7 examples/sec; 0.025 sec/batch)
2018-01-30 16:09:58.164505: step 198800, loss = 2.09 (8992.5 examples/sec; 0.014 sec/batch)
2018-01-30 16:10:01.057484: step 199000, loss = 2.04 (8849.0 examples/sec; 0.014 sec/batch)
2018-01-30 16:10:03.919193: step 199200, loss = 2.16 (8945.7 examples/sec; 0.014 sec/batch)
2018-01-30 16:10:16.746174: step 199400, loss = 2.17 (1995.8 examples/sec; 0.064 sec/batch)
2018-01-30 16:10:19.593719: step 199600, loss = 2.10 (8990.2 examples/sec; 0.014 sec/batch)
2018-01-30 16:10:22.461992: step 199800, loss = 2.11 (8925.2 examples/sec; 0.014 sec/batch)
